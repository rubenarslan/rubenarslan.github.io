[
  {
    "path": "posts/2025-05-20-consensus-how-the-sausage-gets-made/",
    "title": "Consensus: How the sausage gets made",
    "description": "What can we learn about the experts involved in the recent social media effects consensus preprint and what does that say about the consensus.",
    "author": [
      {
        "name": "Ruben C. Arslan",
        "url": "https://rubenarslan.github.io"
      }
    ],
    "date": "2025-05-20",
    "categories": [],
    "contents": "\nA large working group of psychologists recently put out a consensus statement on social media and mental health effects, based on Haidt’s book “The Anxious Generation”.\nMany of the scholars I respect the most in this space (e.g., Lucy Foulkes, Amy Orben, Andrew Przybylski) seem to have not participated or dropped out of this effort. I know that in at least two cases this was because they did not trust the process. I am not an expert in this field myself, but I did read “The Anxious Generation”, “What Mental Illness Really Is … And What It Isn’t” and a few other things and have some familiarity with how strong the evidence base is. Now, personally, I think this consensus effort is premature. For many of the claims they examined we don’t have the evidence to assess many of these claims well. Consensus is not a shortcut to understanding the world.\nIn the end, the participants found consensus for only one causal claim (“Heavy smartphone and social media use can cause sleep problems.”), several weak claims about associations (e.g., “social media may be associated with visual social comparison in girls”) as well as a claim that adolescent mental health has declined in several Western countries. To me, not even the evidence for this mental health decline is very compelling. But generally this consensus is not far away from my own current understanding. What irks me is how this was communicated and what they chose to focus on.\nIt’s not your fault if recent social media posts led you to believe there was a “ton of consensus” that “Adolescent mental health has declined”1 or that rejecting this consensus is akin to denying climate change.2\n\nScreenshot of now deleted tweet by Jay van Bavel\nHowever, even if I don’t think the consensus statement has much value, I think it’s meta-scientifically interesting to look at how the participating scholars assessed the level of evidence for each claim. Maybe this will also give us an idea what kind of experts self-selected into the process. Potentially, the Delphi method corrected for all problems. I am not trying to assess that. I am only looking at Study 1 here to understand “how the sausage gets made”. Similarly, I found it interesting to read how the first author dealt with critical comments, documented on OSF.\nNaively, you might expect that sometimes, when people have expertise in one area but not another, they would not answer questions in those areas where they cannot assess the evidence base. You would not expect them to report a stronger evidence base than exists. Consider replicated experimental field studies. These are very effortful studies, done either at considerable expense or making use of widely reported policy changes. You certainly wouldn’t expect a researcher to say such studies exist in their field of expertise when they don’t. In fact, I fail my students when they make claims their citations cannot support (I’ve gotten stricter about this because it’s such an LLM hallmark). WELL!\nYou might be surprised to hear that, initially, for at least seven claims a substantial fraction of scholars said there were causally informative field studies yet none were cited in the final consensus statement’s evidence summary according to language model Gemini 2.5 Pro.3 The first author, Valerio Capraro, seems to agree that the evidence for most claims is weak and correlational. So, I don’t think my assessment that some scholars overestimated the evidence is out of whack with the first author’s views.\nSo, who are these scholars who think there have been replicated causally informative field studies showing that social media affects body image? Maybe they have references that didn’t make the cut. Unfortunately, I don’t know! Turns out the responses were anonymized, as is standard in the Delphi method. I.e. even for those scholars who participated non-anonymously, we don’t know what they said for each claim. Expertise without accountability? Truly, you can have it all.\n\nWhat question did they answer exactly when assessing the evidence base for causal claims?\n\nTo the best of your knowledge, what is the level of evidence regarding this claim?\n▢ No evidence, only opinions, perspectives, general theory or anecdotes (1)\n▢ Some correlational evidence (laboratories, surveys, online, field) (2)\n▢ Some causal evidence but in limited settings (laboratories, surveys, and online, self-reported measures) (3)\n▢ Causal evidence in a field study (4)\n▢ Replicated causal evidence from field studies (5)\n▢ Wide-scale causal evidence from multiple field studies, policy evaluations or other natural settings (6)\n\nWhat did they say in Study 1?\n\n\nFigure caption for Figure 3\n\nFigure 3. Summary of responses to the “level” question for each of the 26 claims from\nSurvey 1, ordered as in Figure 2. For each claim, the “level” question asked in Survey 1\nwas: “To the best of your knowledge, what is the level of evidence regarding this claim?”.\nAvailable answers depended on the claim. For non-causal claims 1-7, 16, and 22-23, the\navailable answers were: No evidence, only opinions, perspectives, general theory or\nanecdotes; Some empirical evidence but in limited settings (laboratories, surveys and online,\nself-reported measures); Field evidence; Replicated evidence in field studies or other natural\nsettings; Wide-scale evidence from multiple field studies, policy evaluations or other natural\nsettings. For causal claims 8-15, 17-21, and 24-26, the available answers were: No evidence,\nonly opinions, perspectives, general theory or anecdotes; Some correlational evidence\n(laboratories, surveys, online, field); Some causal evidence but in limited settings\n(laboratories, surveys, and online, self-reported measures); Causal evidence in a field study;\nReplicated causal evidence from field studies; Wide-scale causal evidence from multiple field\nstudies, policy evaluations or other natural settings. For visualization purposes, we\nshortened the wording of the claims. Refer to Table 1 for the exact wording of the claims.\n\nAs far as I could tell at a glance, the authors didn’t precisely define what counts as causal vs. correlational. I’m one of those weirdos who believe you can infer causality from observational/correlational data if you’re careful, but from the contrast to “correlational”, I guess the authors considered only experiments and natural experiments “causal evidence”.\nNow, after examining this graph, you would be excused if you thought these categories were mutually exclusive. But no:\n\nMoreover, after careful consideration within the core group, we decided to allow multiple answers for this question to avoid forcing participants to rank the level of evidence.\n\nYes, you certainly wouldn’t want to rank levels of evidence. How would those poor correlational studies and opinions feel? Still, you might be confused, after all, Figure 3 (above) tabulates them as if responses summed to 100%. But actually authors could and did endorse multiple mutually exclusive answers. Most obviously, more than 70 responses say both there is “no evidence” and at the same time say there is evidence. I guess they exist in a state of superposition. It would collapse if we actually observed the evidence, but, like, who has the time?\nvalue\nn()\n1,2\n59\n1,2,3\n9\n1,2,3,4,6\n1\n1,3\n4\n1,3,4\n1\n1,5\n1\n1,6\n1\nThe paper didn’t document how the graph was made, so I asked) and it turns out they expanded the denominator. It sums to 100% because the denominator is “number of checkboxes clicked”. I don’t think that’s a useful denominator. People who checked no box are excluded and you can’t interpret the percentage as the number of experts who endorsed X.\nI looked into this because several experts claimed that there were replicated causally informative field studies showing that social media increases visual social comparison in girls. That seemed odd, because a) this seems difficult to study experimentally in the field b) I looked at this literature briefly a while ago and didn’t encounter anything of the sort. So, I checked the evidence summary in Supplementary Section S5. Indeed, no field studies are highlighted at all. Weird. Did they imagine these studies? Do they exist, but for some reason the best-quality evidence (sorry, sorry, I know evidence shan’t be ranked) isn’t mentioned?\nTo find out how this stacks up for the other claims, I let Gemini 2.5 Pro take a stab at tabulating how often experts say studies exist that then seem to be missing in action in the evidence review. I only hand-checked the visual social comparison section. Gemini agreed with my assessment there. So, here we have it: It’s not just LLMs that occasionally hallucinate scientific papers.\nDoes this mean the consensus statement is wrong? They went through three more rounds and had a chance to rectify such problems with grounding. Still, I worry that people who admit uncertainty self-selected out and people who incorrectly believe causally informative field studies exist where there are none, self-selected in.\n\nClaim\n% Causal Field (Est. from Fig 3)\n% Causal Field or Higher (Est. from Fig 3)\nS5 Text: Experimental Field, Natural Experiment, or Well-Controlled Design in Field Supporting Causal Claim?\nS5 Text: Observational Study Cited as Supporting Causal Claim?\n8. Heavy daily use of smartphones and social media can cause sleep deprivation.\n~10-12%\n~25-30%\nYes (Intervention studies like Martin et al., Perrault et al., Tu et al., described as improving sleep by reducing screen time, could be field experiments). Schechter et al. (meta-analysis of blue light interventions) and Hartstein et al. (consensus panel review) also point to experimental/interventional evidence.\nYes (Longitudinal studies like Rod et al., Liu et al. cited for associations).\n9. Chronic sleep deprivation can cause a decline in mental health.\n~5-7%\n~75-80%\nYes (Scott et al. meta-analysis of sleep-improvement interventions showing mental health benefits is strong. Pires et al. also likely experimental).\nYes (Liu et al. review of longitudinal studies).\n10. Heavy daily use of smartphones and social media can cause attention fragmentation.\n~2-3%\n~10-12%\nPotentially (van Wezel et al. intervention, though manipulation failed, was field-based. “Mere presence” studies like Ward et al. are lab).\nYes (Longitudinal studies like Ra et al., Deng et al. cited for associations with ADHD symptoms).\n11. Attention fragmentation can cause a decline in mental health…\n~2-3%\n~5-7%\nPotentially (Intervention studies on disabling notifications like Fitz et al., Olson et al. are field-based and show some MH impact).\nYes (Correlational studies on multitasking and MH like Becker et al., Shin et al.).\n12. Heavy daily use of smartphones and social media can cause behavioral addiction.\n~7-10%\n~20-25%\nYes (Allcott et al. 2022 is explicitly cited as a field study suggesting habit formation and self-control problems).\nYes (Truzoli et al. on distress post-internet; Anderson & Wood on blunted reward sensitivity).\n13. Behavioral addiction can cause a decline in mental health.\n~5-7%\n~50-55%\nNo direct experimental field study cited for causing decline. The argument is often definitional or based on comorbidity.\nYes (Hartmann & Blaszczynski longitudinal study on gambling; Petry et al., Ko et al. on comorbidity).\n14. Heavy daily use of smartphones and social media can cause social deprivation…\n~2-4%\n~10-15%\nYes (Allcott et al. 2020 deactivating Facebook was an experimental field study. Geraci et al. 2022 quasi-experimental study on internet speed is a natural experiment).\nYes (Meta-analyses by Appel et al., Liu & Baumeister cited, though Cheng et al. found no overall association).\n15. Chronic social deprivation can cause a decline in mental health.\n~10-12%\n~60-65%\nYes (Killgore et al. 2020 on lockdown effects is a natural experiment. Intervention studies like Alvarez et al., Costello et al., Lai et al. on increasing social interaction improve MH and are likely field-based).\nYes (Cacioppo et al. longitudinal study on isolation and depression).\n17. Social media increases visual social comparisons among adolescent girls.\n~2-3%\n~15-20%\nNo (Cited experimental studies like Fardouly et al. 2015, Kleemans et al. 2018, Tiggemann & Anderberg 2020 are lab/online, not field studies measuring social comparison as an outcome in a consequential real-world setting).\nYes (Couture Bue 2020 eye-tracking study; Fardouly et al. 2017 survey).\n18. Social media increases perfectionism among adolescent girls.\n~2-3%\n~5-8%\nNo (Consensus states no experimental studies directly manipulating social media for this. Vanhoffelen et al. 2025 longitudinal study showed weak evidence).\nYes (Surveys by Danielsen et al. 2024, Simon et al. 2022, Harren et al. 2021).\n19. Social media increases relational aggression among adolescent girls…\n~2-3%\n~5-7%\nNo (Consensus states no experimental evidence for SM use -> increased RA. Martins & Weaver meta-analysis included experiments on “exposure to RA -> RA behavior,” not direct SM manipulation).\nYes (Marinoni et al. 2023 on cyberbullying vulnerability).\n20. Among adolescent girls, social media increases exposure to other people displaying or discussing their mental disorders.\n~3-5%\n~15-20%\nNo (Consensus states no direct causal link references. Cited studies are observational, e.g., phenomena on TikTok, Twitter disclosures).\nYes (Alho et al. on peer networks; Hamilton et al. on self-harm content exposure).\n21. Social media increases sexual predation and harassment of adolescent girls…\n~1-2%\n~10-15%\nPotentially (Bhuller et al. 2013 quasi-experimental study on general internet access and sex crimes is a natural experiment, but not specific to SM mechanisms for predation). Consensus states no SM-specific experimental studies.\nYes (Bratton et al. 2020 on Facebook use and online harassment).\n24. If most parents waited… it would benefit the mental health of adolescents overall.\n~2-3%\n~5-8%\nNo (Cited studies are correlational/longitudinal. No experimental field studies manipulating delayed access).\nYes (Dempsey et al., Vaterlaus et al., Gerosa et al., Sapien Labs, Sun 2023, all observational).\n25. Imposing (and enforcing) a legal minimum age of 16… would benefit the mental health of adolescents overall.\n~1-2%\n~5-7%\nNo (Consensus states no strictly related references. Cited experimental work like Ferguson 2024 is on temporary abstention, not age limits).\nYes (General research linking SM to decreased MH like Twenge & Campbell 2018 cited as rationale, but it’s not testing the policy).\n26. Phone-free schools would benefit the mental health of adolescents overall.\n~5-7%\n~10-15%\nYes (Studies of school phone bans cited in Böttger & Zierer 2024 meta-analysis, King et al. 2024, Campbell et al. 2024 review, Goodyear et al. 2025 meta-analysis are natural experiments in field settings).\nNo (The studies cited are evaluations of bans, not general observational studies of phone use in schools).\n\nNotes on Estimation from Figure 3:\nThe percentages for “% Causal Field” and “% Causal Field or Higher” are visual estimations from the provided stacked bar chart (Figure 3, p. 11 of the PDF). They are approximate due to the nature of reading from a graph.\nMy requests\nI made a few requests and then had Gemini summarise. I put the final summary up top, the rest is in conversational order.\n\nFor each of the claims, there is a summary of the evidence in Supplementary Section 5. Look only at claims that make a causal claim. Then, note, for each claim, how many experts assessed the evidence as being causal, causal/field, replicated evidence, widespread causal evidence. Then, assess whether the studies cited in that section are causally informative, field studies or direct replications of one another. Note if there is a conflict with the expert assessment.\nStart with the claim that “Social media increases visual social comparison in girls”. We’ll take the claims in turn.\n\nYou’re right, Figure S3 (which is Figure 3 in the main text on page 11, not Supplementary Section S4 as I previously misremembered) is a stacked bar chart where the segments do appear to represent mutually exclusive portions of the respondents who answered the “level” question for each claim, likely categorized by the highest level of evidence they endorsed.\nMy apologies, my previous interpretation was incorrect based on how these charts are typically constructed to represent a whole. The explicit mention of allowing multiple answers is for the data collection phase, but for visualization in a stacked bar chart aiming for 100%, a recoding to the highest level endorsed is standard.\nLet’s re-examine based on this understanding and your new request to specifically look for experimental field studies, natural experiments in the field, or well-controlled designs cited as supporting the causal claim in Section S5, and to report the “Causal Field” percentage directly from Figure 3.\nRevised Interpretation of Figure 3 (formerly S3):\nThe bars in Figure 3 sum to 100% for each claim (representing all experts who provided a “level of evidence” response for that claim). Each colored segment represents the proportion of those experts for whom that specific level was their highest endorsed level of evidence.\nThe dark purple (“Wide-scale causal”) is the highest.\nThe next blue (“Wide-scale”) would be for non-causal claims.\nThen “Replicated causal field,” “Replicated field,” “Causal field,” “Field,” “Causal limited,” “Correlational,” “Empirical,” and finally “None (e.g., opinion).”\nThe “No response” category mentioned in the Figure S3 caption (page 63) is not visually part of the stacked bars in Figure 3 on page 11, meaning Figure 3 likely only includes those who did provide a level of evidence.\nI will now go through the claims again, try to estimate the “Causal field” percentage from the visual in Figure 3 (page 11), and assess the S5 text for the types of evidence you’ve specified.\nRevised Table\nClaim\n% Causal Field (Est. from Fig 3)\n% Causal Field or Higher (Est. from Fig 3)\nS5 Text: Experimental Field, Natural Experiment, or Well-Controlled Design in Field Supporting Causal Claim?\nS5 Text: Observational Study Cited as Supporting Causal Claim?\n8. Heavy daily use of smartphones and social media can cause sleep deprivation.\n~10-12%\n~25-30%\nYes (Intervention studies like Martin et al., Perrault et al., Tu et al., described as improving sleep by reducing screen time, could be field experiments). Schechter et al. (meta-analysis of blue light interventions) and Hartstein et al. (consensus panel review) also point to experimental/interventional evidence.\nYes (Longitudinal studies like Rod et al., Liu et al. cited for associations).\n9. Chronic sleep deprivation can cause a decline in mental health.\n~5-7%\n~75-80%\nYes (Scott et al. meta-analysis of sleep-improvement interventions showing mental health benefits is strong. Pires et al. also likely experimental).\nYes (Liu et al. review of longitudinal studies).\n10. Heavy daily use of smartphones and social media can cause attention fragmentation.\n~2-3%\n~10-12%\nPotentially (van Wezel et al. intervention, though manipulation failed, was field-based. “Mere presence” studies like Ward et al. are lab).\nYes (Longitudinal studies like Ra et al., Deng et al. cited for associations with ADHD symptoms).\n11. Attention fragmentation can cause a decline in mental health…\n~2-3%\n~5-7%\nPotentially (Intervention studies on disabling notifications like Fitz et al., Olson et al. are field-based and show some MH impact).\nYes (Correlational studies on multitasking and MH like Becker et al., Shin et al.).\n12. Heavy daily use of smartphones and social media can cause behavioral addiction.\n~7-10%\n~20-25%\nYes (Allcott et al. 2022 is explicitly cited as a field study suggesting habit formation and self-control problems).\nYes (Truzoli et al. on distress post-internet; Anderson & Wood on blunted reward sensitivity).\n13. Behavioral addiction can cause a decline in mental health.\n~5-7%\n~50-55%\nNo direct experimental field study cited for causing decline. The argument is often definitional or based on comorbidity.\nYes (Hartmann & Blaszczynski longitudinal study on gambling; Petry et al., Ko et al. on comorbidity).\n14. Heavy daily use of smartphones and social media can cause social deprivation…\n~2-4%\n~10-15%\nYes (Allcott et al. 2020 deactivating Facebook was an experimental field study. Geraci et al. 2022 quasi-experimental study on internet speed is a natural experiment).\nYes (Meta-analyses by Appel et al., Liu & Baumeister cited, though Cheng et al. found no overall association).\n15. Chronic social deprivation can cause a decline in mental health.\n~10-12%\n~60-65%\nYes (Killgore et al. 2020 on lockdown effects is a natural experiment. Intervention studies like Alvarez et al., Costello et al., Lai et al. on increasing social interaction improve MH and are likely field-based).\nYes (Cacioppo et al. longitudinal study on isolation and depression).\n17. Social media increases visual social comparisons among adolescent girls.\n~2-3%\n~15-20%\nNo (Cited experimental studies like Fardouly et al. 2015, Kleemans et al. 2018, Tiggemann & Anderberg 2020 are lab/online, not field studies measuring social comparison as an outcome in a consequential real-world setting).\nYes (Couture Bue 2020 eye-tracking study; Fardouly et al. 2017 survey).\n18. Social media increases perfectionism among adolescent girls.\n~2-3%\n~5-8%\nNo (Consensus states no experimental studies directly manipulating social media for this. Vanhoffelen et al. 2025 longitudinal study showed weak evidence).\nYes (Surveys by Danielsen et al. 2024, Simon et al. 2022, Harren et al. 2021).\n19. Social media increases relational aggression among adolescent girls…\n~2-3%\n~5-7%\nNo (Consensus states no experimental evidence for SM use -> increased RA. Martins & Weaver meta-analysis included experiments on “exposure to RA -> RA behavior,” not direct SM manipulation).\nYes (Marinoni et al. 2023 on cyberbullying vulnerability).\n20. Among adolescent girls, social media increases exposure to other people displaying or discussing their mental disorders.\n~3-5%\n~15-20%\nNo (Consensus states no direct causal link references. Cited studies are observational, e.g., phenomena on TikTok, Twitter disclosures).\nYes (Alho et al. on peer networks; Hamilton et al. on self-harm content exposure).\n21. Social media increases sexual predation and harassment of adolescent girls…\n~1-2%\n~10-15%\nPotentially (Bhuller et al. 2013 quasi-experimental study on general internet access and sex crimes is a natural experiment, but not specific to SM mechanisms for predation). Consensus states no SM-specific experimental studies.\nYes (Bratton et al. 2020 on Facebook use and online harassment).\n24. If most parents waited… it would benefit the mental health of adolescents overall.\n~2-3%\n~5-8%\nNo (Cited studies are correlational/longitudinal. No experimental field studies manipulating delayed access).\nYes (Dempsey et al., Vaterlaus et al., Gerosa et al., Sapien Labs, Sun 2023, all observational).\n25. Imposing (and enforcing) a legal minimum age of 16… would benefit the mental health of adolescents overall.\n~1-2%\n~5-7%\nNo (Consensus states no strictly related references. Cited experimental work like Ferguson 2024 is on temporary abstention, not age limits).\nYes (General research linking SM to decreased MH like Twenge & Campbell 2018 cited as rationale, but it’s not testing the policy).\n26. Phone-free schools would benefit the mental health of adolescents overall.\n~5-7%\n~10-15%\nYes (Studies of school phone bans cited in Böttger & Zierer 2024 meta-analysis, King et al. 2024, Campbell et al. 2024 review, Goodyear et al. 2025 meta-analysis are natural experiments in field settings).\nNo (The studies cited are evaluations of bans, not general observational studies of phone use in schools).\nNotes on Estimation from Figure 3:\nThe percentages for “% Causal Field” and “% Causal Field or Higher” are visual estimations from the provided stacked bar chart (Figure 3, p. 11 of the PDF). They are approximate due to the nature of reading from a graph.\nKey Revisions and Observations:\nThe table now includes an estimated “% Causal Field” directly from the figure.\nThe assessment specifically looks for experimental field studies, natural experiments in the field, or well-controlled designs in the field that Section S5 cites as supporting the causal claim.\nAn additional column notes if Section S5 cites observational studies as evidence for the causal direction (often longitudinal or strong associational data).\nFor many claims, especially those about “Social media increases X,” direct experimental field evidence cited in S5 is often lacking, even if observational data or lab experiments are mentioned. The expert ratings for “Causal Field” for these claims often seem to reflect a broader interpretation or knowledge base beyond what’s explicitly detailed as experimental field research in the S5 summary.\nThe policy-related claims (#24, #25) still show a lack of direct experimental field testing of the policies themselves, though #26 (phone-free schools) has natural experiment data.\nResponse\nOkay, this is a detailed task. I will go through each of the identified causal claims, extract the expert assessment of evidence levels, and then analyze the studies cited in the consensus statement for that claim to assess their nature (causally informative, field, replication) and identify any conflicts.\nThe definition of “Field evidence” provided in the document (page 24) is: “Evidence collected in real-world, consequential settings using objective measures (e.g., official statistics in real-world settings, such as suicide rates; self-reported surveys are not field evidence).” This is a strict definition. “Causally informative” will generally refer to experimental studies (lab, field, or well-designed interventions).\nLet’s start with Claim 17 as requested, then proceed through the other causal claims.\nCausal Claims from Table 1:\n* General Causal Claims: 8, 9, 10, 11, 12, 13, 14, 15\n* Gender-related Causal Claims: 17, 18, 19, 20, 21\n* Policy Recommendation Causal Claims: 24, 25, 26\nClaim 17. Social media increases visual social comparisons among adolescent girls. (Consensus statement: p. 98-100)\nExpert Assessment of Evidence Level (N=96):\nNo evidence: 14.58%\nCorrelational: 57.29%\nCausal limited (lab, surveys, online, self-reported): 44.79%\nCausal Field: 11.46%\nReplicated Causal Field: 12.50%\nWide-scale Causal: 8.33%\n\nAnalysis of Cited Studies in Consensus Statement for Claim 17:\nSaiphoo & Vahedi (2019): Meta-analysis of 63 samples. Found social media use associated with decreased body satisfaction. Gender not a significant moderator. (This is correlational in nature for the primary studies it likely aggregates, though meta-analyses can include experimental work).\nFioravanti et al. (2022): Systematic review of experimental studies. Indicated idealized images increased body dissatisfaction, with appearance comparison as a mediator. (Causally informative - experimental).\nCouture Bue (2020): Eye-tracking study. Showed Instagram use predicted greater attention to body regions associated with anxiety, effect mediated by appearance comparisons and body dissatisfaction. (Observational/correlational, though eye-tracking provides objective attention measures).\nGoodyear et al. (2022): Qualitative studies. Highlights how girls use social media to compare bodies. (Descriptive, not primarily causal).\nFardouly et al. (2015): Experimental study. Assigned young women to browse Facebook, magazines, or websites. Found no significant Facebook effect on body dissatisfaction overall, but identified a moderating role of upward social comparison (those high in it showed increased discrepancy after Facebook). (Causally informative - experimental).\nKleemans et al. (2018): Experimental study. Exposed adolescent girls to edited Instagram content. Girls with higher comparison tendency were more strongly affected. (Causally informative - experimental).\nTiggemann & Anderberg (2020): Experimental study. Viewing “Instagram vs. reality” content reduced body dissatisfaction compared to idealized images. Appearance comparison did not significantly vary across conditions. (Causally informative - experimental).\nFerguson (2018): Cited as a hypothesis that social media reminds women of body dissatisfaction. (Likely a review/theoretical piece, not primary causal data).\nRoberts et al. (2022): Stated as finding “social media contributes to body dissatisfaction, beyond traditional media.” The nature of “contributes” is key; if based on predictive modeling of non-experimental data, it’s correlational.\nFardouly et al. (2017): Found upward comparisons on social media are associated with worse body image outcomes compared to in-person or traditional media. (Correlational).\n\nAssessment of Cited Studies:\nCausally Informative: Fioravanti et al. (2022 review of experiments), Fardouly et al. (2015 experiment), Kleemans et al. (2018 experiment), Tiggemann & Anderberg (2020 experiment). These are primarily lab/online experiments.\nField Studies (as per document definition): None of the cited experimental studies appear to be field studies under the strict definition (real-world, consequential settings, objective measures like official statistics).\nDirect Replications: The experimental studies address similar constructs but use different specific manipulations and measures; they are not direct replications of each other.\n\nConflict Assessment:\nThe expert assessment of “Causal limited” (44.79%) is well-supported by the citation of several experimental studies.\nThe expert ratings for “Causal Field” (11.46%), “Replicated Causal Field” (12.50%), and “Wide-scale Causal” (8.33%) seem less directly supported by the explicitly described experimental studies in the consensus text, as these are mostly lab/online experiments, not field studies.\nThe consensus statement concludes that evidence suggests an association between social media use and body dissatisfaction, with the role of increased visual social comparison as the direct causal link still under consideration and likely moderated. This aligns with the mixed findings from the experimental studies regarding the specific mechanism of visual social comparison.\n\nNow, for the other causal claims:\nClaim 8. Heavy daily use of smartphones and social media can cause sleep deprivation. (Consensus statement: p. 77-79)\nExpert Assessment of Evidence Level (N=105):\nNo evidence: 4.76%\nCorrelational: 64.76%\nCausal limited: 57.14%\nCausal Field: 20.00%\nReplicated Causal Field: 8.57%\nWide-scale Causal: 9.52%\n\nAnalysis of Cited Studies:\nCorrelational: Alonzo et al. (2021), Cheng et al. (2024), Alimoradi et al. (2019), Orben & Przybylski (2020), Gjoneska et al. (2022), Brosnan et al. (2024), Ahmed et al. (2024 meta-analysis of largely correlational studies), Sumter et al. (2024).\nLongitudinal (observational): Rod et al. (2018), Liu et al. (2019).\nIntervention studies (reducing screen time): Martin et al. (2020), Perrault et al. (2019), Tu et al. (2023). (Causally informative). These could be considered field experiments depending on setting and measures.\nLaboratory experiments (biological mechanisms): Chang et al. (2015) - light emission. (Causally informative).\nMeta-analysis of interventions: Schechter et al. (2022) - blue light reduction. (Causally informative).\nConsensus panel review: Hartstein et al. (2024) - reviewed experimental and intervention studies. (Summarizes causally informative work).\n\nAssessment of Cited Studies:\nCausally Informative: Martin et al. (2020), Perrault et al. (2019), Tu et al. (2023), Chang et al. (2015), Schechter et al. (2022), Hartstein et al. (2024 review).\nField Studies: The intervention studies (Martin, Perrault, Tu) could qualify if conducted in naturalistic settings with objective sleep measures (though some might use self-report). The consensus text doesn’t detail their methodology to confirm field status per the document’s definition.\nDirect Replications: The intervention studies are conceptually similar but likely not direct replications.\n\nConflict Assessment:\nThe high expert rating for “Correlational” (64.76%) and “Causal limited” (57.14%) is well-supported by the cited literature. The “Causal Field” (20.00%) rating is plausible given the intervention studies.\nNo major conflict. The consensus accurately reflects a mix of correlational and some experimental/interventional evidence. The conclusion that the extent to which it specifically causes sleep deprivation (vs. general sleep problems) is unclear seems justified.\n\nClaim 9. Chronic sleep deprivation can cause a decline in mental health. (Consensus statement: p. 81)\nExpert Assessment of Evidence Level (N=100):\nNo evidence: 1.00%\nCorrelational: 44.44% (Note: this should be “Correlational” as per the question options for causal claims, the text lists it as 44.00% without the label but it’s the first option after “No evidence”).\nCausal limited: 45.00%\nCausal field: 26.00%\nReplicated causal field: 30.00%\nWide-scale causal: 44.00%\n\nAnalysis of Cited Studies:\nPires et al. (2016): Sleep deprivation shown to increase state anxiety. (Likely experimental if “shown to increase”).\nRoberts & Duong (2014): Six or fewer hours of sleep predicts higher depression. (Longitudinal observational, predictive but not strictly experimental manipulation of sleep deprivation).\nLiu et al. (2020): Review of 41 longitudinal studies; sleep disturbances predictors of suicidal ideation. (Summarizes observational longitudinal work).\nScott et al. (2021): Meta-analysis of sleep-improvement interventions. Improving sleep had positive effect on mental health. (Causally informative).\nShort et al. (2022): Highlights vulnerability of adolescents. (Likely review/conceptual).\nMcEwen & Lasley (2002): Biological mechanisms – sleep deprivation disrupts brain areas. (Reviews of mechanistic studies, some of which could be experimental).\n\nAssessment of Cited Studies:\nCausally Informative: Scott et al. (2021 meta-analysis of interventions), Pires et al. (2016, likely experimental). Mechanistic studies discussed by McEwen & Lasley (2002) could also be experimental.\nField Studies: The interventions reviewed by Scott et al. might include field studies.\nDirect Replications: Unclear from the summary.\n\nConflict Assessment:\nThe expert ratings are quite high across “Causal limited” (45%), “Replicated causal field” (30%), and “Wide-scale causal” (44%).\nThe cited evidence strongly supports a causal link, especially through the meta-analysis of interventions (Scott et al.). The biological mechanism work also supports causality.\nThe consensus is strong. There isn’t a clear conflict; the evidence cited (especially interventions) supports the higher-level causal inferences made by experts.\n\nClaim 10. Heavy daily use of smartphones and social media can cause attention fragmentation. (Consensus statement: p. 82-83)\nExpert Assessment of Evidence Level (N=88):\nNo evidence: 9.09%\nCorrelational: 63.64%\nCausal limited: 57.95%\nCausal field: 9.09%\nReplicated causal field: 5.68%\nWide-scale causal: 7.95%\n\nAnalysis of Cited Studies:\nSantos et al. (2022): Systematic review; excessive screen time in children can be associated with attention problems. (Reviews largely correlational work).\nBarry et al. (2017): Observed an association between number of social media accounts and DSM-5 inattention symptoms. (Correlational).\nLongitudinal studies on smartphone use and ADHD symptoms: George et al. (2017) - association; Ra et al. (2018) - high social media use associated with later ADHD symptoms; Deng et al. (2024) - online social activities linked to ADHD symptoms. (Observational longitudinal).\nStudies on “mere presence” of smartphone: Ward et al. (2017), Skowronek et al. (2023) - presence could reduce cognitive capacity. (Experimental, causally informative for “mere presence” not “heavy use”).\nParry (2024): Meta-analysis; mere presence negatively impacted working memory. (Reviews experimental work on “mere presence”).\nvan Wezel et al. (2021): Experimental study (reducing social media); no effect on attention (manipulation failed). (Causally informative, but with null/inconclusive result due to design issue).\n\nAssessment of Cited Studies:\nCausally Informative: Ward et al. (2017), Skowronek et al. (2023), Parry (2024 meta-analysis of these types of studies), van Wezel et al. (2021). These mostly relate to “mere presence” or effects of reduction, not directly “heavy use causes fragmentation.”\nField Studies: van Wezel et al. was an intervention that could be a field study.\nDirect Replications: Not apparent.\n\nConflict Assessment:\nExperts rate “Correlational” (63.64%) and “Causal limited” (57.95%) high.\nThe cited literature is predominantly correlational or observational longitudinal, especially regarding “heavy use.” The experimental evidence cited (Ward, Skowronek, Parry) focuses on the “mere presence” effect, which is different from “heavy daily use.”\nThe consensus statement itself notes the definitional issues with “attention fragmentation” vs. “inattention/ADHD.” The conclusion that “the evidence is not robust enough to definitively confirm a causal link” for heavy use causing attention fragmentation seems justified given the cited studies. The expert rating for “Causal limited” might be slightly generous if strictly applied to “heavy use causing fragmentation” as opposed to related concepts.\n\nClaim 11. Attention fragmentation can cause a decline in mental health (possibly through mediating factors such as its negative impact on social relationships). (Consensus statement: p. 84-85)\nExpert Assessment of Evidence Level (N=54):\nNo evidence: 29.62%\nCorrelational: 48.15%\nCausal limited: 25.93%\nCausal field: 7.41%\nReplicated causal field: 7.41%\nWide-scale causal: 5.55%\n\nAnalysis of Cited Studies: (The statement notes “attention fragmentation” is not well-established, so it discusses related constructs).\nMind-wandering: Killingsworth & Gilbert (2010) - mind-wandering associated with unhappiness (correlational). Smallwood & Schooler (2015) - review. Poerio et al. (2013), Welz et al. (2018) - null or positive effects of mind-wandering (correlational).\nCognitive load: Mizuno et al. (2011), Hawthorne et al. (2019) - link between cognitive load and negative well-being (correlational). Hu et al. (2021) - cognitive load may increase well-being in some cases (potentially experimental or quasi-experimental).\nMultitasking: Becker et al. (2013), Shin et al. (2024) - media multitasking associated with higher depression/anxiety (correlational). Hatchel et al. (2018) - associated with negative affect/low self-esteem (correlational).\nIntervention studies (disabling notifications): Fitz et al. (2019), Olson et al. (2023) - positive effects on well-being (causally informative). Dekker et al. (2024), Liao et al. (2022) - drawbacks like increased checking/anxiety (causally informative, but for a different outcome).\n\nAssessment of Cited Studies:\nCausally Informative: Fitz et al. (2019), Olson et al. (2023), Dekker et al. (2024), Liao et al. (2022) – these are interventions on notifications, indirectly related to fragmentation. Hu et al. (2021) might be.\nField Studies: The intervention studies might qualify.\nDirect Replications: No.\n\nConflict Assessment:\nExpert ratings are highest for “Correlational” (48.15%) and relatively low for causal levels. “No evidence” is also high (29.62%).\nThe cited literature is heavily correlational for the link between related constructs (mind-wandering, cognitive load, multitasking) and mental health. The experimental evidence on interventions (disabling notifications) is mixed and indirect.\nThe consensus statement’s conclusion that “evidence regarding the effect of attention fragmentation on mental health is preliminary” aligns well with the cited studies and the expert assessment. No major conflict.\n\nClaim 12. Heavy daily use of smartphones and social media can cause behavioral addiction. (Consensus statement: p. 87-88)\nExpert Assessment of Evidence Level (N=94):\nNo evidence: 9.57%\nCorrelational: 64.89%\nCausal limited: 48.93%\nCausal field: 19.15%\nReplicated causal field: 10.64%\nWide-scale causal: 11.70%\n\nAnalysis of Cited Studies:\nTruzoli et al. (2023): Social media users with high Internet Addiction Test scores reported lower distress after ceasing internet session (consistent with mobile device use as sedative). (Correlational/observational).\nAnderson & Wood (2023): Frequent social media users show blunted sensitivity to social rewards (habituation). (Observational, potentially experimental if reward sensitivity was tested).\nAllcott et al. (2022): Field study suggesting social media usage may involve self-control problems and be habit-forming. (Causally informative – likely an intervention or natural experiment).\nTikTok internal study (NPR, 2024): 35 mins of videos sufficient to develop a habit. (Internal industry data, nature of study unclear but suggests rapid habit formation).\nNeuroimaging studies (He et al., 2017; Montag et al., 2017): Parallels in brain activity between problematic smartphone use and other behavioral addictions. (Correlational/Observational).\nBooks on dopamine/addiction (Courtwright, 2019; Lembke, 2021). Design features (Schüll, 2012; Sherman et al., 2016; Lindström et al., 2021). (Theoretical/Mechanistic arguments).\n\nAssessment of Cited Studies:\nCausally Informative: Allcott et al. (2022) is explicitly called a field study and its findings (self-control, habit) are causally suggestive.\nField Studies: Allcott et al. (2022).\nDirect Replications: No.\n\nConflict Assessment:\nExperts rate “Correlational” (64.89%) and “Causal limited” (48.93%) high. “Causal field” is 19.15%.\nThe consensus statement itself notes “no experts provided experimental evidence about this claim” and “experimental evidence is virtually absent.” This seems to contradict the mention of Allcott et al. (2022) as a field study suggesting habit formation. However, “habit formation” and “self-control problems” are not identical to “causing behavioral addiction” as a clinical disorder.\nThere’s a slight tension: experts rate some level of causal evidence, one field study with suggestive findings is cited, but the consensus text emphasizes the absence of direct experimental proof for addiction as a disorder. The conclusion that “preliminary correlational evidence” exists but “experimental evidence is virtually absent” is largely accurate for addiction as a formally recognized disorder, even if related phenomena like habit formation have some experimental backing.\n\nClaim 13. Behavioral addiction can cause a decline in mental health. (Consensus statement: p. 90)\nExpert Assessment of Evidence Level (N=81):\nNo evidence: 12.35%\nCorrelational: 50.62%\nCausal limited: 41.96%\nCausal field: 19.75%\nReplicated causal field: 23.46%\nWide-scale causal: 34.57%\n\nAnalysis of Cited Studies:\nGrant et al. (2010): Review; behavioral addictions share similarities with substance addictions (co-occurring disorders).\nPetry et al. (2005): High comorbidity between pathological gambling and psychiatric disorders (depression, anxiety). (Cross-sectional/Correlational).\nKo et al. (2009): Linked excessive online gaming to depression, anxiety, social phobia. (Likely correlational).\nHartmann & Blaszczynski (2018): Gambling problems can lead to increased depression/anxiety over time. (Longitudinal, suggesting directionality).\nMarchant et al. (2017): Review; linking “internet-related problematic behaviors” to self-harm/suicidal behaviors, but direction of causality unclear.\nAlimoradi et al. (2024): Meta-analysis; problematic internet-related behaviors consistently associated with mental health issues. (Aggregates largely correlational data).\n\nAssessment of Cited Studies:\nCausally Informative: Hartmann & Blaszczynski (2018) is longitudinal and suggests directionality, which is a step towards causal inference but not a manipulated experiment. Most others are correlational or reviews of correlational work. The claim is broad (“behavioral addiction”), and some of these studies (gambling) have a longer history of research than, say, social media addiction.\nField Studies: Not apparent from summaries.\nDirect Replications: No.\n\nConflict Assessment:\nExperts rate “Correlational” (50.62%) and “Causal limited” (41.96%) highest, with “Wide-scale causal” also notably high (34.57%).\nThe literature cited is largely based on associations, comorbidities, and some longitudinal data (gambling). The consensus states the claim is “true by definition” for DSM/ICD, which is a strong point.\nThe conclusion that “the direction of the causal relationship remains unclear” (despite the definitional point) reflects the limitations of most primary research (often correlational) vs. the clinical definitions. No major conflict, but the high “Wide-scale causal” rating might be more based on the definitional aspect and long-standing clinical observation for established addictions (like gambling) rather than rigorous experimental proof for all behavioral addictions causing mental health decline.\n\nClaim 14. Heavy daily use of smartphones and social media can cause social deprivation, such as isolation and lack of formative social experiences. (Consensus statement: p. 92-93)\nExpert Assessment of Evidence Level (N=83):\nNo evidence: 13.25%\nCorrelational: 63.86%\nCausal limited: 42.17%\nCausal field: 14.46%\nReplicated causal field: 9.64%\nWide-scale causal: 8.43%\n\nAnalysis of Cited Studies:\nMeta-analyses on social media & loneliness: Appel et al. (2020), Liu & Baumeister (2016) - found positive correlation. Cheng et al. (2019) - largest meta-analysis, no overall association.\nGeraci et al. (2022): Quasi-experimental; higher connection speeds/internet use can lead to reduced social engagement. (Causally suggestive).\nAllcott et al. (2020): Experimental (deactivating Facebook); results in more in-person socialization but also more solitary TV. (Causally informative).\nPew Research (Anderson & Jiang, 2018): Survey; 81% teens feel more connected via social media. (Correlational, self-report).\nSteinsbekk et al. (2024): Social media use associated with more time with friends offline. (Correlational/Longitudinal).\nStudies on benefits for LGBTQ+ youth (Berger et al., 2022; Paceley et al., 2022) or students with disabilities.\nWang et al. (2018): Longitudinal; U-shaped relationship between active Facebook use and loneliness.\nDisplacement hypothesis studies (Verduyn et al., 2021; Dienlin et al., 2017; Hall et al., 2019a, 2019b) - mixed evidence, social media may displace other entertainment not just social interaction.\n\nAssessment of Cited Studies:\nCausally Informative: Geraci et al. (2022, quasi-experimental), Allcott et al. (2020, experimental).\nField Studies: Allcott et al. (2020) was a field experiment.\nDirect Replications: No.\n\nConflict Assessment:\nExperts rate “Correlational” (63.86%) and “Causal limited” (42.17%) high.\nThe cited literature is mixed: correlational studies go both ways, meta-analyses are mixed, and a couple of experimental/quasi-experimental studies show some causal effects (both positive and negative for social connection aspects).\nThe consensus conclusion that the “strength and even the direction of the potential causal relationship…likely depends on various…factors” and that future work should explore moderators aligns well with the mixed evidence and expert ratings. No major conflict.\n\nClaim 15. Chronic social deprivation can cause a decline in mental health. (Consensus statement: p. 95)\nExpert Assessment of Evidence Level (N=91):\nNo evidence: 1.10%\nCorrelational: 49.45%\nCausal limited: 51.65%\nCausal field: 32.97%\nReplicated causal field: 27.47%\nWide-scale causal: 37.36%\n\nAnalysis of Cited Studies:\nNational health surveys (NHS, Newlove-Delgado et al., 2022): report associations between social deprivation and poor mental health. (Correlational).\nCacioppo et al. (2006): Longitudinal; demonstrated increased depression in socially isolated participants. (Observational longitudinal).\nDickerson & Kemeny (2004): Social-evaluative stressors associated with cortisol levels. (Likely review of experimental and correlational work).\nField studies/Natural experiments (COVID-19): Killgore et al. (2020) - analyzed mental health data across lockdown, found increased depression/suicidal ideation associated with extended social isolation. (Natural experiment, observational).\nExperimental studies: Alvarez et al. (2024), Costello et al. (2022), Lai et al. (2020) - increasing social interactions improves mental health outcomes. (Causally informative - interventions).\nHolt-Lunstad (2024): Review; social connections a key factor.\n\nAssessment of Cited Studies:\nCausally Informative: Alvarez et al. (2024), Costello et al. (2022), Lai et al. (2020) are intervention studies. The natural experiment by Killgore et al. (2020) is also causally suggestive.\nField Studies: The interventions might qualify. Killgore et al. is a natural field study.\nDirect Replications: Not apparent.\n\nConflict Assessment:\nExperts rate “Causal limited” (51.65%) high, with substantial ratings for “Causal field” (32.97%) and “Wide-scale causal” (37.36%).\nThe cited literature includes strong correlational, longitudinal, natural experimental, and intervention studies. The consensus itself notes the converging evidence but points out the ethical impossibility of depriving people of social interaction in controlled experiments for this specific claim.\nThe statement’s conclusion that “experts agreed that the evidence supports the claim” is consistent. The evidence is strong, even if direct experimental deprivation isn’t ethically feasible. No major conflict.\n\nClaim 18. Social media increases perfectionism among adolescent girls. (Consensus statement: p. 101-102)\n* Expert Assessment of Evidence Level (N=72):\n* No evidence: 23.61%\n* Correlational: 63.89%\n* Causal limited: 27.78%\n* Causal field: 9.72%\n* Replicated causal field: 5.55%\n* Wide-scale causal: 5.55%\nAnalysis of Cited Studies:\nDanielsen et al. (2024): Survey; upward social comparison associated with higher perfectionism (boys & girls). (Correlational).\nSimon et al. (2022): Survey; self-reported Instagram addiction positively correlated with concerns about physical imperfections. (Correlational).\nHarren et al. (2021): Survey; problematic Instagram/Facebook uses linked to perfectionism. (Correlational).\nVanhoffelen et al. (2025): Longitudinal study; weak evidence that exposure to superior individuals on social media increases perfectionism. (Observational longitudinal).\nBroader research on rising perfectionism (Curran & Hill, 2019) - not specific to social media.\n\nAssessment of Cited Studies:\nCausally Informative: The consensus states “no evidence from experimental studies directly manipulating social media exposure.” Vanhoffelen et al. (2025) is longitudinal, which offers more than cross-sectional but isn’t experimental.\nField Studies: None cited.\nDirect Replications: No.\n\nConflict Assessment:\nExperts rate “Correlational” (63.89%) highest, with “Causal limited” at 27.78%. “No evidence” is also substantial (23.61%).\nThe cited primary research specific to social media and perfectionism is correlational or longitudinal with weak findings. The consensus explicitly states the lack of experimental evidence.\nThe conclusion that “social media may be associated with perfectionism” and that future studies should “explore potential causal links” aligns perfectly with the cited evidence and expert assessments. No conflict.\n\nClaim 19. Social media increases relational aggression among adolescent girls, for example by providing tools for cyberbullying and exclusion. (Consensus statement: p. 103-104)\nExpert Assessment of Evidence Level (N=71):\nNo evidence: 19.72%\nCorrelational: 69.01%\nCausal limited: 30.99%\nCausal field: 8.45%\nReplicated causal field: 1.41%\nWide-scale causal: 4.23%\n\nAnalysis of Cited Studies:\nMarinoni et al. (2023): Found girls more vulnerable to cyberbullying/victimization than boys during pandemic. (Correlational, specific context).\nMartins & Weaver (2019): Meta-analysis (surveys, longitudinal, experimental); exposure to relational aggression strongly predicted relationally aggressive behaviors. (This meta-analysis includes experimental studies, making it causally informative for the “exposure to RA -> RA behavior” link, but not directly “social media use -> increases RA”).\nKvardova et al. (2025): Higher pro-social social media use positively related to increased prosocial behaviors. (Correlational, opposite direction for prosocial).\nHinduja & Patchin (2014): Argues social media features may amplify relational aggression. (Theoretical/Argumentative).\nLapidot-Lefler et al. (2012): Lab study (adults); lack of eye contact can contribute to disinhibition/toxic behavior. (Experimental, but on adults and general toxic behavior, not specific to social media or relational aggression in girls).\n\nAssessment of Cited Studies:\nCausally Informative: Martins & Weaver (2019) meta-analysis includes experimental studies, but the link is “exposure to RA -> RA behaviors”. Lapidot-Lefler et al. (2012) is experimental but indirect. The consensus notes “no experts provided experimental evidence establishing a causal link between social media use and increased relational aggression.”\nField Studies: None cited directly.\nDirect Replications: No.\n\nConflict Assessment:\nExperts rate “Correlational” (69.01%) highest, with “Causal limited” (30.99%) next.\nThe primary evidence cited is largely correlational or indirect. The experimental evidence from Martins & Weaver is about exposure to RA, not social media causing RA.\nThe consensus conclusion that “the evidence that social media use increases relational aggression among adolescent girls is preliminary” is well-supported. No major conflict.\n\nClaim 20. Among adolescent girls, social media increases exposure to other people displaying or discussing their mental disorders. (Consensus statement: p. 105-106)\nExpert Assessment of Evidence Level (N=72):\nNo evidence: 18.06%\nCorrelational: 69.44%\nCausal limited: 27.78%\nCausal field: 12.50%\nReplicated causal field: 9.72%\nWide-scale causal: 8.33%\n\nAnalysis of Cited Studies:\nAlho et al. (2024): Mental disorders may be socially transmitted in adolescent peer networks (classmates). (Observational, not specific to social media).\nStudies on tic-like behaviors spreading on TikTok: Frey et al. (2022), Giedinghagen (2023), Haltigan et al. (2023), Hull et al. (2021), Olvera et al. (2021). (Observational, case reports, descriptive of a phenomenon).\nErnala et al. (2018): Research on Twitter suggests reciprocal behavior in self-disclosure of mental illness. (Observational/Content analysis).\nHamilton et al. (2025): Exposure to self-harm content associated with greater likelihood of urges/engaging in self-harm. (Correlational/Longitudinal).\nAli et al. (2024): Analyzed DMs about self-harm/suicide on Instagram; found significant helping behavior. (Observational/Content analysis, counter-example).\n\nAssessment of Cited Studies:\nCausally Informative: The consensus states “no experts provided references for a direct causal link.” The studies are observational, content analyses, or describe phenomena (tic-like behaviors).\nField Studies: None in the experimental sense.\nDirect Replications: No.\n\nConflict Assessment:\nExperts rate “Correlational” (69.44%) highest, with “Causal limited” (27.78%) next.\nThe cited literature describes associations and observed phenomena but does not establish social media causes increased exposure in an experimental sense (though it’s highly plausible by nature of the platforms).\nThe conclusion that future studies should “explore potential causal links” and notes the lack of direct causal link evidence is consistent. No major conflict.\n\nClaim 21. Social media increases sexual predation and harassment of adolescent girls, for example by providing predators with access to potential victims. (Consensus statement: p. 107-108)\nExpert Assessment of Evidence Level (N=59):\nNo evidence: 28.81%\nCorrelational: 52.54%\nCausal limited: 13.56%\nCausal field: 5.08%\nReplicated causal field: 3.39%\nWide-scale causal: 11.86%\n\nAnalysis of Cited Studies:\nCommon Sense Media survey (Nesi et al., 2023): Girls report being contacted by strangers. (Self-report survey).\nJournalistic investigations (WSJ, 2024a, 2023): Documenting harassment and platforms facilitating illegal behavior.\nInternal Meta survey (Instagram, 2021), Snap study (WSJ, 2024b): Internal industry data on unwanted advances/sextortion.\nBratton et al. (2020): Correlational study; association between Facebook use and online harassment.\nBhuller et al. (2013): Quasi-experimental; increased internet access associated with rise in sex crimes (linked to pornography). (Causally suggestive for general internet, not specific social media mechanisms for predation).\n\nAssessment of Cited Studies:\nCausally Informative: The consensus states “no experts provided experimental studies manipulating social media usage and measuring its effect on harassment or sexual predation.” Bhuller et al. (2013) is quasi-experimental for general internet access.\nField Studies: Bhuller et al. could be considered a natural field experiment.\nDirect Replications: No.\n\nConflict Assessment:\nExperts rate “Correlational” (52.54%) highest. “No evidence” is also high (28.81%). Causal ratings are low.\nThe cited evidence consists of surveys, journalistic reports, internal industry data, and one quasi-experimental study on general internet access. There’s a lack of direct experimental evidence on social media causing increased predation.\nThe conclusion to “investigate whether a causal link exists” is appropriate. No major conflict.\n\nClaim 24. If most parents waited until their children were in high school to give them their first smartphones, it would benefit the mental health of adolescents overall. (Consensus statement: p. 112-113)\nExpert Assessment of Evidence Level (N=93):\nNo evidence: 50.68%\nCorrelational: 39.73%\nCausal limited: 26.03%\nCausal field: 8.22%\nReplicated causal field: 2.74%\nWide-scale causal: 5.48%\n\nAnalysis of Cited Studies:\nDempsey et al. (2020): Found no significant links between early mobile phone ownership and psychosocial outcomes. (Correlational/Longitudinal).\nVaterlaus et al. (2021): Age of first smartphone had little to no predictive value for later well-being. (Correlational/Longitudinal).\nGerosa et al. (2024): Contrasting relationships by gender; early ownership positive for girls’ life satisfaction, negative for boys. (Correlational/Longitudinal).\nSapien Labs (2024): Large survey; mental health outcomes associated with later age of first smartphone/tablet, more pronounced for females. (Correlational).\nSun (2023): Longitudinal; no significant associations between age of phone acquisition and depression, academic performance, sleep. (Longitudinal).\n\nAssessment of Cited Studies:\nCausally Informative: None of the cited studies are experimental manipulations of delaying smartphone access. They are all observational/longitudinal.\nField Studies: No.\nDirect Replications: No.\n\nConflict Assessment:\nExperts rate “No evidence” (50.68%) highest, followed by “Correlational” (39.73%). Causal ratings are low.\nThe cited literature is entirely correlational/longitudinal with mixed findings.\nThe consensus conclusion that “the evidence is insufficient to draw conclusions about this claim” is well-supported. No conflict.\n\nClaim 25. Imposing (and enforcing) a legal minimum age of 16 for opening social media accounts would benefit the mental health of adolescents overall. (Consensus statement: p. 114-115)\nExpert Assessment of Evidence Level (N=68):\nNo evidence: 70.59%\nCorrelational: 26.47%\nCausal limited: 10.29%\nCausal field: 2.94%\nReplicated causal field: 2.94%\nWide-scale causal: 5.88%\n\nAnalysis of Cited Studies:\nThe consensus states: “Experts provided no references that were strictly related to the claim”.\nIt then cites general research linking social media to decreased mental health (Twenge & Campbell, 2018; Orben et al., 2022).\nFerguson (2024) meta-analysis: No significant mental health differences between experimental abstention and control. (Causally informative, but about temporary abstention, not age limits).\nThrul et al. (2025) re-analysis: Duration of reduction matters. (Methodological re-evaluation of existing studies).\n\nAssessment of Cited Studies:\nCausally Informative: The consensus explicitly states no direct evidence was provided for this specific policy. Ferguson (2024) and Thrul et al. (2025) are about effects of reducing use, not delaying initial access via age limits.\nField Studies: No.\nDirect Replications: No.\n\nConflict Assessment:\nExperts rate “No evidence” (70.59%) overwhelmingly highest. All other ratings are very low.\nThis aligns perfectly with the consensus statement noting the lack of direct evidence. The discussion appropriately shifts to practical/legal/ethical issues and alternative approaches due to this evidentiary gap. No conflict.\n\nClaim 26. Phone-free schools would benefit the mental health of adolescents overall. (Consensus statement: p. 116-117)\nExpert Assessment of Evidence Level (N=77):\nNo evidence: 28.57%\nCorrelational: 44.16%\nCausal limited: 25.97%\nCausal field: 16.88%\nReplicated causal field: 2.60%\nWide-scale causal: 11.69%\n\nAnalysis of Cited Studies:\nBöttger & Zierer (2024): Meta-analysis of 5 studies; small significant overall effect, notable benefits in social well-being (bullying). (Synthesizes existing, likely mixed-methodology studies).\nKing et al. (2024): Pre-registered study (Australia); no meaningful changes in problematic phone use, engagement, belonging after phone ban. (Observational, natural experiment).\nCampbell et al. (2024): Scoping review; highlighted limitations, lack of RCTs. Small majority of studies reported positive effects on bullying/cyberbullying, similar small majority found no significant impact on mental health/well-being.\nGoodyear et al. (2025): Meta-analysis; comparing restrictive vs. permissive policies, no significant differences in mental health/performance (but only 4/20 schools fully implemented phone-free).\n\nAssessment of Cited Studies:\nCausally Informative: Studies included in these reviews/meta-analyses are likely observational or quasi-experimental (natural experiments of school bans). King et al. (2024) is a natural experiment. The consensus notes a lack of RCTs.\nField Studies: School ban studies are inherently field studies.\nDirect Replications: No, methodologies vary.\n\nConflict Assessment:\nExperts rate “Correlational” (44.16%) highest, with “No evidence” (28.57%) and “Causal limited” (25.97%) also substantial.\nThe cited reviews and studies show mixed and limited evidence. Methodological issues abound.\nThe consensus conclusion that “available evidence is too limited and inconsistent to draw conclusions” is strongly supported by the nature of the cited research and the expert ratings. No conflict.\n\nYou are absolutely correct. Claims like “Social media increases…” are indeed causal claims, even if they aren’t explicitly about causing a decline in mental health. I will re-evaluate with that in mind, focusing on the “Causal field” expert assessment for all identified causal claims and explicitly noting if any cited studies are experimental field studies.\nLet’s go through them again.\nClaim 8. Heavy daily use of smartphones and social media can cause sleep deprivation. (Consensus statement: p. 77-79)\nExpert Assessment of Evidence Level (N=105):\nCorrelational: 64.76%\nCausal limited: 57.14%\nCausal Field: 20.00%\nReplicated Causal Field: 8.57%\nWide-scale Causal: 9.52%\n\nAnalysis of Cited Studies (Focus on Experimental Field Studies):\nIntervention studies (reducing screen time): Martin et al. (2020), Perrault et al. (2019), Tu et al. (2023). These are described as showing that reducing screen time can improve sleep. If these interventions were conducted in participants’ natural environments (e.g., homes, not a sleep lab) and measured sleep objectively (e.g., actigraphy) or even through validated diaries over a period, they could qualify as experimental field studies. The consensus text doesn’t provide enough detail on their methodology to definitively categorize them as such per the strict definition, but they are the strongest candidates.\nLaboratory experiments (Chang et al., 2015) are not field studies.\nMeta-analyses (Schechter et al., 2022; Hartstein et al., 2024) might include experimental field studies, but the summary doesn’t break this down.\n\nExperimental Field Studies Present? Potentially, through the intervention studies (Martin, Perrault, Tu).\nExpert Assessment of “Causal Field” Evidence: 20.00% of experts rated the evidence as “Causal Field.” This is plausible if the intervention studies are considered as such.\nClaim 9. Chronic sleep deprivation can cause a decline in mental health. (Consensus statement: p. 81)\nExpert Assessment of Evidence Level (N=100):\nCorrelational: 44.44%\nCausal limited: 45.00%\nCausal field: 26.00%\nReplicated causal field: 30.00%\nWide-scale causal: 44.00%\n\nAnalysis of Cited Studies (Focus on Experimental Field Studies):\nScott et al. (2021): Meta-analysis of sleep-improvement interventions. Improving sleep had a positive effect on mental health. Some of these interventions could have been conducted in field settings (e.g., CBT-I delivered in a community setting with real-world mental health outcomes).\nPires et al. (2016) (likely experimental) and mechanistic studies (McEwen & Lasley, 2002) are more likely lab-based.\n\nExperimental Field Studies Present? Potentially, within the interventions reviewed by Scott et al. (2021).\nExpert Assessment of “Causal Field” Evidence: 26.00%. This aligns with the possibility that some interventions reviewed were field-based.\nClaim 10. Heavy daily use of smartphones and social media can cause attention fragmentation. (Consensus statement: p. 82-83)\nExpert Assessment of Evidence Level (N=88):\nCorrelational: 63.64%\nCausal limited: 57.95%\nCausal field: 9.09%\nReplicated causal field: 5.68%\nWide-scale causal: 7.95%\n\nAnalysis of Cited Studies (Focus on Experimental Field Studies):\nvan Wezel et al. (2021): Experimental study (reducing social media); no effect on attention (manipulation failed). This was an intervention in participants’ daily lives, so it could be considered an experimental field study.\nStudies on “mere presence” (Ward et al., 2017; Skowronek et al., 2023) are lab experiments.\n\nExperimental Field Studies Present? Yes, van Wezel et al. (2021) is a candidate.\nExpert Assessment of “Causal Field” Evidence: 9.09%. This is a low percentage, consistent with only one potential (and inconclusive) field experiment being cited.\nClaim 11. Attention fragmentation can cause a decline in mental health… (Consensus statement: p. 84-85)\nExpert Assessment of Evidence Level (N=54):\nCorrelational: 48.15%\nCausal limited: 25.93%\nCausal field: 7.41%\nReplicated causal field: 7.41%\nWide-scale causal: 5.55%\n\nAnalysis of Cited Studies (Focus on Experimental Field Studies):\nIntervention studies (disabling notifications): Fitz et al. (2019), Olson et al. (2023), Dekker et al. (2024), Liao et al. (2022). These interventions, if conducted in participants’ natural environments with real-world well-being measures, would be experimental field studies.\n\nExperimental Field Studies Present? Potentially, through the notification intervention studies.\nExpert Assessment of “Causal Field” Evidence: 7.41%. This is low, suggesting experts are cautious about the strength or directness of these field interventions for this specific broad claim.\nClaim 12. Heavy daily use of smartphones and social media can cause behavioral addiction. (Consensus statement: p. 87-88)\nExpert Assessment of Evidence Level (N=94):\nCorrelational: 64.89%\nCausal limited: 48.93%\nCausal field: 19.15%\nReplicated causal field: 10.64%\nWide-scale causal: 11.70%\n\nAnalysis of Cited Studies (Focus on Experimental Field Studies):\nAllcott et al. (2022): Explicitly called a field study suggesting social media usage may involve self-control problems and be habit-forming. This is an experimental field study (likely involving an intervention or random assignment in a real-world context).\n\nExperimental Field Studies Present? Yes, Allcott et al. (2022).\nExpert Assessment of “Causal Field” Evidence: 19.15%. This aligns with one prominent field experiment being cited.\nClaim 13. Behavioral addiction can cause a decline in mental health. (Consensus statement: p. 90)\nExpert Assessment of Evidence Level (N=81):\nCorrelational: 50.62%\nCausal limited: 41.96%\nCausal field: 19.75%\nReplicated causal field: 23.46%\nWide-scale causal: 34.57%\n\nAnalysis of Cited Studies (Focus on Experimental Field Studies):\nThe consensus mentions Hartmann & Blaszczynski (2018) showed gambling problems can lead to increased depression/anxiety over time. While longitudinal, it’s observational rather than an experimental field study manipulating addiction to see its effect on mental health. The claim is broad (“behavioral addiction”). It’s difficult to ethically conduct experimental field studies where addiction is induced to observe mental health decline. Treatment studies (interventions for addiction showing improved mental health) would be more common and could be field-based, but aren’t the primary focus of this claim’s wording.\n\nExperimental Field Studies Present? Unlikely for causing decline through manipulation of addiction itself in a field setting. Intervention studies for addiction that measure mental health outcomes in the field would be relevant but focus on reversal.\nExpert Assessment of “Causal Field” Evidence: 19.75%. This might reflect interpretations of strong longitudinal data or broader clinical field observations rather than direct experimental field manipulations of causing decline.\nClaim 14. Heavy daily use of smartphones and social media can cause social deprivation… (Consensus statement: p. 92-93)\nExpert Assessment of Evidence Level (N=83):\nCorrelational: 63.86%\nCausal limited: 42.17%\nCausal field: 14.46%\nReplicated causal field: 9.64%\nWide-scale causal: 8.43%\n\nAnalysis of Cited Studies (Focus on Experimental Field Studies):\nGeraci et al. (2022): Quasi-experimental study; higher connection speeds/internet use can lead to reduced social engagement. This could be a natural experiment in the field.\nAllcott et al. (2020): Experimental study (deactivating Facebook); results in more in-person socialization but also more solitary TV. This was a field experiment.\n\nExperimental Field Studies Present? Yes, Allcott et al. (2020) and potentially Geraci et al. (2022) as a natural field experiment.\nExpert Assessment of “Causal Field” Evidence: 14.46%. This is consistent with a couple of relevant studies.\nClaim 15. Chronic social deprivation can cause a decline in mental health. (Consensus statement: p. 95)\nExpert Assessment of Evidence Level (N=91):\nCorrelational: 49.45%\nCausal limited: 51.65%\nCausal field: 32.97%\nReplicated causal field: 27.47%\nWide-scale causal: 37.36%\n\nAnalysis of Cited Studies (Focus on Experimental Field Studies):\nKillgore et al. (2020): Analyzed mental health data across lockdown periods (COVID-19). This is a natural experiment in a field setting.\nExperimental studies (Alvarez et al., 2024; Costello et al., 2022; Lai et al., 2020): Increasing social interactions improves mental health. These are interventions. If conducted in real-world settings with community participants, they would be experimental field studies.\n\nExperimental Field Studies Present? Yes, Killgore et al. (2020) as a natural field experiment, and potentially the intervention studies.\nExpert Assessment of “Causal Field” Evidence: 32.97%. This aligns well with the presence of a significant natural experiment and other intervention studies that are likely field-based.\nClaim 17. Social media increases visual social comparisons among adolescent girls. (Consensus statement: p. 98-100)\nExpert Assessment of Evidence Level (N=96):\nCorrelational: 57.29%\nCausal limited: 44.79%\nCausal Field: 11.46%\nReplicated Causal Field: 12.50%\nWide-scale Causal: 8.33%\n\nAnalysis of Cited Studies (Focus on Experimental Field Studies):\nFardouly et al. (2015), Kleemans et al. (2018), Tiggemann & Anderberg (2020): These are described as experimental studies. They involve manipulating exposure to social media content (or similar) and measuring outcomes. These are generally lab or online experiments, not field studies according to the strict definition (real-world, consequential settings, objective measures like official statistics for the outcome of social comparison itself). The outcomes are typically self-reported comparison tendencies or body dissatisfaction.\n\nExperimental Field Studies Present? Unlikely, based on the descriptions. The experiments are more controlled (lab/online) settings.\nExpert Assessment of “Causal Field” Evidence: 11.46%. This percentage seems a bit high if the cited experimental studies are primarily lab/online. Experts might be interpreting “field” more broadly or considering other non-cited studies.\nClaim 18. Social media increases perfectionism among adolescent girls. (Consensus statement: p. 101-102)\nExpert Assessment of Evidence Level (N=72):\nCorrelational: 63.89%\nCausal limited: 27.78%\nCausal field: 9.72%\nReplicated causal field: 5.55%\nWide-scale causal: 5.55%\n\nAnalysis of Cited Studies (Focus on Experimental Field Studies):\nThe consensus states: “experts provided no evidence from experimental studies directly manipulating social media exposure to measure its effect on perfectionism.”\nVanhoffelen et al. (2025): Longitudinal study offered weak evidence. Not an experimental field study.\n\nExperimental Field Studies Present? No, as per the consensus statement.\nExpert Assessment of “Causal Field” Evidence: 9.72%. Given the lack of cited experimental (field or lab) evidence directly manipulating social media to measure perfectionism, this rating is likely based on broader interpretations or non-cited work.\nClaim 19. Social media increases relational aggression among adolescent girls… (Consensus statement: p. 103-104)\nExpert Assessment of Evidence Level (N=71):\nCorrelational: 69.01%\nCausal limited: 30.99%\nCausal field: 8.45%\nReplicated causal field: 1.41%\nWide-scale causal: 4.23%\n\nAnalysis of Cited Studies (Focus on Experimental Field Studies):\nThe consensus states: “no experts provided experimental evidence establishing a causal link between social media use and increased relational aggression.”\nMartins & Weaver (2019) meta-analysis included experimental studies on “exposure to RA -> RA behaviors,” not specific social media use causing RA.\nLapidot-Lefler et al. (2012) lab study on adults and general toxic behavior.\n\nExperimental Field Studies Present? No, for the direct claim.\nExpert Assessment of “Causal Field” Evidence: 8.45%. This is low and consistent with the lack of direct experimental field evidence.\nClaim 20. Among adolescent girls, social media increases exposure to other people displaying or discussing their mental disorders. (Consensus statement: p. 105-106)\nExpert Assessment of Evidence Level (N=72):\nCorrelational: 69.44%\nCausal limited: 27.78%\nCausal field: 12.50%\nReplicated causal field: 9.72%\nWide-scale causal: 8.33%\n\nAnalysis of Cited Studies (Focus on Experimental Field Studies):\nThe consensus states: “no experts provided references for a direct causal link.”\nStudies cited are observational (Alho et al. on peer networks; phenomena on TikTok; Ernala et al. on Twitter disclosures; Hamilton et al. on self-harm content exposure; Ali et al. on Instagram DMs). These describe patterns of exposure or association, not experimental manipulation of social media use to measure changes in exposure levels in a field setting.\n\nExperimental Field Studies Present? No.\nExpert Assessment of “Causal Field” Evidence: 12.50%. This rating seems to suggest some experts believe such evidence exists or interpret observational field data more strongly in a causal direction for this specific type of “increase.”\nClaim 21. Social media increases sexual predation and harassment of adolescent girls… (Consensus statement: p. 107-108)\nExpert Assessment of Evidence Level (N=59):\nCorrelational: 52.54%\nCausal limited: 13.56%\nCausal field: 5.08%\nReplicated causal field: 3.39%\nWide-scale causal: 11.86%\n\nAnalysis of Cited Studies (Focus on Experimental Field Studies):\nThe consensus states: “no experts provided experimental studies manipulating social media usage and measuring its effect on harassment or sexual predation.”\nBhuller et al. (2013): Quasi-experimental on general internet access and sex crimes. This is a natural field experiment.\n\nExperimental Field Studies Present? Yes, Bhuller et al. (2013) for general internet, but not specifically social media mechanisms for predation/harassment.\nExpert Assessment of “Causal Field” Evidence: 5.08%. Very low, consistent with the lack of direct social media field experiments.\nClaim 24. If most parents waited… it would benefit the mental health of adolescents overall. (Consensus statement: p. 112-113)\nExpert Assessment of Evidence Level (N=93):\nCorrelational: 39.73%\nCausal limited: 26.03%\nCausal field: 8.22%\nReplicated causal field: 2.74%\nWide-scale causal: 5.48%\n\nAnalysis of Cited Studies (Focus on Experimental Field Studies):\nThe consensus notes studies are correlational/longitudinal (Dempsey et al., 2020; Vaterlaus et al., 2021; Gerosa et al., 2024; Sapien Labs, 2024; Sun, 2023). None are experimental field studies manipulating delayed smartphone access for a cohort and measuring mental health. The discussion of “group-level effects” is hypothetical due to lack of data.\n\nExperimental Field Studies Present? No.\nExpert Assessment of “Causal Field” Evidence: 8.22%. Low, reflecting the absence of such studies.\nClaim 25. Imposing (and enforcing) a legal minimum age of 16 for opening social media accounts would benefit the mental health of adolescents overall. (Consensus statement: p. 114-115)\nExpert Assessment of Evidence Level (N=68):\nCorrelational: 26.47%\nCausal limited: 10.29%\nCausal field: 2.94%\nReplicated causal field: 2.94%\nWide-scale causal: 5.88%\n\nAnalysis of Cited Studies (Focus on Experimental Field Studies):\nConsensus: “Experts provided no references that were strictly related to the claim.” Ferguson (2024) and Thrul et al. (2025) are about effects of reducing use, not delaying initial access via legal age limits in a field experiment.\n\nExperimental Field Studies Present? No.\nExpert Assessment of “Causal Field” Evidence: 2.94%. Extremely low, reflecting the absence of such policy evaluation field experiments.\nClaim 26. Phone-free schools would benefit the mental health of adolescents overall. (Consensus statement: p. 116-117)\nExpert Assessment of Evidence Level (N=77):\nCorrelational: 44.16%\nCausal limited: 25.97%\nCausal field: 16.88%\nReplicated causal field: 2.60%\nWide-scale causal: 11.69%\n\nAnalysis of Cited Studies (Focus on Experimental Field Studies):\nBöttger & Zierer (2024) meta-analysis: Reviews 5 studies on school bans. These are natural experiments in field settings.\nKing et al. (2024): Pre-registered study of a phone ban. This is a natural experiment in a field setting.\nCampbell et al. (2024) scoping review: Includes studies of school bans (natural field experiments).\nGoodyear et al. (2025) meta-analysis: Compares restrictive vs. permissive policies in schools (natural field experiments).\n\nExperimental Field Studies Present? Yes, studies of school phone bans are natural experiments in field settings.\nExpert Assessment of “Causal Field” Evidence: 16.88%. This is consistent with several (though limited in number and with mixed/methodologically critiqued results) natural field experiments being available.\nSummary of Experimental Field Studies & Expert Assessment:\nClaim 8 (Sleep Deprivation): Potentially yes (intervention studies). 20.00% experts say Causal Field.\nClaim 9 (Sleep Dep & MH Decline): Potentially yes (interventions). 26.00% experts say Causal Field.\nClaim 10 (Attention Frag.): Potentially one (van Wezel et al.). 9.09% experts say Causal Field.\nClaim 11 (Attention Frag & MH Decline): Potentially yes (notification interventions). 7.41% experts say Causal Field.\nClaim 12 (Behavioral Addiction): Yes (Allcott et al.). 19.15% experts say Causal Field.\nClaim 13 (Behav. Add. & MH Decline): Unlikely direct experimental field study causing decline. 19.75% experts say Causal Field (likely broader interpretation).\nClaim 14 (Social Deprivation): Yes (Allcott et al., Geraci et al.). 14.46% experts say Causal Field.\nClaim 15 (Social Dep. & MH Decline): Yes (Killgore et al., interventions). 32.97% experts say Causal Field.\nClaim 17 (Visual Social Comp.): Unlikely from cited studies. 11.46% experts say Causal Field.\nClaim 18 (Perfectionism): No from cited studies. 9.72% experts say Causal Field.\nClaim 19 (Relational Aggression): No direct from cited studies. 8.45% experts say Causal Field.\nClaim 20 (Exposure to Disorders): No from cited studies. 12.50% experts say Causal Field.\nClaim 21 (Sexual Predation): Yes, for general internet (Bhuller et al.). 5.08% experts say Causal Field.\nClaim 24 (Delay Smartphones & MH): No from cited studies. 8.22% experts say Causal Field.\nClaim 25 (Age Limits & MH): No from cited studies. 2.94% experts say Causal Field.\nClaim 26 (Phone-Free Schools & MH): Yes (school ban studies). 16.88% experts say Causal Field.\nIt seems for many claims directly testing “Social media increases X” with experimental field studies, the evidence is sparse or non-existent according to the cited literature. The expert ratings for “Causal Field” are generally low for these, but sometimes higher than what the cited experimental field literature in the consensus statements would directly support, suggesting experts might be drawing on a wider body of knowledge or interpreting observational field data more causally. For policy-related claims, the expert assessment of causal field evidence is also very low, reflecting the difficulty and rarity of such large-scale experimental policy evaluations.\nRequest for summary table\nFirst draft:\n\nSo, experts could endorse multiple levels of evidence (despite some of them being mutually exclusive). Do the authors describe how they recoded these responses to make a graph where categories sum to 100%? Highest/lowest level of evidence endorsed?\n\nSecond draft:\n\nEarlier I asked for experimental field studies, but causal evidence could also come from natural experiments or well-controlled designs. Please revise the table and assess whether an experiment or natural experiment in the field supporting the claim is reported. If the text in Section S5 makes the case that an observational study can be used to back up the causal claim, mention it.\nAlso, don’t sum up the reported percentages for causal field and above since they are not mutually exclusive. Instead, try to read it off the plot and also report the percentage who say causal field in another column.\n\n\nIn reality, this claim in the graph pertains only to the US, surely they could make room for that important qualification.↩︎\nJay van Bavel later deleted this tweet and said he was not trying to equate their consensus to the consensus around climate change. I would say he tried… and had to walk it back.↩︎\nI manually vetted only one claim (about body image/visual social comparison), so Gemini could get this wrong (e.g. misclassify a study), but I think distilling documents is one of the things it does rather well.↩︎\n",
    "preview": {},
    "last_modified": "2025-05-20T09:18:04+02:00",
    "input_file": "consensus-how-the-sausage-gets-made.knit.md"
  },
  {
    "path": "posts/2024-05-24-simulation-on-adjusting-for-confounders-in-a-mixed-model/",
    "title": "Simulation on adjusting for confounders in a mixed model",
    "description": "A brief tidyverse-style simulation to figure out what happens, if I don't include\nvarying slopes in a model.",
    "author": [
      {
        "name": "Ruben C. Arslan",
        "url": "https://rubenarslan.github.io"
      }
    ],
    "date": "2024-05-24",
    "categories": [],
    "contents": "\nI recently tried to a write a very simple simulation script in tidyverse to demonstrate simulation-based power analysis for students. Today, I had occasion to adapt the script myself, because a student asked me whether she needed to include varying slopes for adjusted confounders in a multilevel model. My first thought was “yes, that’s better” but I was a little scared of the impact on model complexity, because it’s a large dataset.\nSo, I ran a little simulation and found out I was wrong to expect bias. To be frank, this runs counter to my intuition, so I’m hoping that posting this will either make someone point out an error in my simulation or help me develop the right intuition.\n\n\nlibrary(tidyverse)\nlibrary(lmerTest)\nlibrary(broom.mixed)\n\ntheme_set(theme_bw())\nlmer <- function(...) tryCatch({ suppressMessages(suppressWarnings(lmerTest::lmer(...))) }, \n                              error = function(e) NULL)\n\nalpha = 0.05 # false positive rate\n\n\nSet up simulation scenarios\nThe expand_grid function makes this easy. For a power analysis, we could also\ncalculate the budget and filter out scenarios we cannot afford (or scenarios that make no sense a priori).\n\n\nScenarios = expand_grid(\n    # Number of countries\n    N = c(20, 30, 40),\n    # Number of days per person\n    obs_per_country = c(1000),\n    # Effect of female parliamentarians\n    b_fem_parl = c(-0.2),\n    # Does the effect vary between countries\n    sd_fem_parl = c(0.2),\n    # Effect of GDP\n    b_GDP = c(1),\n    # Does the effect vary between countries\n    sd_GDP = c(0, 1),\n    b_fem_parl_GDP = c(1),\n    sd_fem_parl_GDP = c(0, 1),\n    # Simulate each scenario 200 times\n    Simulation = 1:50)\n\n\n\n\ngenerate_data_and_fit_model <- function(N, obs_per_country, \n                                        b_fem_parl, sd_fem_parl, \n                                        b_GDP, sd_GDP, \n                                        b_fem_parl_GDP, sd_fem_parl_GDP, \n                                        Simulation) {\n  \n  # Store scenario's parameters\n  Parameters <- environment() %>% as.list() %>% as_tibble()\n  \n  # Generate Dataset\n  Days = tibble(\n    # A number that identifies each person, repeated days_per_person times\n    country = 1:N %>% rep(each = obs_per_country),\n    # N draws from the normal distribution with mean = 0, SD = 1\n    general_wellbeing = rnorm(N, mean = 0, sd = 1)[country],\n    # The effect of GDP varies from country to country\n    GDP_i = rnorm(N, mean = b_GDP, sd = sd_GDP)[country],\n    # the effect of fem_parl also varies from country to country\n    fem_parl_i = rnorm(N, mean = b_fem_parl, sd = sd_fem_parl)[country],\n    # the effect of GDP on female parliamentarians also varies from country to country \n    fem_parl_GDP_i = rnorm(N, mean = b_fem_parl_GDP, sd = sd_fem_parl_GDP)[country],\n    # within country predictors\n    GDP = rnorm(N * obs_per_country),\n    fem_parl = rnorm(N * obs_per_country) + fem_parl_GDP_i * GDP,\n    # today's well being results from stable country differences\n    # the GDP effect, the female parliamentarians effect, and a residual term\n    well_being = general_wellbeing + fem_parl_i * fem_parl\n                    + GDP_i * GDP + rnorm(N * obs_per_country)\n  )\n\n  # Our three statistical models predict well-being\n  models <- list(\n    Model_varFP = lmer(well_being ~ fem_parl + GDP + (1 + fem_parl | country), data = Days),\n    Model_varFP_GDP = lmer(well_being ~ fem_parl + GDP + (1 + fem_parl + GDP || country), data = Days),\n    Model_varFP_GDP_cor = lmer(well_being ~ fem_parl + GDP + (1 + fem_parl + GDP | country), data = Days)\n   )\n  \n  # store their output in tidy form, together with the scenario parameters\n  models %>% map(~ broom.mixed::tidy(., conf.int = TRUE)) %>% \n    bind_rows(.id = \"Model\") %>% \n    bind_cols(Parameters, .)\n}\n\n# To try this out and debug the script\n# debug(generate_data_and_fit_model)\n\n# Scenarios %>% slice(1) %>% pmap(generate_data_and_fit_model) %>% bind_rows() %>% filter(term == \"fem_parl\") %>% View()\n\n\nSimulate\nWe loop over all scenarios, generate the data, fit the models, and store the results.\n\n\n\n\n\nSimulationResults = Scenarios %>%\n  pmap(generate_data_and_fit_model, .progress = T) %>%\n  # Combine everything into a data frame\n  bind_rows()\n\n\nThis call above wasn’t parallelised, but could easily be using furrr instead of purrr.\n\n\nlibrary(furrr)\nplan(multisession)\nSimulationResults = Scenarios %>%\n  furrr::future_pmap(generate_data_and_fit_model, .progress = T,\n                     .options = furrr::furrr_options(seed = 20191005)) %>%\n  # Combine everything into a data frame\n  bind_rows()\n\nwrite_rds(SimulationResults, \"SimulationResults3.rds\")\n\n\nIf our goal was a power analysis, we could check how often the effect is significant.\n\n\nSimulationResults <- SimulationResults %>%\n  mutate(significant_two_sided = p.value < alpha)\n\n\nHere, my goal is finding out whether my estimate is likely to be biased away from the true simulated value.\n\n\nSimulationResults <- SimulationResults %>%\n  mutate(bias = if_else(term == \"fem_parl\", b_fem_parl - estimate, NA_real_))\n\n\nWe summarise across the scenarios and models to average power, estimates,\nstandard error, and bias.\n\n\nSummarisedResults <- SimulationResults %>%\n  filter(term == \"fem_parl\") %>% \n  group_by(across(c(N:Model,-Simulation))) %>%\n  summarise(\n    power = mean(p.value < alpha),\n    bias = mean(b_fem_parl - estimate),\n    mean(std.error),\n    mean(estimate),\n    Simulations = n()\n  ) %>%\n  arrange(desc(abs(bias)))\n\n\nI expected bias to be bigger if our model does not match the data-generating process (i.e., Model_varFP but the effect of GDP on FP and WB varied). This was not the case, though standard errors increased. I am little surprised by this, but I’ve learned not to trust my intuition on these topics.\n\n\nSimulationResults %>% \n  filter(term == \"fem_parl\") %>% \n  filter(b_fem_parl != 0) %>% \n  group_by(sd_GDP, sd_fem_parl_GDP, Model, b_fem_parl) %>% \n  summarise(bias = mean(bias), \n            est = mean(estimate, na.rm = T), \n            std.error = mean(std.error), \n            n()) %>% \n  kableExtra::kbl()\n\n\nsd_GDP\n\n\nsd_fem_parl_GDP\n\n\nModel\n\n\nb_fem_parl\n\n\nbias\n\n\nest\n\n\nstd.error\n\n\nn()\n\n\n0\n\n\n0\n\n\nModel_varFP\n\n\n-0.2\n\n\n-0.0071441\n\n\n-0.1928559\n\n\n0.0378787\n\n\n150\n\n\n0\n\n\n0\n\n\nModel_varFP_GDP\n\n\n-0.2\n\n\n-0.0071438\n\n\n-0.1928562\n\n\n0.0378704\n\n\n150\n\n\n0\n\n\n0\n\n\nModel_varFP_GDP_cor\n\n\n-0.2\n\n\n-0.0071425\n\n\n-0.1928575\n\n\n0.0379609\n\n\n150\n\n\n0\n\n\n1\n\n\nModel_varFP\n\n\n-0.2\n\n\n0.0020667\n\n\n-0.2020667\n\n\n0.0372315\n\n\n150\n\n\n0\n\n\n1\n\n\nModel_varFP_GDP\n\n\n-0.2\n\n\n0.0021055\n\n\n-0.2021055\n\n\n0.0372255\n\n\n150\n\n\n0\n\n\n1\n\n\nModel_varFP_GDP_cor\n\n\n-0.2\n\n\n0.0020659\n\n\n-0.2020659\n\n\n0.0372329\n\n\n150\n\n\n1\n\n\n0\n\n\nModel_varFP\n\n\n-0.2\n\n\n0.0016501\n\n\n-0.2016501\n\n\n0.1001414\n\n\n150\n\n\n1\n\n\n0\n\n\nModel_varFP_GDP\n\n\n-0.2\n\n\n0.0025946\n\n\n-0.2025946\n\n\n0.0376310\n\n\n150\n\n\n1\n\n\n0\n\n\nModel_varFP_GDP_cor\n\n\n-0.2\n\n\n0.0025944\n\n\n-0.2025944\n\n\n0.0376307\n\n\n150\n\n\n1\n\n\n1\n\n\nModel_varFP\n\n\n-0.2\n\n\n-0.0016691\n\n\n-0.1983309\n\n\n0.0840834\n\n\n150\n\n\n1\n\n\n1\n\n\nModel_varFP_GDP\n\n\n-0.2\n\n\n-0.0043495\n\n\n-0.1956505\n\n\n0.0378779\n\n\n150\n\n\n1\n\n\n1\n\n\nModel_varFP_GDP_cor\n\n\n-0.2\n\n\n-0.0043470\n\n\n-0.1956530\n\n\n0.0378743\n\n\n150\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2024-05-24T16:12:44+02:00",
    "input_file": "simulation-on-adjusting-for-confounders-in-a-mixed-model.knit.md"
  },
  {
    "path": "posts/2024-02-15-the-reliability-of-multilevel-parameters-in-bayesian-regressions/",
    "title": "The reliability of multilevel parameters in Bayesian regressions",
    "description": {},
    "author": [
      {
        "name": "Ruben C. Arslan",
        "url": "https://rubenarslan.github.io"
      }
    ],
    "date": "2024-02-26",
    "categories": [
      "brms",
      "measurement",
      "reliability",
      "modelling"
    ],
    "contents": "\n\nContents\nSimulation, of course\nWhy does this work?\nFootnote on other approaches.\nBoring footnote to ingratiate myself with fellow full luxury Bayesians\nEnough with the footnotes\n\nFor most psychologists, the concept of reliability is the main way they think about measurement error. At least it used to be that way for me and a recent conference I attended didn’t disabuse me of this impression.\nAt the conference, many researchers presented work in which they aimed to explain variation in a parameter of a multilevel model, or used such a parameter to predict something. Basically, in personality, people are moving on from individual differences in means. Some example parameters include random slopes (e.g., the varying effects of some daily event on daily affect), residual variability (e.g., varying within-person negative affect variability), but there are many others (e.g., within-subject autocorrelation, “inertia”).\nNow, these higher-level parameters have to be estimated of course. Most researchers are aware that these quantities are not error-free.1 Still, in most talks I saw, people either didn’t address this, or they computed a reliability in some roundabout way (e.g., split-half).2\nWhen we were recently interested in the reliability of within-subject variability in a censored model, we did not find an existing approach fit for our needs. Our solution happens to be easy, fully general and works for random intercepts, slopes, residual variances, etc.\nSimulation, of course\nIn Bayesian models, we already have an estimate of the error of our multilevel parameters, namely the variance across MCMC draws. Let’s simulate a N=300 within-person diary study. People take part for, on average, 14 days.\n\n\nShow code\n\nset.seed(20191005)\nN <- 300\nn_days = 14\nn_days_per_person = rpois(N, n_days)\npeople <- tibble(\n  id = 1:N,\n  x_mean = rnorm(N),\n  y_mean = rnorm(N),\n  x_slope = rnorm(N, 0.5, 0.2),\n  y_var = rnorm(N, 0.3, 0.2),\n  days = n_days_per_person\n  )\n\ndays <- people %>% \n  full_join(tibble(\n              id = rep(1:N, times = n_days_per_person)\n            ), by = \"id\", multiple = \"all\") %>% \n            mutate(\n              x = x_mean + rnorm(n()),\n              y = rnorm(n(), \n                        mean = y_mean + x_slope*x,\n                        sd = exp(y_var))\n            )\n\n\n\n\nShow code\n\nrmarkdown::paged_table(people)\n\n\n\n\n\n\nShow code\n\nrmarkdown::paged_table(days)\n\n\n\n\nLet’s not make it too simple. Our outcome y is a function of x, which has stable between-person variance, but also varies within-subject. But the effect of x varies between persons. Also, the residual variance of y varies between persons. Let me specify a brms model to recover our data-generating model.\n\n\nShow code\n\nm1 <- brm(bf(y ~ 1 + x + (1 + x|id),\n             sigma ~ (1|id)), data = days,\n          backend = \"cmdstanr\",\n          cores = 4,\n          file = \"example_model\",\n          file_refit = \"on_change\")\n\n\nNow, for each person’s random intercept, slope or residual variance, we can look at the distribution of the MCMC draws for their estimate.3\n\n\nShow code\n\ntrue_score_var <- m1 %>%\n    # grab all estimates of varying/random effects\n    gather_draws( `sd_.+`, regex = TRUE) %>% \n    mutate(.variable = stringr::str_sub(.variable, 4)) %>% \n    mutate(latent_var = .value^2) %>% \n    group_by(.variable) %>%\n    summarise(latent_var = mean(latent_var))\n\n\nWe get our estimate of the variance of the true scores by taking the sd hyperparameters for the random intercept, slope and residual variance. We square the SDs to get the variance and then average across MCMC draws.4\n\n\nShow code\n\nSEE2 <- m1 %>%\n      # grab all estimates of varying/random effects\n      gather_draws( `r_.+__.+`[id,parameter], `r_[^_]+_?[^_]+`[id,parameter], \n                    regex = TRUE) %>%\n      ungroup() %>% \n      tidyr::unite(.variable, .variable, parameter, sep = \"__\") %>% \n      mutate(.variable = stringr::str_sub(.variable, 3)) %>% \n      mutate(.variable = stringr::str_replace(.variable, \n                                              \"_sigma__\", \"_sigma_\")) %>% \n      group_by(.variable, id) %>%\n      # compute variance across draws\n      summarise(SEE2 = var(.value)) %>%\n      left_join(people %>% select(id, days)) %>% \n      # give more weight to error var estimated from people \n      # who contributed more data\n      summarise(\n        # weighted SEE^2 reflects the amount of error you get if you take \n        # along the uncertainty in your model\n        weighted_SEE2 = sum(SEE2*days)/sum(days),\n        # unweighted SEE^2 reflects the amount of error you get if you \n        # extract the manifest random intercepts/slopes/variances\n        SEE2 = mean(SEE2)\n        )\n\n\nNow, in addition to the hyperparameter sd, the model also estimated a random intercept, slope, residual variance for each person. The sd hyperparameter affected how much the observed intercepts/slopes/variances were shrunk towards the mean. Some random intercepts are estimated with more, some with less error, depending (in our example) on the number of days the person participated in the diary. The uncertainty of random intercept is reflected by the variance across MCMC draws, which I call \\({SEE}^2\\) here. So, we compute the variance across draws for each person and then average across people.5\nFinally, we can get an estimate of each parameter’s reliability by dividing the average \\({SEE}^2\\) by the average between-person variance and subtracting this fraction from 1.\n\n\nShow code\n\ntrue_score_var %>% left_join(SEE2) %>% \n    mutate(reliability = 1 - SEE2/latent_var,\n           weighted_reliability = 1 - weighted_SEE2/latent_var,\n           ) %>% \n  knitr::kable()\n\n.variable\nlatent_var\nweighted_SEE2\nSEE2\nreliability\nweighted_reliability\nid__Intercept\n1.00\n0.15\n0.16\n0.84\n0.85\nid__sigma_Intercept\n0.05\n0.02\n0.02\n0.52\n0.55\nid__x\n0.03\n0.02\n0.02\n0.19\n0.20\n\nWhy does this work?\nTo get here, it helped us to work our way back from how a confidence interval is calculated under the regression hypothesis in classical test theory using Kelley’s formula (1923, p. 2146). For me, the CI under the regression hypothesis was my first encounter with shrinkage. In my teaching, the connection to the Bayesian concepts was not made, though apparently Novick (1971) made the connection to Kelley’s formula7\nNamely, the standard error of the shrunk estimate is computed as follows, where \\(SEE\\) stands for “standard error of the estimate” and \\(s_X\\) is the observed SD:\n\\[SEE = s_X \\cdot \\sqrt{Rel \\cdot (1-Rel)}\\]\nWe can square this to get the error variance of the shrunk estimates. Now \\(s_X^2\\) is our observed variance and \\(V_E\\) is \\(se_{shrunk}^2\\).\n\\[{SEE}^2=s_X^2⋅(1−Rel) * Rel\\]\nRearranging the classical test theory definition of reliability, we can express the observed variance (\\(s_X^2\\)) as the latent true score variance (\\(V_T\\)) divided by the reliability (\\(Rel\\)).\n\\[\nRel=\\frac{V_T}{s_X^2}\n\\]\n\\[\ns_X^2=\\frac{V_T}{Rel}\n\\]\nSubstitute\n\\[{SEE}^2=\\frac{V_T}{Rel}⋅(1−Rel) * Rel\\]\nSimplify\n\\[{SEE}^2=V_T⋅(1−Rel)\\]\nSolve for \\(Rel\\)\n\\[Rel = 1 - \\frac{{SEE}^2}{V_T}\\]\nThe standard errors (SEs) of our random slopes are the SEs of the shrunk estimates. As such, they can be analogised (in our vanilla, uninformative prior model) to the SE computed under the regression hypothesis (\\(SEE\\)), where we first shrink the observed value to the population mean and then compute a standard error around that estimate.\nI thought this was neat. I’d appreciate to hear if it is wrong in some way and/or is discussed more elsewhere.8 I am also still looking for a way to compute the right uncertainty around the reliability estimates.\nThanks especially to Stefan Schmukle (who has a related paper draft he really needs to publish), but also Niclas Kuper, Julia Rohrer, and Taym Alsalti for discovering basic mistakes in my thinking and fun exchanges on this topic.\nFootnote on other approaches.\nI saw uses of split half, as well as people getting parameters for multiple items, then computing reliability of the item indices using Cronbach’s alpha. Methodologists have published various solutions for specific cases, like random slopes, though I haven’t seen them used in the primary literature (e.g., Neubauer, Voelkle, Voss, & Mertens, 2020, Schneider & Junghaenel, 2022). All of these approaches also require estimating additional models. I.e., you can’t just use the model that is being used to study e.g. the moderator that could explain random slope variation. The latter two approaches also require us to know the residual variance. Most importantly for us though, they weren’t fully general (e.g., didn’t work to estimate the reliability of the residual variability).\nBoring footnote to ingratiate myself with fellow full luxury Bayesians\nI guess I won’t be needing this formula/snippet very often, as I prefer full luxury Bayes. I usually won’t extract model parameters to use in two-step modelling, but would rather predict the latent variable in the same model. I am also moving away from interpreting metrics that are relativised to the in-sample standard deviation for various reasons. But in a recent preprint by Julia Rohrer et al., we argued that the low reliability of the random slopes we were studying made it fairly pointless to study them further with the amount of data we had. That seems like a suitable use case and will hopefully motivate someone to collect more data. I think the reliability coefficient can also help break down the results of complex models for other researchers and ease the transition as personality psychologists study these complex parameters more.\nEnough with the footnotes\nThis blog post is subject to my bug bounty policy, which means that I’d appreciate hearing about errors in this post and will pay you (or a charity) a small bounty.\n\nLike everything else in psychology↩︎\nSee below on other approaches↩︎\nPresumably standard errors calculated with bootstrapping in empirical Bayes models would work too?↩︎\nCaution, don’t just square the sd parameter reported in the brms model summary or in VarCorr, mean(sd)^2 != mean(sd^2)↩︎\nThere’s a wrinkle here. For the main calculation, I take the unweighted average across people. That’s the relevant case for people who extract random slopes from a model and use them as a predictor. But if you estimate a moderator in a one-step model or if you take along the uncertainty by working with the draws, I think the weighted SEE^2 is more appropriate↩︎\nKelley, T. L. (1923). Statistical method. Macmillan.↩︎\nI only learned about this after the fact. Thanks to Stefan Schmukle for pointing me to Levy, R. & Mislevy, R. J. (2016). Bayesian psychometric modeling. Taylor & Francis↩︎\nI fully expect to be embarrassed by someone having pointed this out in 1930 already, but I’ll be a little less embarrassed because I asked around and it’s not obvious to everyone in my field.↩︎\n",
    "preview": {},
    "last_modified": "2024-02-26T17:27:04+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-09-19-modeling-misclicks-in-an-app-interface/",
    "title": "Modeling misclicks in an app interface",
    "description": "Thinking through my approach for modelling false positive button clicks in an app interface.",
    "author": [
      {
        "name": "Ruben C. Arslan",
        "url": "https://rubenarslan.github.io"
      }
    ],
    "date": "2023-09-02",
    "categories": [
      "measurement",
      "methods"
    ],
    "contents": "\n\nContents\nAppendix\n\nIf you’ve seen me talk at a conference in recent years, you know that I have been working with data collected by the cycle tracking app Clue.\nThe cool thing about this dataset is that it’s a) big b) highly repeated (daily tracking) c) spans long time frames (for some users, years).\nThe not so cool thing is that all outcomes are tracked using single on-off button for each outcome. No multiple items, no Likert scales, not even a way to tell a nonresponse from a no.\nStill, the amount of data, in my view, can more than compensate for this problem, and I find very similar results on e.g. sexual desire as in my other work, which hopefully does better psychometrically but falls way short of Clue’s sample size.\nAnother problem that I have been grappling with is that there seems to be a nonzero rate of misclicking in the app. Unfortunately, the Clue database does not record when a button is toggled off again, so even if users correct their misclick, it ends up in my data.\nThe substantive question I’m trying to answer is to estimate the rate of mittelschmerz, i.e.\nhow many people ever experience mittelschmerz and/or in how many cycles mittelschmerz\nis experienced. Mittelschmerz is a pain that occurs around ovulation, i.e. in the middle of the cycle. Mittelschmerz should not occur during (pre-)menstruation. Nor should people who take combined oral contraceptive pills be ovulating especially frequently. Yet, I observe both of these things. Why?\nI suspect the interface. In Clue, symptom buttons are thematically arranged. As you can see, the “ovulation pain” button is next to the button for cramps and tender breasts. Guess which forms of pain do occur during (pre-)menstruation and for people on the pill. That’s right, cramps and breast tenderness.\nThe interfaceSo, I think that in some non-zero percent of cases, users will accidentally turn on the “ovulation pain” button instead of or along with the “cramps” and “tender breasts” button.\nHere, let me simulate that real quick:\n\n\nset.seed(05102019)\nlibrary(tidyverse)\nreps = 10000\ndays <- 30\nlogit <- boot::logit\nmiss_prob <- 0\ndaily <- tibble(\n  # users\n  id = rep(1:reps, each = 30),\n  # cycle days\n  time = rep(1:days, times = reps),\n  ovulating = c(rep(1, times = 30 * 0.75 * reps), \n                rep(0, times = 30 * 0.25 * reps)),\n  feel_ms = c(rep(1, times = 30 * 0.25 * reps), \n                rep(0, times = 30 * 0.75 * reps)),\n  ovulation_day = rep(round(rnorm(reps, 15, sd = 1.5)), each = 30),\n  cs = 1 * rbinom(reps * days, 1, \n                  plogis(-5 + 1 * if_else(time < 6, 5-time, 0))),\n  os = if_else(time == ovulation_day & ovulating & feel_ms, 1, 0),\n  c_clicks = rbinom(reps * days, 1, prob = (1 - miss_prob) * cs),\n  o_clicks = rbinom(reps * days, 1, prob = \n                      plogis(-4 + 3 * cs + 10 * os))) %>% \n  mutate(time = factor(time))\n\n# readr::write_csv(daily, \"daily.csv\")\n# daily %>% group_by(time) %>% summarise_all(mean) %>% View\n# daily %>% group_by(time) %>% summarise(sprintf(\"%.7f\",mean(o_clicks))) %>% View\n\n\nAs you can see, I’ve simulated an effect of time on the variable cs\nFor a subset of ids, there is one ovulation per cycle, sometime\nin the middle, which causes os. The true rate is 1/4 * 1/30 = 0.00833, i.e. a fourth of people feel ovulation on one day per cycle.\n\n\nlibrary(marginaleffects)\nglm1 <- glm(os ~ 1, data = daily, family = gaussian)\npredictions(glm1)[1,]\n\n\n Estimate Pr(>|z|)   S   2.5 %  97.5 %\n  0.00833   <0.001 Inf 0.00801 0.00866\n\nColumns: rowid, estimate, p.value, s.value, conf.low, conf.high, os \nType:  invlink(link) \n\nIn the next step, I’ve modelled clicks on these buttons. People hit their button of\nchoice with a 95% accuracy. But there’s a 20% chance of fat thumbs hitting the o button when aiming for b or c.\nSo, while o truly peaks in the middle, we actually see additional\npeaks at either end of the cycle in the observed clicks.\n\n\nggplot(daily %>% pivot_longer(c(cs:os,c_clicks, o_clicks, -time)) %>% \n         mutate(clicks = if_else(str_detect(name, \"clicks\"), \n                                 \"observed\", \"true\"),\n                name = str_sub(name, 1, 1)), aes(time)) +\n  geom_point(aes(y = value, color = name), stat = 'summary') +\n  facet_wrap(~ clicks) +\n  scale_x_discrete(breaks = c(1, 5, 10, 15, 20, 25, 30)) +\n  theme_bw()\n\n\n\nNow, how do I get rid of this bias? Here’s an idea:\nI find a ~pure false positive subset to estimate the relationship of false positives to my predictors.\nIn my real data, these would be cycle days during (pre-)menstruation for users of the combined oral contraceptive pill. In my simulations, I subset in a similar way and then fit a general linear model.\n\n\nggplot(daily %>% filter(ovulating == 0) %>% \n         pivot_longer(c(cs:os,c_clicks, o_clicks, -time)) %>% \n         mutate(clicks = if_else(str_detect(name, \"clicks\"), \n                                 \"observed\", \"true\"),\n                name = str_sub(name, 1, 1)), aes(time)) +\n  geom_point(aes(y = value, color = name), stat = 'summary') +\n  facet_wrap(~ clicks) +\n  scale_x_discrete(breaks = c(1, 5, 10, 15, 20, 25, 30)) +\n  theme_bw()\n\n\n\n\n\nlibrary(marginaleffects)\nglm2 <- glm(o_clicks ~ 1 + c_clicks, data = daily %>% \n                      filter(!ovulating, ! time %in% 10:20 ), family = binomial)\n\n\nNow, I can compute the rate before and after setting my predictors to zero.\n\n\navg_predictions(glm2)\n\n\n Estimate Pr(>|z|)   S  2.5 % 97.5 %\n   0.0204   <0.001 Inf 0.0192 0.0218\n\nColumns: estimate, p.value, s.value, conf.low, conf.high \nType:  invlink(link) \n\navg_predictions(glm2, newdata = tibble(c_clicks = 0))\n\n\n Estimate Pr(>|z|)   S  2.5 % 97.5 %\n   0.0188   <0.001 Inf 0.0176 0.0201\n\nColumns: rowid, estimate, p.value, s.value, conf.low, conf.high, c_clicks, rowid_dedup \nType:  invlink(link) \n\nThis is very close to plogis(-4) ~= 0.018, i.e. the false positive intercept I added in my simulation above.\nI can estimate the rate of false positives per day in all of the data, including data where there should be true positives.\n\n\ndaily$fp <- predict(glm2, newdata = daily, type = \"response\")\nmean(daily$fp,na.rm=T)\n\n[1] 0.02373439\n\nTo estimate the true rate, I subtract the false positive rate and divide by (1-FP) to account for clicks which were both intended and\naccidental. Normally, two clicks on the same button would turn the button off again. However, in Clue, they unfortunately had a problem with their database, so off-toggles went unrecorded. This probably contributes to the fairly high false positive rate.\n\n\nglm5 <- glm(o_clicks ~ 1, data = daily, family = binomial)\ndaily$obs_rate <- predict(glm5, type = \"response\")\ndaily %>% \n  summarise(obs_rate = mean(obs_rate), fp_rate = mean(fp)) %>% \n  mutate(\n    est_true_rate = (obs_rate - fp_rate) / (1 - fp_rate)\n    )\n\n# A tibble: 1 × 3\n  obs_rate fp_rate est_true_rate\n     <dbl>   <dbl>         <dbl>\n1   0.0318  0.0237       0.00823\n\nLet’s bring the time variable back in. The black line is the true curve.\n\n\nglm7 <- glm(o_clicks ~ time, data = daily, family = binomial)\ndaily$obs_rate <- predict(glm7, type = \"response\")\ndaily %>% \n  group_by(time) %>% \n  summarise(obs_rate = mean(obs_rate), fp_rate = mean(fp)) %>% \n  mutate(\n    est_true_rate = (obs_rate - fp_rate) / (1 - fp_rate)\n    ) %>% \n  mutate(est_true_rate = if_else(est_true_rate < 0, 0, est_true_rate)\n         ) %>% \n  pivot_longer(c(-time)) %>% \n  ggplot(aes(time, value, color = name)) +\n  geom_line(aes(time, os, group = 1),color = \"black\", stat = \"summary\", data = daily) +\n  geom_point(position = position_dodge(width = 0.4)) +\n  theme_bw() +\n  scale_color_brewer(type = 'qual', palette = 2)\n\n\n\nIn the real example, I’m interested in estimating how many users ever experience mittelschmerz. So, I also wanted to estimate false positives at the cycle and user level. For simplification, I just consider user/cycle as one thing here. The strategy remains the same: focus on the (pre-)menstruation period for users who are not ovulating to estimate the false positive rate. Extrapolate it to the rest of the users. Then compute the true rate from the observed rate.\nComplications:\nI had to implement an offset to correctly estimate the FP rate from the shorter cycles (after excluding the midcycle period).\nI had to use the sum score of c_clicks as a predictor, instead of the mean (so that it would scale with the observation period)\n\n\nno_o_users <- daily %>% \n  filter(!feel_ms, !time %in% 10:20) %>% \n  group_by(id, feel_ms) %>% \n  summarise(fp_daily = mean(fp),\n            any_o_clicks = if_else(any(o_clicks == 1), 1, 0),\n            days = n(),\n            o_clicks = sum(o_clicks),\n            c_clicks = sum(c_clicks))\n\nusers <- daily %>% group_by(id, feel_ms) %>% \n  summarise(fp_daily = mean(fp),\n            any_o_clicks = if_else(any(o_clicks == 1), 1, 0),\n            days = n(),\n            o_clicks = sum(o_clicks), \n            c_clicks = sum(c_clicks))\n\nglm9 <- glm(any_o_clicks ~ 1 + c_clicks + offset(log(days)), data = no_o_users, family = binomial())\navg_predictions(glm9)\n\n\n Estimate Pr(>|z|)     S 2.5 % 97.5 %\n    0.386   <0.001 267.3 0.374  0.397\n\nColumns: estimate, p.value, s.value, conf.low, conf.high \nType:  invlink(link) \n\n(fp_rate <- avg_predictions(glm9, newdata = tibble(days = 30, c_clicks = mean(users$c_clicks)))[,\"estimate\"])\n\n[1] 0.5111248\n\nusers$fp_users <- predict(glm9, newdata = users, type = \"response\")\nglm10 <- glm(any_o_clicks ~ 1 + offset(log(days)), data = users, family = binomial())\nobs_rate <- avg_predictions(glm10, newdata = tibble(days = 30, c_clicks = mean(no_o_users$c_clicks)))[,\"estimate\"]\n\nmean(users$fp_users,na.rm=T)\n\n[1] 0.5078552\n\nfp_rate <- mean(users$fp_users)\n(mean(users$any_o_clicks) - fp_rate)/(1- fp_rate)\n\n[1] 0.2611931\n\nLooks good! Now I’ll try my approach on the real data, to be inevitably disappointed.\nAppendix\nI first tried to use offsets in binomial regressions to fit my generative model. But I ran into trouble setting this up. Also, in both cases, I have to use log(fp) as the offset which implies that setting fp=0 in my marginal effects yields -Inf. This doesn’t seem right and I gave up on this. If you’re interested, the code follows.\nI can then use the estimated rate of false positives as an offset (a regression coefficient whose slope has been fixed to 1).\n\n\nknitr::opts_chunk$set(eval = F)\n\n\n\n\nglm5 <- glm(o_clicks ~ offset((fp)), data = daily)\nglm5 <- glm(o_clicks ~ offset(log(fp)), data = daily, family = binomial(link = \"cloglog\"))\nglm5 <- glm(o_clicks - fp ~ 1, data = daily)\navg_predictions(glm5)\navg_predictions(glm5, newdata = tibble(fp = 0.01))\nmean(daily$o_clicks)-mean(daily$fp)\n\n\nThis is still higher than the true value of .008, but closer.\nLet’s bring the time variable back in.\n\n\nglm7 <- glm(o_clicks ~ time, data = daily, family = binomial)\nglm8 <- glm(o_clicks ~ time + offset(log(fp)), data = daily, family = binomial)\nbind_rows(\n  no_offset = predictions(glm7, newdata = tibble(time = factor(1:30))),\n  offset = predictions(glm8, newdata = tibble(time = factor(1:30),\n                                              fp = 0.01)),\n  .id = 'model') %>%\n  ggplot(aes(time, estimate, color = model)) +\n  geom_line(aes(time, os, group = 1),color = \"black\", stat = \"summary\", data = daily) +\n  geom_point(position = position_dodge(width = 0.4)) +\n  theme_bw() +\n  scale_color_brewer(type = 'qual')\n\n\nAfter taking the offset into account, the peaks near the end disappear. I’m pretty satisfied with this.\nIn the real example, I’m interested in estimating how many users ever experience mittelschmerz. So, I a) want to implement a false positive model at the daily level b) estimate rates of any mittelschmerz per cycle and c) per user. Especially for c) I’d like to specify a model that takes the number of cycles into account (since I’m more likely to observe a cycle with mittelschmerz, the more cycles I have for the user). I have this all set up, but I’m struggling with implementing a false positive model using lme4. It sort of works with brms, but that’s a poor fit for the huge sample size I’m working with. Also, in both cases, I have to use log(fp) as the offset which implies that setting fp=0 in my marginal effects yields -Inf. This doesn’t seem right.\nAt this point, I’m wondering whether the approach via offset is silly, but I don’t really have an alternative idea.\nUsing lme4\n\n\nlibrary(lme4)\n# https://stats.stackexchange.com/questions/88960/lme4-glmer-problems-with-offset\n#\nglm9 <- glmer(o_clicks ~ time + (1|id), data = daily, family = binomial())\nglm10 <- glmer(o_clicks ~ time + offset(log(fp)) + (1|id), data = daily,\n               family = binomial(link=\"cloglog\")) # does not work\nglm10 <- glmer(o_clicks ~ time + log(fp) + (1|id), data = daily,\n               family = binomial(link=\"cloglog\")) # works and estimate the slope as 1, so why does the other model not work\n\nbind_rows(\n  no_offset = predictions(glm9, newdata = tibble(time = factor(1:30)),\n                          re.form = NA),\n  offset = predictions(glm10, newdata = tibble(time = factor(1:30),\n                                              fp = 0.01), re.form = NA),\n  .id = 'model') %>%\n  ggplot(aes(time, estimate, color = model)) +\n  geom_point(position = position_dodge(width = 0.4)) +\n  theme_bw() +\n  scale_color_brewer(type = 'qual')\n\n\n\n\n\n",
    "preview": "posts/2023-09-19-modeling-misclicks-in-an-app-interface/clue_interface.png",
    "last_modified": "2023-09-21T16:21:45+02:00",
    "input_file": {},
    "preview_width": 604,
    "preview_height": 1212
  },
  {
    "path": "posts/2023-03-11-latent-group-mean-centering-revisited/",
    "title": "Latent group means with brms",
    "description": "Richard McElreath recently shared a tutorial on how to do estimate latent group means (à la Mundlak) in Stan. I try to follow his code, do the same with brms, and try to implement a solution Matti Vuorre posted.",
    "author": [
      {
        "name": "Ruben C. Arslan",
        "url": "https://rubenarslan.github.io"
      }
    ],
    "date": "2023-03-15",
    "categories": [
      "brms",
      "modelling",
      "quick job"
    ],
    "contents": "\n\nContents\nHow did it go? High-level summary\nBernoulli simulations\nNaive\nFixed effects\nMultilevel\nMultilevel Mundlak\nMultilevel Latent Mundlak\nnon-working implementation of Matti Vuorre’s approach\nSummary\n\nGaussian simulations\nNaive\nFixed effects\nMultilevel\nMultilevel Mundlak\nMultilevel Latent Mundlak\nnon-working implementation of Matti Vuorre’s approach\nSummary\n\nGaussian + binary exposure simulations\nNaive\nFixed effects\nMultilevel\nMultilevel Mundlak\nMultilevel Latent Mundlak\nSummary\n\n\nIn a recent lecture, Richard McElreath gave an introduction to Mundlak devices (code). Briefly, social scientists would often like to remove confounding at the group level.1\n\n\nShow code\n\nknitr::include_graphics(\"mundlak.jpg\")\n\n\n\nYair Mundlak2 in his Full Luxury Bayes outfit, courtesy of DALL-E.\nEconometricians and epidemiologists tend to use fixed effects regression to do so. Psychologists often use random effects regressions and adjust for the mean of the predictor of interest. The two solutions yield similar results for the within-group effect, but differ on how they treat group-level effects.\nArguably, there are more ways to mess up the random effects model, but it’s more flexible and efficient and it makes it easier to model e.g., cross-level interactions and effect heterogeneity.\nOne way you can mess up, is if your group sizes are small (e.g., sibling groups in a low fertility country) and your estimate of the group mean is a poor stand-in for the confounds you’d like to adjust for. A solution to this is to estimate the latent group mean instead, i.e. to account for the fact that we are estimating it with uncertainty. Doing so3 is fairly easy in Stan, but it’s less clear how to do it with everyone’s favourite Stan interface, brms.\nIn which way does this sampling error at the group level bias your results? It attenuates your estimate of b_Xbar (Lüdtke’s bias). I thought it would also bias my estimate of b_X, because I’m underadjusting by ignoring the measurement error in my covariate. That is not so. Why? To get the intuition, it helped me to consider the case where I first subtract Xbar from X. Xdiff is necessarily uncorrelated with Xbar, meaning any changes in the association of Xbar and Y (which we get by modelling sampling error) are irrelevant to Y ~ Xdiff.\nI think I’m not the only who is confused about this. Matti Vuorre shared a solution to center X with the latent group mean using brms’ non-linear syntax. Centering with the latent group mean is not necessary though. Also, I tried the syntax and it doesn’t correctly recover simulated effects.4\nMy two attempts at a solution:\nUsing me()\nCalculate the group mean Xbar, e.g. df %>% group_by(id) %>% mutate(Xbar = mean(X)).\nCalculate the standard error of the mean , e.g. df %>% group_by(id) %>% mutate(Xse = sd(X)/sqrt(n())).\nAdjust for the term me(Xbar, Xse, gr = id) explicitly specifying the grouping variable.\nProfit\nUsing mi()\nDuplicating the exposure X->X2\nCalculating the SE of the group mean outside the brms call\nUsing a second formula bf(X2 | mi(Xse) ~ (1|id)) in brms to estimate the latent group mean with shrinkage.\nAdjusting for mi(X2) in the regression on Y.\nHow did it go? High-level summary\nMy solutions seem to work, but they’re a little hacky. The me approach didn’t work for me at first until I specified the grouping variable, which makes sense.\nI had to add a constant .01 to Xse when SE was zero (in my Bernoulli exposure simulation).\nmi() cannot be combined with non-Gaussian variables. If I treat binary exposure X as Gaussian in a LPM, convergence was difficult and I had to fix the sigma to 0.01. The me() approach seems to work though. If I do all this, the three approaches (rethinking, me, mi) converge.\nI never managed to create a simulated scenaro where the latent Mundlak approach consistently outperforms the non-latent Mundlak for b_X. It improves estimates for b_Xbar (i.e. the group-level confound), so it’s doing what it’s supposed to, but b_X is always already estimated close to the true value with a non-latent Mundlak model. I tried to create conditions to make outperformance likely (small group sizes, much within-subject variation in X relative to between-subject variation, so that Xbar poorly correlates with the group-level confound). Am I missing something in my simulations or is latent group mean centering rarely worth the effort? I’d be glad for any pointers. Edit: Niclas Kuper gave me the pointer I needed, so I have clarified above that we do not expect estimation of b_X to improve by using latent group means.\nThe simulations and their results are documented in detail below. Click on the “Implementations” to see the model code.\nBernoulli simulations\nI started with Richard’s simulations of a binary outcome. I was not able to reproduce his model performances exactly and ended up increasing the simulated sample size to make estimates a bit more stable (to more clearly show bias vs. variance).\nUp to the latent Mundlak model, the brms equivalents, perform, well, equivalently. For the latent Mundlak model, the brms gets slightly different coefficients, but a similar overall result.\n\n\nShow code\n\nlibrary(tidyverse)\nlibrary(rethinking)\nlibrary(tidybayes)\nlibrary(tidybayes.rethinking)\nlibrary(brms)\noptions(brms.backend = \"cmdstanr\",  # I use the cmdstanr backend\n        mc.cores = 8, \n        brms.threads = 1,           # which allows me to multithread\n        brms.file_refit = \"on_change\") # this is useful when doing \n\nset.seed(201910)\n\n# families\nN_groups <- 300\na0 <- (-2)\nb_X <- (1)\nb_Z <- (-0.5)\nb_Ug <- (3)\n# 2 or more siblings\ng_sizes <- 2 + rpois(N_groups, lambda = 0.2) # sample into groups\ntable(g_sizes)\nN_id <- sum(g_sizes)\ng <- rep(1:N_groups, times = g_sizes)\nUg <- rnorm(N_groups, sd = 0.8) # group confounds\nX <- rnorm(N_id, Ug[g] ) # individual varying trait\nZ <- rnorm(N_groups) # group varying trait (observed)\nY <- rbern(N_id, p=inv_logit( a0 + b_X * X + b_Ug*Ug[g] + b_Z*Z[g] ) )\n\n# glm(Y ~ X + Z[g] + Ug[g], binomial())\n# glm(Y ~ X + Z[g], binomial())\ngroups <- tibble(id = factor(1:N_groups), Ug, Z)\nsim <- tibble(id = factor(g), X, Y) %>% full_join(groups, by = \"id\") %>% arrange(id) %>% group_by(id) %>% \n  mutate(Xbar = mean(X)) %>% ungroup()\nsim %>% distinct(id, Ug, Xbar) %>% select(-id) %>% cor(use = \"p\")\nglm(Y ~ X + Z, data = sim, binomial(link = \"logit\"))\nglm(Y ~ Ug + X + Z, data = sim, binomial(link = \"logit\"))\nglm(Y ~ id + X + Z, data = sim, binomial(link = \"logit\"))\n\nsim <- sim %>% group_by(id) %>% \n  mutate(Xse = sd(X)/sqrt(n())) %>% ungroup() %>% \n  mutate(X2 = X)\n\n\n\nImplementations of the various models\nNaive\nrethinking\n\n\nShow code\n\ndat <-  list(Y = Y, X = X, g = g, Ng = N_groups, Z = Z)\nmn <- ulam(\n    alist(\n        Y ~ bernoulli(p),\n        logit(p) <- a + b_X*X + b_Z*Z[g],\n        a ~ dnorm(0,2),\n        c(b_X,b_Z) ~ dnorm(0,1)\n    ) , data=dat , chains=4 , cores=4 )\nsummary(mn)\n\nmn %>% gather_draws(b_X, b_Z) %>% mean_hdci()\n\n\nbrms\n\n\nShow code\n\nb_mn <- brm(Y ~ X + Z, data = sim,\n            family = bernoulli(),\n            prior = c(\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"X\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"Z\"),\n              set_prior(\"normal(0, 2)\", class = \"Intercept\")))\nb_mn\n\nb_mn %>% gather_draws(b_X, b_Z) %>% mean_hdci()\n\n\nFixed effects\nrethinking\n\n\nShow code\n\ndat <-  list(Y = Y, X = X, g = g, Ng = N_groups, Z = Z)\n# fixed effects\nmf <- ulam(alist(\n  Y ~ bernoulli(p),\n  logit(p) <- a[g] + b_X*X + b_Z*Z[g],\n  a[g] ~ dnorm(0,1.5), # can't get it to work using dunif(-1,1)\n  c(b_X,b_Z) ~ dnorm(0,1)\n), data=dat, chains = 4, cores=4 )\nsummary(mf)\n\nmf %>% gather_draws(b_X, b_Z) %>% mean_hdci()\n\n\nbrms\n\n\nShow code\n\nb_mf <- brm(Y ~ 1 + id + X + Z, data = sim,\n            family = bernoulli(),\n            prior = c(\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"X\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"Z\")\n              ,set_prior(\"uniform(-1, 1)\",lb = -1, ub = 1, class = \"b\"))\n            # , sample_prior = \"only\"\n            )\n\n# b_mf %>% gather_draws(`b_id.*`, regex=T) %>% \n#   ggplot(aes(inv_logit(.value))) + geom_histogram(binwidth = .01)\nb_mf %>% gather_draws(b_X, b_Z) %>% mean_hdci()\n\n\nMultilevel\nRethinking\n\n\nShow code\n\n# varying effects (non-centered - next week! )\nmr <- ulam(\n  alist(\n    Y ~ bernoulli(p),\n    logit(p) <- a[g] + b_X*X + b_Z*Z[g],\n    transpars > vector[Ng]:a <<- abar + z*tau,\n    z[g] ~ dnorm(0,1),\n    c(b_X,b_Z) ~ dnorm(0,1),\n    abar ~ dnorm(0,1),\n    tau ~ dexp(1)\n  ), data=dat , chains=4, cores=4, sample=TRUE)\n\nmr %>% gather_draws(b_X, b_Z) %>% mean_hdci()\n\n\nbrms\n\n\nShow code\n\nb_mr <- brm(Y ~ (1|id) + X + Z, data = sim,\n            family = bernoulli(),\n            prior = c(\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"X\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"Z\"),\n              set_prior(\"exponential(1)\", class = \"sd\")))\nb_mr\n\nb_mr %>% gather_draws(b_X, b_Z) %>% mean_hdci()\n\n\nMultilevel Mundlak\n\n\nShow code\n\n# The Mundlak Machine\nxbar <- sapply(1:N_groups, function(j) mean (X[g==j]))\ndat$Xbar <- xbar\nmrx <- ulam(\n  alist(\n    Y ~ bernoulli(p),\n    logit (p) <- a[g] + b_X*X + b_Z*Z[g] + buy*Xbar[g],\n    transpars> vector[Ng]:a <<- abar + z*tau, \n    z[g] ~ dnorm(0,1),\n    c(b_X, buy, b_Z) ~ dnorm(0,1),\n    abar ~ dnorm(0,1),\n    tau ~ dexp(1)\n  ),\ndata=dat, chains=4, cores=4 ,\nsample=TRUE )\n\nmrx %>% gather_draws(b_X, b_Z) %>% mean_hdci()\n\n\nbrms\n\n\nShow code\n\nb_mrx <- brm(Y ~ (1|id) + X + Z + Xbar, data = sim,\n            family = bernoulli(),\n            prior = c(\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"X\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"Z\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"Xbar\"),\n              set_prior(\"exponential(1)\", class = \"sd\")))\nb_mrx\n\nb_mrx %>% gather_draws(b_X, b_Z) %>% mean_hdci()\n\n\nsubtracting the group mean instead of adjusting for it\n\n\nShow code\n\nb_mrc <- brm(Y ~ (1|id) + X + Z, data = sim %>% mutate(X = X - Xbar),\n            family = bernoulli(),\n            prior = c(\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"X\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"Z\"),\n              set_prior(\"exponential(1)\", class = \"sd\")))\nb_mrc\n\nb_mrc %>% gather_draws(b_X, b_Z) %>% mean_hdci()\n\n\nMultilevel Latent Mundlak\n\n\nShow code\n\n# The Latent Mundlak Machine\nmru <- ulam(\n  alist(\n    # y model \n    Y ~ bernoulli(p),\n    logit(p) <- a[g] + b_X*X + b_Z*Z[g] + buy*u[g],\n    transpars> vector[Ng]:a <<- abar + z*tau,\n    # X model\n    X ~ normal(mu,sigma),\n    mu <- aX + bux*u[g],\n    vector[Ng]:u ~ normal (0,1),\n    \n    # priors\n    z[g] ~ dnorm(0,1),\n    c(aX, b_X, buy, b_Z) ~ dnorm(0, 1),\n    bux ~ dexp(1),\n    abar ~ dnorm (0,1),\n    tau ~ dexp(1),\n    sigma ~ dexp(1)\n  ),\n  data = dat, chains = 4, cores=4, sample=TRUE)\n\nmru %>% gather_draws(b_X, b_Z) %>% mean_hdci()\n\n\nbrms\nbrms me() measurement error notation\n\n\nShow code\n\nb_mru_gr <- brm(Y ~ 1 +(1|id) + X + Z + me(Xbar, Xse, gr = id), data = sim, \n                family = bernoulli(),\n             prior = c(\n                 set_prior(\"normal(0, 1)\", class = \"b\", coef = \"X\"),\n                 set_prior(\"normal(0, 1)\", class = \"b\", coef = \"Z\"),\n                 set_prior(\"normal(0, 1)\", class = \"b\", coef = \"meXbarXsegrEQid\"),\n                 set_prior(\"exponential(1)\", class = \"sd\"),\n                 set_prior(\"exponential(1)\", class = \"sdme\")))\n\n\nbrms mi() missingness notation\n\n\nShow code\n\nb_mru_mi <- brm(bf(Y ~ (1|id) + X + Z + mi(X2), family = bernoulli()) +\n             bf(X2 | mi(Xse) ~ (1|id), \n                family = gaussian()), data = sim,\n            prior = c(\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"X\", resp = \"Y\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"Z\", resp = \"Y\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"miX2\", resp = \"Y\"),\n              set_prior(\"exponential(1)\", class = \"sd\", resp = \"Y\"),\n              set_prior(\"exponential(1)\", class = \"sd\", resp = \"X2\"),\n              set_prior(\"exponential(1)\", class = \"sigma\", resp = \"X2\")))\nb_mru_mi\n\nb_mru_mi %>% gather_draws(b_Y_X, b_Y_Z) %>% mean_hdci()\n\n\nnon-working implementation of Matti Vuorre’s approach\nThis is the approach Matti Vuorre posted on his blog, except that I rearranged to instead the slope of Xbar. This approach does not work at all, as far as I can tell.\n\n\nShow code\n\nlatent_formula <- bf(\n  # Y model\n  Y ~ interceptY  + bX*X + bXbar*Xbar + bZ*Z,\n  interceptY ~ 1 + (1 | id),\n  bX + bZ + bXbar ~ 1,\n\n  # Xbar model\n  nlf(X ~ Xbar),\n  Xbar ~ 1 + (1 | id),\n  nl = TRUE\n) #+\n#  bernoulli()\n\nget_prior(latent_formula, data = sim)\n\n                prior class      coef group resp dpar      nlpar lb\n student_t(3, 0, 2.5) sigma                                       0\n               (flat)     b                                   bX   \n               (flat)     b Intercept                         bX   \n               (flat)     b                                bXbar   \n               (flat)     b Intercept                      bXbar   \n               (flat)     b                                   bZ   \n               (flat)     b Intercept                         bZ   \n               (flat)     b                           interceptY   \n               (flat)     b Intercept                 interceptY   \n student_t(3, 0, 2.5)    sd                           interceptY  0\n student_t(3, 0, 2.5)    sd              id           interceptY  0\n student_t(3, 0, 2.5)    sd Intercept    id           interceptY  0\n               (flat)     b                                 Xbar   \n               (flat)     b Intercept                       Xbar   \n student_t(3, 0, 2.5)    sd                                 Xbar  0\n student_t(3, 0, 2.5)    sd              id                 Xbar  0\n student_t(3, 0, 2.5)    sd Intercept    id                 Xbar  0\n ub       source\n         default\n         default\n    (vectorized)\n         default\n    (vectorized)\n         default\n    (vectorized)\n         default\n    (vectorized)\n         default\n    (vectorized)\n    (vectorized)\n         default\n    (vectorized)\n         default\n    (vectorized)\n    (vectorized)\n\nShow code\n\nb_mru_mv <- brm(latent_formula, data = sim,\n            prior = c(\n              set_prior(\"normal(0, 1)\", class = \"b\", nlpar = \"bX\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", nlpar = \"bZ\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", nlpar = \"bXbar\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", nlpar = \"Xbar\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", nlpar = \"interceptY\"),\n              set_prior(\"exponential(1)\", class = \"sd\", nlpar = \"interceptY\"),\n              set_prior(\"exponential(1)\", class = \"sd\", nlpar = \"Xbar\")))\n\n\nSAMPLING FOR MODEL '3f860d32989dccf8dfe1419c5089d23e' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000188 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.88 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 57.3365 seconds (Warm-up)\nChain 1:                43.5455 seconds (Sampling)\nChain 1:                100.882 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL '3f860d32989dccf8dfe1419c5089d23e' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0.000173 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 1.73 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 52.9574 seconds (Warm-up)\nChain 2:                73.3569 seconds (Sampling)\nChain 2:                126.314 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL '3f860d32989dccf8dfe1419c5089d23e' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0.000139 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 1.39 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 69.7308 seconds (Warm-up)\nChain 3:                62.2176 seconds (Sampling)\nChain 3:                131.948 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL '3f860d32989dccf8dfe1419c5089d23e' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0.000137 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 1.37 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 63.9066 seconds (Warm-up)\nChain 4:                69.5717 seconds (Sampling)\nChain 4:                133.478 seconds (Total)\nChain 4: \n\nShow code\n\nb_mru_mv\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Y ~ interceptY + bX * X + bXbar * Xbar + bZ * Z \n         interceptY ~ 1 + (1 | id)\n         bX ~ 1\n         bZ ~ 1\n         bXbar ~ 1\n         X ~ Xbar\n         Xbar ~ 1 + (1 | id)\n   Data: sim (Number of observations: 646) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~id (Number of levels: 300) \n                         Estimate Est.Error l-95% CI u-95% CI Rhat\nsd(interceptY_Intercept)     0.20      0.11     0.01     0.36 1.12\nsd(Xbar_Intercept)           0.55      0.44     0.05     1.75 1.02\n                         Bulk_ESS Tail_ESS\nsd(interceptY_Intercept)       30      194\nsd(Xbar_Intercept)            242      255\n\nPopulation-Level Effects: \n                     Estimate Est.Error l-95% CI u-95% CI Rhat\ninterceptY_Intercept     0.22      0.51    -0.95     1.24 1.01\nbX_Intercept             0.08      0.83    -1.55     1.73 1.04\nbZ_Intercept            -0.03      0.02    -0.07     0.02 1.00\nbXbar_Intercept          0.11      0.83    -1.52     1.74 1.05\nXbar_Intercept           0.02      0.75    -1.45     1.53 1.01\n                     Bulk_ESS Tail_ESS\ninterceptY_Intercept     1921     1750\nbX_Intercept               79     1543\nbZ_Intercept             2207     2403\nbXbar_Intercept            52     1353\nXbar_Intercept           1418     1121\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.31      0.01     0.29     0.34 1.00     1625     1830\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nSummary\n\n\nShow code\n\ndraws <- bind_rows(\n  rethinking_naive = mn %>% gather_draws(b_X, b_Z),\n  rethinking_fixed = mf %>% gather_draws(b_X, b_Z),\n  rethinking_multilevel = mr %>% gather_draws(b_X, b_Z),\n  rethinking_mundlak = mrx %>% gather_draws(b_X, b_Z, buy),\n  brms_naive = b_mn %>% gather_draws(b_X, b_Z),\n  brms_fixed = b_mf %>% gather_draws(b_X, b_Z),\n  brms_multilevel = b_mr %>% gather_draws(b_X, b_Z),\n  brms_mundlak = b_mrx %>% gather_draws(b_X, b_Z, b_Xbar),\n  brms_mundlak_centered = b_mrc %>% gather_draws(b_X, b_Z),\n  rethinking_latent_mundlak = mru %>% gather_draws(b_X, b_Z, buy),\n  brms_latent_mundlak = b_mru_gr %>% gather_draws(b_X, b_Z, bsp_meXbarXsegrEQid),\n  brms_latent_mundlak_mi = b_mru_mi %>% gather_draws(b_Y_X, b_Y_Z, bsp_Y_miX2),\n .id = \"model\") %>% \n  separate(model, c(\"package\", \"model\"), extra = \"merge\") %>% \n  mutate(model = fct_inorder(factor(model)),\n         .variable = str_replace(.variable, \"_Y\", \"\"),\n         .variable = recode(.variable, \n                            \"buy\" = \"b_Xbar\",\n                            \"bsp_miX2\" = \"b_Xbar\",\n                            \"bsp_meXbarXsegrEQid\" = \"b_Xbar\"),\n         .variable = factor(.variable, levels = c(\"b_X\", \"b_Z\", \"b_Xbar\")))\n\ndraws <- draws %>% group_by(package, model, .variable) %>% \n  mean_hdci(.width = c(.95, .99)) %>% \n  ungroup()\n\nggplot(draws, aes(y = package, x = .value, xmin = .lower, xmax = .upper)) +\n  geom_pointinterval(position = position_dodge(width = .4)) +\n  ggrepel::geom_text_repel(aes(label = if_else(.width == .95, sprintf(\"%.2f\", .value), NA_character_)), nudge_y = .1) +\n  geom_vline(aes(xintercept = true_val), linetype = 'dashed', data = tibble(true_val = c(b_X, b_Z, b_Ug - b_X), .variable = factor(c(\"b_X\", \"b_Z\", \"b_Xbar\"), levels = c(\"b_X\", \"b_Z\", \"b_Xbar\")))) +  scale_color_discrete(breaks = rev(levels(draws$model))) +\n  facet_grid(model ~ .variable, scales = \"free_x\") +\n  theme_bw() +\n  theme(legend.position = c(0.99,0.99),\n        legend.justification = c(1,1))\n\n\n\nFigure 1: Estimated coefficients and the true values (dashed line)\n\n\n\nFor some reason, I did not manage to specify a uniform prior in rethinking. I think this explains the poor showing for the rethinking fixed effects model. Fixed effects regression + Bayes is not really a happy couple.\nGaussian simulations\nThen, I simulated a Gaussian outcome instead. I didn’t run the rethinking models here, except for the latent Mundlak model.\n\n\nShow code\n\nset.seed(201910)\n# families\nN_groups <- 300\na0 <- (-2)\nb_X <- (1)\nb_Z <- (-0.5)\nb_Ug <- (3)\n# 2 or more siblings\ng_sizes <- 2 + rpois(N_groups, lambda = 0.2) # sample into groups\ntable(g_sizes)\nN_id <- sum(g_sizes)\ng <- rep(1:N_groups, times = g_sizes)\nUg <- rnorm(N_groups, sd = 0.8) # group confounds\nX <- rnorm(N_id, Ug[g] ) # individual varying trait\nZ <- rnorm(N_groups) # group varying trait (observed)\nY <- a0 + b_X * X + b_Ug*Ug[g] + b_Z*Z[g] + rnorm(N_id)\n\ngroups <- tibble(id = factor(1:N_groups), Ug, Z)\nsim <- tibble(id = factor(g), X, Y) %>% full_join(groups, by = \"id\") %>% arrange(id) %>% group_by(id) %>% \n  mutate(Xbar = mean(X)) %>% ungroup()\nsim %>% distinct(id, Ug, Xbar) %>% select(-id) %>% cor(use = \"p\")\nlm(Y ~ X + Z, data = sim)\nlm(Y ~ Ug + X + Z, data = sim)\nlm(Y ~ id + X + Z, data = sim)\n\nsim <- sim %>% group_by(id) %>% \n  mutate(Xse = sd(X)/sqrt(n())) %>% ungroup() %>% \n  mutate(X2 = X)\n\n\n\nImplementations of the various models\nNaive\nbrms\n\n\nShow code\n\nb_mn <- brm(Y ~ X + Z, data = sim,\n            prior = c(\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"X\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"Z\"),\n              set_prior(\"normal(0, 2)\", class = \"Intercept\")))\nb_mn\n\nb_mn %>% gather_draws(b_X, b_Z) %>% mean_hdci()\n\n\nFixed effects\nbrms\n\n\nShow code\n\nb_mf <- brm(Y ~ 1 + id + X + Z, data = sim,\n            prior = c(\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"X\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"Z\"),\n              set_prior(\"normal(0, 2)\", class = \"b\"))\n            # , sample_prior = \"only\"\n            )\n\n# b_mf %>% gather_draws(`b_id.*`, regex=T) %>% \n#   ggplot(aes(inv_logit(.value))) + geom_histogram(binwidth = .01)\nb_mf %>% gather_draws(b_X, b_Z) %>% mean_hdci()\n\n\nMultilevel\nbrms\n\n\nShow code\n\nb_mr <- brm(Y ~ (1|id) + X + Z, data = sim,\n            prior = c(\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"X\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"Z\"),\n              set_prior(\"exponential(1)\", class = \"sd\")))\nb_mr\n\nb_mr %>% gather_draws(b_X, b_Z) %>% mean_hdci()\n\n\nMultilevel Mundlak\nbrms\n\n\nShow code\n\nb_mrx <- brm(Y ~ (1|id) + X + Z + Xbar, data = sim,\n            prior = c(\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"X\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"Z\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"Xbar\"),\n              set_prior(\"exponential(1)\", class = \"sd\")))\nb_mrx\n\nb_mrx %>% gather_draws(b_X, b_Z) %>% mean_hdci()\n\n\nsubtracting the group mean instead of adjusting for it\n\n\nShow code\n\nb_mrc <- brm(Y ~ (1|id) + X + Z, data = sim %>% mutate(X = X - Xbar),\n            prior = c(\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"X\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"Z\"),\n              set_prior(\"exponential(1)\", class = \"sd\")))\nb_mrc\n\nb_mrc %>% gather_draws(b_X, b_Z) %>% mean_hdci()\n\n\nMultilevel Latent Mundlak\nrethinking\n\n\nShow code\n\ndat <-  list(Y = Y, X = X, g = g, Ng = N_groups, Z = Z)\n# The Latent Mundlak Machine\nmru <- ulam(\n  alist(\n    # y model \n    Y ~ normal(muY, sigmaY),\n    muY <- a[g] + b_X*X + b_Z*Z[g] + buy*u[g],\n    transpars> vector[Ng]:a <<- abar + z*tau,\n    # X model\n    X ~ normal(mu,sigma),\n    mu <- aX + bux*u[g],\n    vector[Ng]:u ~ normal (0,1),\n    \n    # priors\n    z[g] ~ dnorm(0,1),\n    c(aX, b_X, buy, b_Z) ~ dnorm(0, 1),\n    bux ~ dexp(1),\n    abar ~ dnorm (0,1),\n    tau ~ dexp(1),\n    c(sigma, sigmaY) ~ dexp(1)\n  ),iter = 2000,\n  data = dat, chains = 4, cores=4, sample=TRUE)\n\nmru %>% gather_draws(b_X, b_Z) %>% mean_hdci()\n\n\nbrms\nbrms me() measurement error notation\n\n\nShow code\n\nb_mru_gr <- brm(Y ~ 1 +(1|id) + X + Z + me(Xbar, Xse, gr = id), data = sim, \n             prior = c(\n                 set_prior(\"normal(0, 1)\", class = \"b\", coef = \"X\"),\n                 set_prior(\"normal(0, 1)\", class = \"b\", coef = \"Z\"),\n                 set_prior(\"normal(0, 1)\", class = \"b\", coef = \"meXbarXsegrEQid\"),\n                 set_prior(\"exponential(1)\", class = \"sd\"),\n                 set_prior(\"exponential(1)\", class = \"sdme\")))\n\n\nbrms mi() missingness notation\n\n\nShow code\n\nb_mru_mi <- brm(bf(Y ~ (1|id) + X + Z + mi(X2)) +\n             bf(X2 | mi(Xse) ~ (1|id), family = gaussian()), data = sim %>% mutate(X2 = X),\n            prior = c(\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"X\", resp = \"Y\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"Z\", resp = \"Y\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"miX2\", resp = \"Y\"),\n              set_prior(\"exponential(1)\", class = \"sd\", resp = \"Y\"),\n              set_prior(\"exponential(1)\", class = \"sd\", resp = \"X2\"),\n              set_prior(\"exponential(1)\", class = \"sigma\", resp = \"X2\")))\nb_mru_mi\n\nb_mru_mi %>% gather_draws(b_Y_X, b_Y_Z) %>% mean_hdci()\n\n\nworking implementation of the mi model using nonlinear syntax\n\n\nShow code\n\nlatent_formula <- bf(nl = TRUE,\n  # Y model\n  Y ~ interceptY  + bX*X + X2l + bZ*Z,\n  X2l ~ 0 + mi(X2),\n  interceptY ~ 1 + (1 | id),\n  bX + bZ ~ 1, family = gaussian()) +\n  bf(X2 | mi(Xse) ~ 1 + (1|id), family = gaussian()) #+\n#  bernoulli()\n\n# get_prior(latent_formula, data = sim)\n\nb_mru_minl <- brm(latent_formula, data = sim,\n            prior = c(\n              set_prior(\"normal(0, 1)\", class = \"b\", nlpar = \"X2l\", resp = \"Y\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", nlpar = \"bX\", resp = \"Y\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", nlpar = \"bZ\", resp = \"Y\"),\n              # set_prior(\"normal(0, 1)\", class = \"b\", nlpar = \"bX2\", resp = \"Y\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", nlpar = \"interceptY\", resp = \"Y\"),\n              set_prior(\"exponential(1)\", class = \"sd\", nlpar = \"interceptY\", resp = \"Y\"),\n              set_prior(\"normal(0, 1)\", class = \"Intercept\", resp = \"X2\"),\n              set_prior(\"exponential(1)\", class = \"sd\", resp = \"X2\")))\nb_mru_minl\n\n\nnon-working implementation of Matti Vuorre’s approach\nThis is the approach Matti Vuorre posted on his blog, except that I rearranged to instead the slope of Xbar. This approach does not work at all, as far as I can tell.\n\n\nShow code\n\nlatent_formula <- bf(\n  # Y model\n  Y ~ interceptY  + bX*X + bXbar*Xbar + bZ*Z,\n  interceptY ~ 1 + (1 | id),\n  bX + bZ + bXbar ~ 1,\n\n  # Xbar model\n  nlf(X ~ Xbar),\n  Xbar ~ 1 + (1 | id),\n  nl = TRUE\n) #+\n#  bernoulli()\n\nget_prior(latent_formula, data = sim)\n\nb_mru_mv <- brm(latent_formula, data = sim,\n            prior = c(\n              set_prior(\"normal(0, 1)\", class = \"b\", nlpar = \"bX\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", nlpar = \"bZ\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", nlpar = \"bXbar\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", nlpar = \"Xbar\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", nlpar = \"interceptY\"),\n              set_prior(\"exponential(1)\", class = \"sd\", nlpar = \"interceptY\"),\n              set_prior(\"exponential(1)\", class = \"sd\", nlpar = \"Xbar\")))\n\n\n\n\nShow code\n\nb_mru_mv\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Y ~ interceptY + bX * X + bXbar * Xbar + bZ * Z \n         interceptY ~ 1 + (1 | id)\n         bX ~ 1\n         bZ ~ 1\n         bXbar ~ 1\n         X ~ Xbar\n         Xbar ~ 1 + (1 | id)\n   Data: sim (Number of observations: 646) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~id (Number of levels: 300) \n                         Estimate Est.Error l-95% CI u-95% CI Rhat\nsd(interceptY_Intercept)     1.76      1.19     0.06     3.43 1.35\nsd(Xbar_Intercept)           1.54      0.99     0.03     3.67 1.19\n                         Bulk_ESS Tail_ESS\nsd(interceptY_Intercept)       10       88\nsd(Xbar_Intercept)             16       84\n\nPopulation-Level Effects: \n                     Estimate Est.Error l-95% CI u-95% CI Rhat\ninterceptY_Intercept    -0.72      0.92    -2.25     1.14 1.03\nbX_Intercept             0.11      1.10    -2.08     2.16 1.24\nbZ_Intercept            -0.41      0.18    -0.75    -0.04 1.01\nbXbar_Intercept          0.11      1.08    -1.98     2.13 1.24\nXbar_Intercept           0.06      1.17    -1.80     3.70 1.19\n                     Bulk_ESS Tail_ESS\ninterceptY_Intercept      190      554\nbX_Intercept               12      140\nbZ_Intercept             1252     1963\nbXbar_Intercept            12      136\nXbar_Intercept             15       51\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.47      0.06     1.37     1.59 1.02      257       96\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nSummary\n\n\nShow code\n\ndraws <- bind_rows(\n  brms_naive = b_mn %>% gather_draws(b_X, b_Z),\n  brms_fixed = b_mf %>% gather_draws(b_X, b_Z),\n  brms_multilevel = b_mr %>% gather_draws(b_X, b_Z),\n  brms_mundlak = b_mrx %>% gather_draws(b_X, b_Z, b_Xbar),\n  brms_mundlak_centered = b_mrc %>% gather_draws(b_X, b_Z),\n  rethinking_latent_mundlak = mru %>% gather_draws(b_X, b_Z, buy),\n  brms_latent_mundlak = b_mru_gr %>% gather_draws(b_X, b_Z, bsp_meXbarXsegrEQid),\n  brms_latent_mundlak_mi = b_mru_mi %>% gather_draws(b_Y_X, b_Y_Z, bsp_Y_miX2),\n .id = \"model\") %>% \n  separate(model, c(\"package\", \"model\"), extra = \"merge\") %>% \n  mutate(model = fct_inorder(factor(model)),\n         .variable = str_replace(.variable, \"_Y\", \"\"),\n         .variable = recode(.variable, \n                            \"buy\" = \"b_Xbar\",\n                            \"bsp_miX2\" = \"b_Xbar\",\n                            \"bsp_meXbarXsegrEQid\" = \"b_Xbar\"),\n         .variable = factor(.variable, levels = c(\"b_X\", \"b_Z\", \"b_Xbar\")))\n\ndraws <- draws %>% group_by(package, model, .variable) %>% \n  mean_hdci(.width = c(.95, .99)) %>% \n  ungroup()\n\nggplot(draws, aes(y = package, x = .value, xmin = .lower, xmax = .upper)) +\n  geom_pointinterval(position = position_dodge(width = .4)) +\n  ggrepel::geom_text_repel(aes(label = if_else(.width == .95, sprintf(\"%.2f\", .value), NA_character_)), nudge_y = .1) +\n  geom_vline(aes(xintercept = true_val), linetype = 'dashed', data = tibble(true_val = c(b_X, b_Z, b_Ug - b_X), .variable = factor(c(\"b_X\", \"b_Z\", \"b_Xbar\"), levels = c(\"b_X\", \"b_Z\", \"b_Xbar\")))) +  scale_color_discrete(breaks = rev(levels(draws$model))) +\n  facet_grid(model ~ .variable, scales = \"free_x\") +\n  theme_bw() +\n  theme(legend.position = c(0.99,0.99),\n        legend.justification = c(1,1))\n\n\n\nFigure 2: Estimated coefficients and the true values (dashed line)\n\n\n\nGaussian + binary exposure simulations\nFinally, I simulated a Gaussian outcome and a binary exposure (X). Especially in small groups, the group average of a binary variable can be pretty far off. I didn’t run the rethinking models here, except for the latent Mundlak model.\nI was not able to implement this model in brms. brms does not permit the specification of a binary variable as missing using mi(). And adjusting for Xbar using a linear probability model did not work.\n\n\nShow code\n\nset.seed(201910)\n# families\nN_groups <- 300\na0 <- (-2)\nb_X <- (1)\nb_Z <- (-0.5)\nb_Ug <- (3)\n# 2 or more siblings\ng_sizes <- 2 + rpois(N_groups, lambda = 0.2) # sample into groups\ntable(g_sizes)\nN_id <- sum(g_sizes)\ng <- rep(1:N_groups, times = g_sizes)\nUg <- rnorm(N_groups, sd = 0.8) # group confounds\nX <- rbern(N_id, p=inv_logit(rnorm(N_id, Ug[g] ) ) )   # individual varying trait\ntable(X)\n\nZ <- rnorm(N_groups) # group varying trait (observed)\nY <- a0 + b_X * X + b_Ug*Ug[g] + b_Z*Z[g] + rnorm(N_id)\n\ngroups <- tibble(id = factor(1:N_groups), Ug, Z)\nsim <- tibble(id = factor(g), X, Y) %>% full_join(groups, by = \"id\") %>% arrange(id) %>% group_by(id) %>% \n  mutate(Xbar = mean(X)) %>% ungroup()\nsim %>% distinct(id, Ug, Xbar) %>% select(-id) %>% cor(use = \"p\")\nlm(Y ~ X + Z, data = sim)\nlm(Y ~ Ug + X + Z, data = sim)\nlm(Y ~ id + X + Z, data = sim)\n\nsim <- sim %>% group_by(id) %>% \n  mutate(Xse = sd(X)/sqrt(n())) %>% ungroup() %>% \n  mutate(X2 = X)\n\n\n\nImplementations of the various models\nNaive\nbrms\n\n\nShow code\n\nb_mn <- brm(Y ~ X + Z, data = sim,\n            prior = c(\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"X\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"Z\"),\n              set_prior(\"normal(0, 2)\", class = \"Intercept\")))\nb_mn\n\nb_mn %>% gather_draws(b_X, b_Z) %>% mean_hdci()\n\n\nFixed effects\nbrms\n\n\nShow code\n\nb_mf <- brm(Y ~ 1 + id + X + Z, data = sim,\n            prior = c(\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"X\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"Z\"),\n              set_prior(\"normal(0, 2)\", class = \"b\"))\n            # , sample_prior = \"only\"\n            )\n\n# b_mf %>% gather_draws(`b_id.*`, regex=T) %>% \n#   ggplot(aes(inv_logit(.value))) + geom_histogram(binwidth = .01)\nb_mf %>% gather_draws(b_X, b_Z) %>% mean_hdci()\n\n\nMultilevel\nbrms\n\n\nShow code\n\nb_mr <- brm(Y ~ (1|id) + X + Z, data = sim,\n            prior = c(\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"X\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"Z\"),\n              set_prior(\"exponential(1)\", class = \"sd\")))\nb_mr\n\nb_mr %>% gather_draws(b_X, b_Z) %>% mean_hdci()\n\n\nMultilevel Mundlak\nbrms\n\n\nShow code\n\nb_mrx <- brm(Y ~ (1|id) + X + Z + Xbar, data = sim,\n            prior = c(\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"X\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"Z\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"Xbar\"),\n              set_prior(\"exponential(1)\", class = \"sd\")))\nb_mrx\n\nb_mrx %>% gather_draws(b_X, b_Z) %>% mean_hdci()\n\n\nsubtracting the group mean instead of adjusting for it\n\n\nShow code\n\nb_mrc <- brm(Y ~ (1|id) + X + Z, data = sim %>% mutate(X = X - Xbar),\n            prior = c(\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"X\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"Z\"),\n              set_prior(\"exponential(1)\", class = \"sd\")))\nb_mrc\n\nb_mrc %>% gather_draws(b_X, b_Z) %>% mean_hdci()\n\n\nMultilevel Latent Mundlak\nrethinking\n\n\nShow code\n\ndat <-  list(Y = Y, X = X, g = g, Ng = N_groups, Z = Z)\n# The Latent Mundlak Machine\nmru <- ulam(\n  alist(\n    # y model \n    Y ~ normal(muY, sigmaY),\n    muY <- a[g] + b_X*X + b_Z*Z[g] + buy*u[g],\n    transpars> vector[Ng]:a <<- abar + z*tau,\n    # X model\n    X ~ bernoulli(p),\n    logit(p) <- aX + bux*u[g],\n    vector[Ng]:u ~ normal (0,1),\n    \n    # priors\n    z[g] ~ dnorm(0,1),\n    c(aX, b_X, buy, b_Z) ~ dnorm(0, 1),\n    bux ~ dexp(1),\n    abar ~ dnorm (0,1),\n    tau ~ dexp(1),\n    c(sigma, sigmaY) ~ dexp(1)\n  ),\n  data = dat, chains = 4, cores=4, sample=TRUE)\n\nmru %>% gather_draws(b_X, b_Z) %>% mean_hdci()\n\n\nbrms\nbrms me() measurement error notation\nI had to add .01 to the Xse because brms does not permit 0 measurement error (when modelling measurement error).\n\n\nShow code\n\nb_mru_gr <- brm(Y ~ 1 +(1|id) + X + Z + me(Xbar, Xse, gr = id), data = sim %>% mutate(Xse = Xse + 0.01), \n             prior = c(\n                 set_prior(\"normal(0, 1)\", class = \"b\", coef = \"X\"),\n                 set_prior(\"normal(0, 1)\", class = \"b\", coef = \"Z\"),\n                 set_prior(\"normal(0, 1)\", class = \"b\", coef = \"meXbarXsegrEQid\"),\n                 set_prior(\"exponential(1)\", class = \"sd\"),\n                 set_prior(\"exponential(1)\", class = \"sdme\")))\n\n\nbrms mi() missingness notation\nbrms does not support the mi() notation for binary variables. Also, it does not accept zero measurement error, so I averaged across groups to take the mean Xse.\n\n\nShow code\n\nb_mru_mi <- brm(bf(Y ~ (1|id) + X + Z + mi(X2)) +\n             bf(X2 | mi(Xse) ~ (1|id)), family = gaussian(), data = sim %>% mutate(Xse = Xse + 0.01), iter = 4000,\n            prior = c(\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"X\", resp = \"Y\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"Z\", resp = \"Y\"),\n              set_prior(\"normal(0, 1)\", class = \"b\", coef = \"miX2\", resp = \"Y\"),\n              set_prior(\"exponential(1)\", class = \"sd\", resp = \"Y\"),\n              set_prior(\"exponential(1)\", class = \"sd\", resp = \"X2\"),\n              set_prior(\"constant(0.01)\", class = \"sigma\", resp = \"X2\")))\nb_mru_mi\n\nb_mru_mi %>% gather_draws(b_Y_X, b_Y_Z) %>% mean_hdci()\n\n\nSummary\n\n\nShow code\n\ndraws <- bind_rows(\n  brms_naive = b_mn %>% gather_draws(b_X, b_Z),\n  brms_fixed = b_mf %>% gather_draws(b_X, b_Z),\n  brms_multilevel = b_mr %>% gather_draws(b_X, b_Z),\n  brms_mundlak = b_mrx %>% gather_draws(b_X, b_Z, b_Xbar),\n  brms_mundlak_centered = b_mrc %>% gather_draws(b_X, b_Z),\n  rethinking_latent_mundlak = mru %>% gather_draws(b_X, b_Z, buy),\n  brms_latent_mundlak = b_mru_gr %>% gather_draws(b_X, b_Z, bsp_meXbarXsegrEQid),\n  brms_latent_mundlak_mi = b_mru_mi %>% gather_draws(b_Y_X, b_Y_Z, bsp_Y_miX2),\n .id = \"model\") %>% \n  separate(model, c(\"package\", \"model\"), extra = \"merge\") %>% \n  mutate(model = fct_inorder(factor(model)),\n         .variable = str_replace(.variable, \"_Y\", \"\"),\n         .variable = recode(.variable, \n                            \"buy\" = \"b_Xbar\",\n                            \"bsp_miX2\" = \"b_Xbar\",\n                            \"bsp_meXbarXsegrEQid\" = \"b_Xbar\"),\n         .variable = factor(.variable, levels = c(\"b_X\", \"b_Z\", \"b_Xbar\")))\n\ndraws <- draws %>% group_by(package, model, .variable) %>% \n  mean_hdci(.width = c(.95, .99)) %>% \n  ungroup()\n\nggplot(draws, aes(y = package, x = .value, xmin = .lower, xmax = .upper)) +\n  geom_pointinterval(position = position_dodge(width = .4)) +\n  ggrepel::geom_text_repel(aes(label = if_else(.width == .95, sprintf(\"%.2f\", .value), NA_character_)), nudge_y = .1) +\n  geom_vline(aes(xintercept = true_val), linetype = 'dashed', data = tibble(true_val = c(b_X, b_Z, b_Ug - b_X), .variable = factor(c(\"b_X\", \"b_Z\", \"b_Xbar\"), levels = c(\"b_X\", \"b_Z\", \"b_Xbar\")))) +  scale_color_discrete(breaks = rev(levels(draws$model))) +\n  facet_grid(model ~ .variable, scales = \"free_x\") +\n  theme_bw() +\n  theme(legend.position = c(0.99,0.99),\n        legend.justification = c(1,1))\n\n\n\nFigure 3: Estimated coefficients and the true values (dashed line)\n\n\n\n\nthe group here can be an individual measured several times, a sibling group, a school, a nation, etc.↩︎\nMundlak, Y. 1978: On the pooling of time series and cross section data. Econometrica 46:69-85.↩︎\nWhat McElreath calls Full Luxury Bayes: Latent Mundlak machine↩︎\nI think the reason he thought it works is that in the data he used, from McNeish & Hamaker 2020, the group mean varies very little except for measurement error, so we expect it not to make a difference.↩︎\n",
    "preview": "posts/2023-03-11-latent-group-mean-centering-revisited/mundlak.jpg",
    "last_modified": "2023-03-20T14:12:40+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-03-05-multilevel-censored-location-scale-models/",
    "title": "Multilevel censored location-scale models",
    "description": "Simulating several models and their ability to recover within-subject and between-subject effects on scale",
    "author": [
      {
        "name": "Ruben C. Arslan",
        "url": "https://rubenarslan.github.io"
      }
    ],
    "date": "2023-03-13",
    "categories": [],
    "contents": "\n\nContents\nTwo questionable assumptions in the quantile regression\nSimulation results\nLessons learned\nSimulations\nLocation-scale within-subject\nLocation-scale between-subject\nCensored location-scale within-subject\nCensored location-scale between-subject\n\n\nRecently, I got mad at a ceiling.\n\n\nShow code\n\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\noptions(mc.cores = parallel::detectCores(),\n        brms.backend = \"cmdstanr\",\n        brms.file_refit = \"on_change\")\n\ntheme_set(theme_bw())\n\n\n\n\nShow code\n\nlibrary(tweetrmd)\ntweet_embed(\"https://twitter.com/rubenarslan/status/1632170569448685569\")\n\nMy new personality? I'm going to lean into being mad about low ceilings in happiness measures and elsewhere. pic.twitter.com/6Bu19pn2rm— Ruben C. Arslan (@rubenarslan) March 5, 2023\n\n\nLet me explain. Kahneman & Deaton (2010) reported that happiness plateaued at an income of $75,000 or higher. Their measure of happiness was a binary question that 450,000 people answered in daily, random phone calls to ~1,000 people. Killingsworth 2021 used his experience sampling data from more than 35,000 individuals responding ~50 times to refute this. The Killingsworth study had a better happiness measure, a sliding scale from very unhappy to very happy. He found a loglinear relationship across the entire income range1. He attributed Kahneman & Deaton’s (2010) plateau finding to a ceiling effect in their measure.\nNow, in a recent paper, Killingsworth, Kahneman, and Meller (2023) re-analyzed Killingsworth’s (2021) experience sampling data to determine the relationship between income and experienced well-being, happiness. In the reanalysis, KKM wanted to find out why the results differed in an “adversarial collaboration”.\nTo do so, they aggregated daily happiness in the Killingsworth data to derive the subject’s observed mean. Then they used a form of distributional regression, quantile regression, to predict not only the mean of happiness but also various quantiles by log(income). Specifically, they investigated whether the relationship to the mean plateaued above a yearly household income of $100,000. This was not the case, the relationship between income and mean happiness is loglinear above and below this threshold with near-identical slopes. It’s also a quite small relationship, log(income) explains less than 1% of the variance in happiness.\nHowever, they find that the relationship between log(income) and the 15% quantile does indeed flatten after $100,000. In my view, they inappropriately interpret this as evidence that there is an unhappy group of people for which money cannot alleviate suffering (above 100k).\nA better way to put it would have been: at higher log(income) the variability in average happiness is higher. That is, some people have a lot of money and spend it on holidays, household help, and time with their loved ones. Others buy a social media platform and make themselves miserable. But even if this statistical relationship is real2 and reflects a causal relationship3, it does not imply that there is one group of people whose unhappiness cannot be alleviated with money.\nNow, I’m not particularly well-acquainted with quantile regression. However, I’ve worked with multilevel location scale models a good deal in the past, a different form of distributional regression. In so doing, I’ve learned not to trust my intuition, or at least to calibrate with fake data simulation. So, I simulated to check a hunch.\nTwo questionable assumptions in the quantile regression\nMy conclusion is that KKM make at least two4 assumptions that I think are unlikely to hold and which could bias their estimate:\nthat the variance of happiness within-subjects is homogeneous\nthat their aggregated means of happiness are free of sampling error\nI was fairly sure that #1 is false, because, you know, I’ve seen happiness data, and because I am a human being who knows other human beings. I did not see their happiness data, mind you, which they only shared in aggregated and rounded form upon publication of KKM 2023, so we cannot check within-subject variance homogeneity.5\nI was less sure whether #2 has much impact with 51 days per subject on average.\nSimulation results\nEven with on average 51 days per subject and homogeneous within-subject variance, sampling error in the mean attenuates the relationship between x and sd(y) on the between-subject level. Especially at low levels of x, sampling error can still dominate the true sd(id) in y.\n\n\nShow code\n\nknitr::include_graphics(\"multilevel-censored-location-scale-models_files/figure-html5/BS_0_8_WS_0__means-22-1.png\")\n\n\n\nFigure 1: Compare the scatter of y around the regression line at the top left and the bottom left to see how sampling error creates a slight bias when estimating the effect on sd(y) on the between-subject level. From the second simulation below.\n\n\n\n\n\nShow code\n\nknitr::include_graphics(\"multilevel-censored-location-scale-models_files/figure-html5/BS_0_8_WS_0__bias-24-1.png\")\n\n\n\nFigure 2: A between-subject effect on the log(sd) of .8 reduces to .65 because of sampling variance in aggregated means.\n\n\n\nHeterogeneous within-subject variance can bias the estimate of the between-subject variance (i.e., what is really a strong tendency for an increased within-subject variance at higher x is mistaken for a weak tendency for an increase in between-subject variance at higher x).\n\n\nShow code\n\nknitr::include_graphics(\"multilevel-censored-location-scale-models_files/figure-html5/BS_0_WS_0_8__bias-1.png\")\n\n\n\nFigure 3: A within-subject effect on the log(sd) of .8 shows up as a .2 effect on the between-subject level.\n\n\n\nCensoring increases bias. When values above or below a certain cutoff (e.g., the maximum value of the scale is 100) get clipped, the bias in the aggregated between-subject model is exacerbated. Why? The aggregation is done without regard to censoring. So, a person with c(94, 95, 96, 95) gets 95, just like a person with c(90, 100, 100, 90). But if we model censoring at the within-subject level, we would estimate a higher true value for the second person (my 2-part model correctly recovers the true value). Killingsworth 2021 reports only 5.5% of responses at ceiling and the relationship with the mean is quite weak, so this probably did not hit hard. Still, aggregating loses relevant information (after aggregation “less than 0.5% of people in any income group had experienced well-being equal to the response ceiling, on average”).\n\n\nShow code\n\nknitr::include_graphics(\"multilevel-censored-location-scale-models_files/figure-html5/BS_0_8_WS_0_2_bias-24-1.png\")\n\n\n\nFigure 4: The true between-subject effect on the log(sd) is underestimated more (~.55 with censoring vs. .65 without censoring vis a vis a true value of .8), even though just 10% of values are right-censored.\n\n\n\n\n\nShow code\n\nknitr::include_graphics(\"multilevel-censored-location-scale-models_files/figure-html5/BS_0_WS_0_8_2_bias-1.png\")\n\n\n\nFigure 5: The between-subject effect on the log(sd) is overestimated more (~.3 censored vs. .16 non-censored vis a vis a true value of 0) in the censored within-subject model, even though just 12% of values are censored.\n\n\n\n\nTechnical note: There are Mplus (McNeish, 2020) and R-Stan (Martin, 2022) models to estimate two-part mixed effects location-scale models that allow simultaneous estimation of scale effects at both the within- and between-subject level. However, they do not implement censored distributions. My favourite package brms does, but does not natively permit scale effects on the between-subject level to be estimated. However, it allows us to estimate different sd(id) for subgroups. I formed septiles on x and used these as subgroups. Doing so allowed me to get a sort of poor man’s version of a censored two-part mixed effects location-scale model. A better man would have coded it up in raw Stan. As far as I can tell, this model both gets me closer to the between-subject effect than the aggregated model and gets me an unbiased estimate of the within-subject effect.\n\n\nShow code\n\nknitr::include_graphics(\"multilevel-censored-location-scale-models_files/figure-html5/BS_0_8_WS_0_2_betweensds-25-1.png\")\n\n\n\nFigure 6: Regressing x on the estimated sd(y) for the quantiles in my two-part model recovers the true between-subject effect even in the censored model.\n\n\n\nYou can find my simulations (well, one round) below.\nLessons learned\nKillingsworth 2021 was a wonderful paper. I think this quantile analysis and adversarial collaboration actually muddled the matter. In German, we of course have a word for this: Verschlimmbesserung.6 How come?\nI have an idea. Adversarial collaborations don’t just sound like an oxymoron. Especially with large status differences between adversaries, the interesting stuff happens behind the scenes. Without transparency, power, resources, and stubbornness can win over superior arguments.7 I think such disputes should be carried out in the open. And if you want to share data with someone you consider an adversary, may I suggest the following as insurance:\n\n\nShow code\n\nlibrary(tweetrmd)\ntweet_embed(\"https://twitter.com/rubenarslan/status/1631789472407859200\")\n\nIf you truly treat the other researcher as an adversary, perhaps you should try medieval hostage giving. Each sends the other a random half of their data. The holdout data and mutuality may help ensure good faith.— Ruben C. Arslan (@rubenarslan) March 3, 2023\n\n\n\nSimulations\nLocation-scale within-subject\nHere, mu(y) is a function of x, as is sigma(y) (only at the within-subject level).\n\n\nShow code\n\nfit_models <- function(b_mean,  b_sd_bs, b_sd_ws, y_ceiling) {\n  name = paste(\"BS\", b_sd_bs, \"WS\", b_sd_ws, if_else(y_ceiling<Inf,as.character(y_ceiling),\"\"))\n  rmdpartials::partial(\"fit_models.Rmd\", \n                       name = name,\n                       b_mean = b_mean,  \n                       b_sd_bs = b_sd_bs, \n                       b_sd_ws = b_sd_ws, \n                       y_ceiling = y_ceiling)\n}\n\n\n\n\nShow code\n\nfit_models(\n  b_mean = 1,  b_sd_bs = 0, b_sd_ws = 0.8, y_ceiling = Inf)\n\n\n\nShow code\n\nN <- 250\nn_days = 51\nset.seed(20191005)\npeople <- tibble(\n  id = 1:N,\n  x = rnorm(N))\nn_days_per_person = rpois(N, n_days)\n\n\n\n\nShow code\n\npeople <- people %>% \n  mutate(\n    mean_log_sd_y = -1 + b_sd_bs * x,\n    log_sd_y = 0 + b_sd_ws * x,\n    mean_y = rnorm(N, sd = exp(mean_log_sd_y)) + b_mean * x,\n    xQ = ntile(x, 6)\n    )\ndays <- people %>% \n  full_join(tibble(\n              id = rep(1:N, times = n_days_per_person)\n            ), by = \"id\", multiple = \"all\") %>% \n            mutate(\n              latent_y = rnorm(n(), \n                        mean = mean_y,\n                        sd = exp(log_sd_y)),\n              y = case_when(\n                latent_y >= y_ceiling ~ y_ceiling,\n                # latent_y <= -1.5 ~ -1.5,\n                TRUE ~ latent_y\n              ),\n              ycens = case_when(\n                latent_y >= y_ceiling ~ \"right\",\n                # latent_y <= -1.5 ~ \"left\",\n                TRUE ~ \"none\"\n              )\n            )\n\n\nPercentage censored: 0.00\n\n\nShow code\n\nggplot(days, aes(x, y)) +\n  geom_point(alpha = 0.3)\n\n\n\nFigure 7: Raw data\n\n\n\n\n\nShow code\n\nsel_ids <- c(43, 36, 8, 40, 88, 29, 11, 49, 84, 41, 98)\n\nggplot(days, aes(x, y)) +\n  geom_smooth(method = 'lm', color = \"gray\", se = F) + \n  geom_pointrange(stat = \"summary\", \n                  fun = mean, \n                  fun.min = function(x) { mean(x)-sd(x) },\n                  fun.max = function(x) { mean(x)+sd(x) }, data = days %>% filter(id %in% sel_ids) ) +\n  geom_point(alpha = 0.3, data = days %>% filter(id %in% sel_ids) )\n\n\n\nFigure 8: Selected individuals with their means, standard deviations, and the regression line\n\n\n\n\n\nShow code\n\nlibrary(cowplot)\np1 <- days %>% \n  group_by(id) %>% \n  summarise(x = mean(x, na.rm = T),\n            mean_y = mean(y, na.rm = T)) %>% \n  ggplot(., aes(x, mean_y)) +\n  geom_smooth(method = 'lm', color = \"gray\", se = F) + \n  geom_point(alpha = 0.4)\n\np2 <- days %>% \n  group_by(id) %>% \n  summarise(x = mean(x, na.rm = T),\n            sd_y = sd(y, na.rm = T)) %>% \n  ggplot(., aes(x, sd_y)) +\n  geom_smooth(method = 'lm', color = \"gray\", se = F) + \n  geom_point(alpha = 0.4) + \n  scale_y_continuous(trans = \"log\", breaks = c(0.1, 0.25, 0.5, 1, 2, 4))\n\np3 <- people %>% \n  ggplot(., aes(x, mean_y)) +\n  geom_smooth(method = 'lm', color = \"gray\", se = F) + \n  geom_point(alpha = 0.4) + \n  ylab(\"latent mean y\")\nplot_grid(p1,p2,p3,ncol = 2)\n\n\n\nFigure 9: The mean and the intraindividual SD as a function of X\n\n\n\nBias?\n\n\nShow code\n\nm_mixed <- brm(bf(y | cens(ycens) ~ x + (1|id),\n             sigma ~ x), data = days, silent = 2, refresh = 0)\nm_between <- brm(bf(y | cens(ycens) ~ x,\n             sigma ~ x), data = days %>% group_by(x, id) %>% \n               summarise(y = mean(y)) %>% \n               mutate(ycens = case_when(\n                y >= y_ceiling ~ \"right\",\n                TRUE ~ \"none\"\n              )), silent = 2, refresh = 0)\nm_mixed_2part <- brm(bf(y | cens(ycens) ~ x + (1|gr(id, by = xQ)),\n             sigma ~ x), data = days, silent = 2, refresh = 0)\ndraws <- m_mixed_2part %>% gather_draws(`sd_.*x.*`, regex = T)\nbetweenhdis <- draws %>% mean_hdci(.width = .95) %>% \n  mutate(xQ = as.numeric(str_match(.variable, \"xQ(\\\\d)\")[,2])) %>% \n  left_join(people %>% group_by(xQ) %>% summarise(x = mean(x)))\nm_2part_level2 <- brm(log(.value) | se(se, sigma = FALSE) ~ x, data = betweenhdis %>% mutate(se = (log(.value)-log(.lower))/2))\n\n\n\n\nShow code\n\ndraws <- bind_rows(\n  m_mixed = m_mixed %>% gather_draws(`b_(sigma_)?x`, regex = T),\n  m_mixed_2part = m_mixed_2part %>% gather_draws(`b_(sigma_)?x`, regex = T),\n  m_2part_between = m_2part_level2 %>% gather_draws(`b_(sigma_)?x`, regex = T) %>% mutate(.variable = \"b_sigma_x\"),\n  m_between = m_between %>% gather_draws(`b_(sigma_)?x`, regex = T),\n .id = \"model\") %>% \n  mutate(model = fct_inorder(factor(model)))\ndraws <- draws %>% group_by(model, .variable) %>% \n  mean_hdci(.width = c(.95, .99)) %>% \n  ungroup()\n\nggplot(draws, aes(y = .variable, x = .value, xmin = .lower, xmax = .upper)) +\n  geom_pointinterval(position = position_dodge(width = .4)) +\n  ggrepel::geom_text_repel(aes(label = if_else(.width == .95, sprintf(\"%.2f\", .value), NA_character_)), nudge_y = .1) +\n  geom_vline(aes(xintercept = true_value), linetype = 'dashed', \n             data = tibble(model = c(\"m_between\", \"m_mixed\", \"m_mixed_2part\", \"m_between\", \"m_mixed\", \"m_mixed_2part\", \"m_2part_between\"), .variable = c(\"b_x\", \"b_x\", \"b_x\", \"b_sigma_x\", \"b_sigma_x\", \"b_sigma_x\", \"b_sigma_x\"), true_value = c(b_mean, b_mean, b_mean, b_sd_bs, b_sd_ws, b_sd_ws, b_sd_bs))) +  scale_color_discrete(breaks = rev(levels(draws$model))) +\n  facet_grid(model ~ .variable) +\n  theme_bw() +\n  theme(legend.position = c(0.99,0.99),\n        legend.justification = c(1,1))\n\n\n\nFigure 10: Estimated coefficients and the true values (dashed line)\n\n\n\n\n\nShow code\n\nggplot(betweenhdis, aes(x, .value, \n                 ymin = .lower, ymax = .upper)) +\n  geom_pointrange() + \n  geom_line() + \n  ylab(\"sd(id)\") +\n  scale_y_continuous(trans = \"log\", breaks = c(0.1, 0.25, 0.5, 1, 2, 4)) +\n  theme_bw()\n\n\n\nFigure 11: Relationship between x and sd(id) in the two-part model\n\n\n\n\n\nLocation-scale between-subject\nHere, mu(y) is a function of x, as is sigma(y) at the between-subject level.\n\n\nShow code\n\nfit_models(\n  b_mean = 1,  b_sd_bs = 0.8, b_sd_ws = 0, y_ceiling = Inf)\n\n\n\nShow code\n\nN <- 250\nn_days = 51\nset.seed(20191005)\npeople <- tibble(\n  id = 1:N,\n  x = rnorm(N))\nn_days_per_person = rpois(N, n_days)\n\n\n\n\nShow code\n\npeople <- people %>% \n  mutate(\n    mean_log_sd_y = -1 + b_sd_bs * x,\n    log_sd_y = 0 + b_sd_ws * x,\n    mean_y = rnorm(N, sd = exp(mean_log_sd_y)) + b_mean * x,\n    xQ = ntile(x, 6)\n    )\ndays <- people %>% \n  full_join(tibble(\n              id = rep(1:N, times = n_days_per_person)\n            ), by = \"id\", multiple = \"all\") %>% \n            mutate(\n              latent_y = rnorm(n(), \n                        mean = mean_y,\n                        sd = exp(log_sd_y)),\n              y = case_when(\n                latent_y >= y_ceiling ~ y_ceiling,\n                # latent_y <= -1.5 ~ -1.5,\n                TRUE ~ latent_y\n              ),\n              ycens = case_when(\n                latent_y >= y_ceiling ~ \"right\",\n                # latent_y <= -1.5 ~ \"left\",\n                TRUE ~ \"none\"\n              )\n            )\n\n\nPercentage censored: 0.00\n\n\nShow code\n\nggplot(days, aes(x, y)) +\n  geom_point(alpha = 0.3)\n\n\n\nFigure 12: Raw data\n\n\n\n\n\nShow code\n\nsel_ids <- c(43, 36, 8, 40, 88, 29, 11, 49, 84, 41, 98)\n\nggplot(days, aes(x, y)) +\n  geom_smooth(method = 'lm', color = \"gray\", se = F) + \n  geom_pointrange(stat = \"summary\", \n                  fun = mean, \n                  fun.min = function(x) { mean(x)-sd(x) },\n                  fun.max = function(x) { mean(x)+sd(x) }, data = days %>% filter(id %in% sel_ids) ) +\n  geom_point(alpha = 0.3, data = days %>% filter(id %in% sel_ids) )\n\n\n\nFigure 13: Selected individuals with their means, standard deviations, and the regression line\n\n\n\n\n\nShow code\n\nlibrary(cowplot)\np1 <- days %>% \n  group_by(id) %>% \n  summarise(x = mean(x, na.rm = T),\n            mean_y = mean(y, na.rm = T)) %>% \n  ggplot(., aes(x, mean_y)) +\n  geom_smooth(method = 'lm', color = \"gray\", se = F) + \n  geom_point(alpha = 0.4)\n\np2 <- days %>% \n  group_by(id) %>% \n  summarise(x = mean(x, na.rm = T),\n            sd_y = sd(y, na.rm = T)) %>% \n  ggplot(., aes(x, sd_y)) +\n  geom_smooth(method = 'lm', color = \"gray\", se = F) + \n  geom_point(alpha = 0.4) + \n  scale_y_continuous(trans = \"log\", breaks = c(0.1, 0.25, 0.5, 1, 2, 4))\n\np3 <- people %>% \n  ggplot(., aes(x, mean_y)) +\n  geom_smooth(method = 'lm', color = \"gray\", se = F) + \n  geom_point(alpha = 0.4) + \n  ylab(\"latent mean y\")\nplot_grid(p1,p2,p3,ncol = 2)\n\n\n\nFigure 14: The mean and the intraindividual SD as a function of X\n\n\n\nBias?\n\n\nShow code\n\nm_mixed <- brm(bf(y | cens(ycens) ~ x + (1|id),\n             sigma ~ x), data = days, silent = 2, refresh = 0)\nm_between <- brm(bf(y | cens(ycens) ~ x,\n             sigma ~ x), data = days %>% group_by(x, id) %>% \n               summarise(y = mean(y)) %>% \n               mutate(ycens = case_when(\n                y >= y_ceiling ~ \"right\",\n                TRUE ~ \"none\"\n              )), silent = 2, refresh = 0)\nm_mixed_2part <- brm(bf(y | cens(ycens) ~ x + (1|gr(id, by = xQ)),\n             sigma ~ x), data = days, silent = 2, refresh = 0)\ndraws <- m_mixed_2part %>% gather_draws(`sd_.*x.*`, regex = T)\nbetweenhdis <- draws %>% mean_hdci(.width = .95) %>% \n  mutate(xQ = as.numeric(str_match(.variable, \"xQ(\\\\d)\")[,2])) %>% \n  left_join(people %>% group_by(xQ) %>% summarise(x = mean(x)))\nm_2part_level2 <- brm(log(.value) | se(se, sigma = FALSE) ~ x, data = betweenhdis %>% mutate(se = (log(.value)-log(.lower))/2))\n\n\n\n\nShow code\n\ndraws <- bind_rows(\n  m_mixed = m_mixed %>% gather_draws(`b_(sigma_)?x`, regex = T),\n  m_mixed_2part = m_mixed_2part %>% gather_draws(`b_(sigma_)?x`, regex = T),\n  m_2part_between = m_2part_level2 %>% gather_draws(`b_(sigma_)?x`, regex = T) %>% mutate(.variable = \"b_sigma_x\"),\n  m_between = m_between %>% gather_draws(`b_(sigma_)?x`, regex = T),\n .id = \"model\") %>% \n  mutate(model = fct_inorder(factor(model)))\ndraws <- draws %>% group_by(model, .variable) %>% \n  mean_hdci(.width = c(.95, .99)) %>% \n  ungroup()\n\nggplot(draws, aes(y = .variable, x = .value, xmin = .lower, xmax = .upper)) +\n  geom_pointinterval(position = position_dodge(width = .4)) +\n  ggrepel::geom_text_repel(aes(label = if_else(.width == .95, sprintf(\"%.2f\", .value), NA_character_)), nudge_y = .1) +\n  geom_vline(aes(xintercept = true_value), linetype = 'dashed', \n             data = tibble(model = c(\"m_between\", \"m_mixed\", \"m_mixed_2part\", \"m_between\", \"m_mixed\", \"m_mixed_2part\", \"m_2part_between\"), .variable = c(\"b_x\", \"b_x\", \"b_x\", \"b_sigma_x\", \"b_sigma_x\", \"b_sigma_x\", \"b_sigma_x\"), true_value = c(b_mean, b_mean, b_mean, b_sd_bs, b_sd_ws, b_sd_ws, b_sd_bs))) +  scale_color_discrete(breaks = rev(levels(draws$model))) +\n  facet_grid(model ~ .variable) +\n  theme_bw() +\n  theme(legend.position = c(0.99,0.99),\n        legend.justification = c(1,1))\n\n\n\nFigure 15: Estimated coefficients and the true values (dashed line)\n\n\n\n\n\nShow code\n\nggplot(betweenhdis, aes(x, .value, \n                 ymin = .lower, ymax = .upper)) +\n  geom_pointrange() + \n  geom_line() + \n  ylab(\"sd(id)\") +\n  scale_y_continuous(trans = \"log\", breaks = c(0.1, 0.25, 0.5, 1, 2, 4)) +\n  theme_bw()\n\n\n\nFigure 16: Relationship between x and sd(id) in the two-part model\n\n\n\n\n\n\n\nShow code\n\n# remotes::install_github(\"stephensrmmartin/LMMELSM\")\n# library(LMMELSM)\n# m_melsm_2part <- lmmelsm(list(observed ~ y,\n#                     location ~ x,\n#                     scale ~ x,\n#                     between ~ x),\n#                id, days)\n# summary(m_melsm_2part)\n\n\nCensored location-scale within-subject\nHere, mu(y) is a function of x, as is sigma(y) (only at the within-subject level).\n\n\nShow code\n\nfit_models(b_mean = 1,  b_sd_bs = 0, b_sd_ws = 0.8, y_ceiling = 2)\n\n\n\nShow code\n\nN <- 250\nn_days = 51\nset.seed(20191005)\npeople <- tibble(\n  id = 1:N,\n  x = rnorm(N))\nn_days_per_person = rpois(N, n_days)\n\n\n\n\nShow code\n\npeople <- people %>% \n  mutate(\n    mean_log_sd_y = -1 + b_sd_bs * x,\n    log_sd_y = 0 + b_sd_ws * x,\n    mean_y = rnorm(N, sd = exp(mean_log_sd_y)) + b_mean * x,\n    xQ = ntile(x, 6)\n    )\ndays <- people %>% \n  full_join(tibble(\n              id = rep(1:N, times = n_days_per_person)\n            ), by = \"id\", multiple = \"all\") %>% \n            mutate(\n              latent_y = rnorm(n(), \n                        mean = mean_y,\n                        sd = exp(log_sd_y)),\n              y = case_when(\n                latent_y >= y_ceiling ~ y_ceiling,\n                # latent_y <= -1.5 ~ -1.5,\n                TRUE ~ latent_y\n              ),\n              ycens = case_when(\n                latent_y >= y_ceiling ~ \"right\",\n                # latent_y <= -1.5 ~ \"left\",\n                TRUE ~ \"none\"\n              )\n            )\n\n\nPercentage censored: 0.12\n\n\nShow code\n\nggplot(days, aes(x, y)) +\n  geom_point(alpha = 0.3)\n\n\n\nFigure 7: Raw data\n\n\n\n\n\nShow code\n\nsel_ids <- c(43, 36, 8, 40, 88, 29, 11, 49, 84, 41, 98)\n\nggplot(days, aes(x, y)) +\n  geom_smooth(method = 'lm', color = \"gray\", se = F) + \n  geom_pointrange(stat = \"summary\", \n                  fun = mean, \n                  fun.min = function(x) { mean(x)-sd(x) },\n                  fun.max = function(x) { mean(x)+sd(x) }, data = days %>% filter(id %in% sel_ids) ) +\n  geom_point(alpha = 0.3, data = days %>% filter(id %in% sel_ids) )\n\n\n\nFigure 8: Selected individuals with their means, standard deviations, and the regression line\n\n\n\n\n\nShow code\n\nlibrary(cowplot)\np1 <- days %>% \n  group_by(id) %>% \n  summarise(x = mean(x, na.rm = T),\n            mean_y = mean(y, na.rm = T)) %>% \n  ggplot(., aes(x, mean_y)) +\n  geom_smooth(method = 'lm', color = \"gray\", se = F) + \n  geom_point(alpha = 0.4)\n\np2 <- days %>% \n  group_by(id) %>% \n  summarise(x = mean(x, na.rm = T),\n            sd_y = sd(y, na.rm = T)) %>% \n  ggplot(., aes(x, sd_y)) +\n  geom_smooth(method = 'lm', color = \"gray\", se = F) + \n  geom_point(alpha = 0.4) + \n  scale_y_continuous(trans = \"log\", breaks = c(0.1, 0.25, 0.5, 1, 2, 4))\n\np3 <- people %>% \n  ggplot(., aes(x, mean_y)) +\n  geom_smooth(method = 'lm', color = \"gray\", se = F) + \n  geom_point(alpha = 0.4) + \n  ylab(\"latent mean y\")\nplot_grid(p1,p2,p3,ncol = 2)\n\n\n\nFigure 9: The mean and the intraindividual SD as a function of X\n\n\n\nBias?\n\n\nShow code\n\nm_mixed <- brm(bf(y | cens(ycens) ~ x + (1|id),\n             sigma ~ x), data = days, silent = 2, refresh = 0)\nm_between <- brm(bf(y | cens(ycens) ~ x,\n             sigma ~ x), data = days %>% group_by(x, id) %>% \n               summarise(y = mean(y)) %>% \n               mutate(ycens = case_when(\n                y >= y_ceiling ~ \"right\",\n                TRUE ~ \"none\"\n              )), silent = 2, refresh = 0)\nm_mixed_2part <- brm(bf(y | cens(ycens) ~ x + (1|gr(id, by = xQ)),\n             sigma ~ x), data = days, silent = 2, refresh = 0)\ndraws <- m_mixed_2part %>% gather_draws(`sd_.*x.*`, regex = T)\nbetweenhdis <- draws %>% mean_hdci(.width = .95) %>% \n  mutate(xQ = as.numeric(str_match(.variable, \"xQ(\\\\d)\")[,2])) %>% \n  left_join(people %>% group_by(xQ) %>% summarise(x = mean(x)))\nm_2part_level2 <- brm(log(.value) | se(se, sigma = FALSE) ~ x, data = betweenhdis %>% mutate(se = (log(.value)-log(.lower))/2))\n\n\n\n\nShow code\n\ndraws <- bind_rows(\n  m_mixed = m_mixed %>% gather_draws(`b_(sigma_)?x`, regex = T),\n  m_mixed_2part = m_mixed_2part %>% gather_draws(`b_(sigma_)?x`, regex = T),\n  m_2part_between = m_2part_level2 %>% gather_draws(`b_(sigma_)?x`, regex = T) %>% mutate(.variable = \"b_sigma_x\"),\n  m_between = m_between %>% gather_draws(`b_(sigma_)?x`, regex = T),\n .id = \"model\") %>% \n  mutate(model = fct_inorder(factor(model)))\ndraws <- draws %>% group_by(model, .variable) %>% \n  mean_hdci(.width = c(.95, .99)) %>% \n  ungroup()\n\nggplot(draws, aes(y = .variable, x = .value, xmin = .lower, xmax = .upper)) +\n  geom_pointinterval(position = position_dodge(width = .4)) +\n  ggrepel::geom_text_repel(aes(label = if_else(.width == .95, sprintf(\"%.2f\", .value), NA_character_)), nudge_y = .1) +\n  geom_vline(aes(xintercept = true_value), linetype = 'dashed', \n             data = tibble(model = c(\"m_between\", \"m_mixed\", \"m_mixed_2part\", \"m_between\", \"m_mixed\", \"m_mixed_2part\", \"m_2part_between\"), .variable = c(\"b_x\", \"b_x\", \"b_x\", \"b_sigma_x\", \"b_sigma_x\", \"b_sigma_x\", \"b_sigma_x\"), true_value = c(b_mean, b_mean, b_mean, b_sd_bs, b_sd_ws, b_sd_ws, b_sd_bs))) +  scale_color_discrete(breaks = rev(levels(draws$model))) +\n  facet_grid(model ~ .variable) +\n  theme_bw() +\n  theme(legend.position = c(0.99,0.99),\n        legend.justification = c(1,1))\n\n\n\nFigure 10: Estimated coefficients and the true values (dashed line)\n\n\n\n\n\nShow code\n\nggplot(betweenhdis, aes(x, .value, \n                 ymin = .lower, ymax = .upper)) +\n  geom_pointrange() + \n  geom_line() + \n  ylab(\"sd(id)\") +\n  scale_y_continuous(trans = \"log\", breaks = c(0.1, 0.25, 0.5, 1, 2, 4)) +\n  theme_bw()\n\n\n\nFigure 11: Relationship between x and sd(id) in the two-part model\n\n\n\n\n\nCensored location-scale between-subject\nHere, mu(y) is a function of x, as is sigma(y) only at the between-subject level.\n\n\nShow code\n\nfit_models(b_mean = 1,  b_sd_bs = 0.8, b_sd_ws = 0, y_ceiling = 2)\n\n\n\nShow code\n\nN <- 250\nn_days = 51\nset.seed(20191005)\npeople <- tibble(\n  id = 1:N,\n  x = rnorm(N))\nn_days_per_person = rpois(N, n_days)\n\n\n\n\nShow code\n\npeople <- people %>% \n  mutate(\n    mean_log_sd_y = -1 + b_sd_bs * x,\n    log_sd_y = 0 + b_sd_ws * x,\n    mean_y = rnorm(N, sd = exp(mean_log_sd_y)) + b_mean * x,\n    xQ = ntile(x, 6)\n    )\ndays <- people %>% \n  full_join(tibble(\n              id = rep(1:N, times = n_days_per_person)\n            ), by = \"id\", multiple = \"all\") %>% \n            mutate(\n              latent_y = rnorm(n(), \n                        mean = mean_y,\n                        sd = exp(log_sd_y)),\n              y = case_when(\n                latent_y >= y_ceiling ~ y_ceiling,\n                # latent_y <= -1.5 ~ -1.5,\n                TRUE ~ latent_y\n              ),\n              ycens = case_when(\n                latent_y >= y_ceiling ~ \"right\",\n                # latent_y <= -1.5 ~ \"left\",\n                TRUE ~ \"none\"\n              )\n            )\n\n\nPercentage censored: 0.10\n\n\nShow code\n\nggplot(days, aes(x, y)) +\n  geom_point(alpha = 0.3)\n\n\n\nFigure 12: Raw data\n\n\n\n\n\nShow code\n\nsel_ids <- c(43, 36, 8, 40, 88, 29, 11, 49, 84, 41, 98)\n\nggplot(days, aes(x, y)) +\n  geom_smooth(method = 'lm', color = \"gray\", se = F) + \n  geom_pointrange(stat = \"summary\", \n                  fun = mean, \n                  fun.min = function(x) { mean(x)-sd(x) },\n                  fun.max = function(x) { mean(x)+sd(x) }, data = days %>% filter(id %in% sel_ids) ) +\n  geom_point(alpha = 0.3, data = days %>% filter(id %in% sel_ids) )\n\n\n\nFigure 13: Selected individuals with their means, standard deviations, and the regression line\n\n\n\n\n\nShow code\n\nlibrary(cowplot)\np1 <- days %>% \n  group_by(id) %>% \n  summarise(x = mean(x, na.rm = T),\n            mean_y = mean(y, na.rm = T)) %>% \n  ggplot(., aes(x, mean_y)) +\n  geom_smooth(method = 'lm', color = \"gray\", se = F) + \n  geom_point(alpha = 0.4)\n\np2 <- days %>% \n  group_by(id) %>% \n  summarise(x = mean(x, na.rm = T),\n            sd_y = sd(y, na.rm = T)) %>% \n  ggplot(., aes(x, sd_y)) +\n  geom_smooth(method = 'lm', color = \"gray\", se = F) + \n  geom_point(alpha = 0.4) + \n  scale_y_continuous(trans = \"log\", breaks = c(0.1, 0.25, 0.5, 1, 2, 4))\n\np3 <- people %>% \n  ggplot(., aes(x, mean_y)) +\n  geom_smooth(method = 'lm', color = \"gray\", se = F) + \n  geom_point(alpha = 0.4) + \n  ylab(\"latent mean y\")\nplot_grid(p1,p2,p3,ncol = 2)\n\n\n\nFigure 14: The mean and the intraindividual SD as a function of X\n\n\n\nBias?\n\n\nShow code\n\nm_mixed <- brm(bf(y | cens(ycens) ~ x + (1|id),\n             sigma ~ x), data = days, silent = 2, refresh = 0)\nm_between <- brm(bf(y | cens(ycens) ~ x,\n             sigma ~ x), data = days %>% group_by(x, id) %>% \n               summarise(y = mean(y)) %>% \n               mutate(ycens = case_when(\n                y >= y_ceiling ~ \"right\",\n                TRUE ~ \"none\"\n              )), silent = 2, refresh = 0)\nm_mixed_2part <- brm(bf(y | cens(ycens) ~ x + (1|gr(id, by = xQ)),\n             sigma ~ x), data = days, silent = 2, refresh = 0)\ndraws <- m_mixed_2part %>% gather_draws(`sd_.*x.*`, regex = T)\nbetweenhdis <- draws %>% mean_hdci(.width = .95) %>% \n  mutate(xQ = as.numeric(str_match(.variable, \"xQ(\\\\d)\")[,2])) %>% \n  left_join(people %>% group_by(xQ) %>% summarise(x = mean(x)))\nm_2part_level2 <- brm(log(.value) | se(se, sigma = FALSE) ~ x, data = betweenhdis %>% mutate(se = (log(.value)-log(.lower))/2))\n\n\n\n\nShow code\n\ndraws <- bind_rows(\n  m_mixed = m_mixed %>% gather_draws(`b_(sigma_)?x`, regex = T),\n  m_mixed_2part = m_mixed_2part %>% gather_draws(`b_(sigma_)?x`, regex = T),\n  m_2part_between = m_2part_level2 %>% gather_draws(`b_(sigma_)?x`, regex = T) %>% mutate(.variable = \"b_sigma_x\"),\n  m_between = m_between %>% gather_draws(`b_(sigma_)?x`, regex = T),\n .id = \"model\") %>% \n  mutate(model = fct_inorder(factor(model)))\ndraws <- draws %>% group_by(model, .variable) %>% \n  mean_hdci(.width = c(.95, .99)) %>% \n  ungroup()\n\nggplot(draws, aes(y = .variable, x = .value, xmin = .lower, xmax = .upper)) +\n  geom_pointinterval(position = position_dodge(width = .4)) +\n  ggrepel::geom_text_repel(aes(label = if_else(.width == .95, sprintf(\"%.2f\", .value), NA_character_)), nudge_y = .1) +\n  geom_vline(aes(xintercept = true_value), linetype = 'dashed', \n             data = tibble(model = c(\"m_between\", \"m_mixed\", \"m_mixed_2part\", \"m_between\", \"m_mixed\", \"m_mixed_2part\", \"m_2part_between\"), .variable = c(\"b_x\", \"b_x\", \"b_x\", \"b_sigma_x\", \"b_sigma_x\", \"b_sigma_x\", \"b_sigma_x\"), true_value = c(b_mean, b_mean, b_mean, b_sd_bs, b_sd_ws, b_sd_ws, b_sd_bs))) +  scale_color_discrete(breaks = rev(levels(draws$model))) +\n  facet_grid(model ~ .variable) +\n  theme_bw() +\n  theme(legend.position = c(0.99,0.99),\n        legend.justification = c(1,1))\n\n\n\nFigure 15: Estimated coefficients and the true values (dashed line)\n\n\n\n\n\nShow code\n\nggplot(betweenhdis, aes(x, .value, \n                 ymin = .lower, ymax = .upper)) +\n  geom_pointrange() + \n  geom_line() + \n  ylab(\"sd(id)\") +\n  scale_y_continuous(trans = \"log\", breaks = c(0.1, 0.25, 0.5, 1, 2, 4)) +\n  theme_bw()\n\n\n\nFigure 16: Relationship between x and sd(id) in the two-part model\n\n\n\n\n\n\n\nso, a doubling of income is associated with same happiness increment throughout the income range↩︎\nBig if!↩︎\nEven bigger if!↩︎\ntwo assumptions that I’ll touch on here. They do not make their assumptions regarding causal inference explicit, but still very much imply causality. But, like, what else is new?↩︎\nI requested the raw data on March 4 and received an answer that ping-level data could not be shared owing to promises made to participants on March 15.↩︎\nMy dictionary says “disimprovement”. Is that actually in use?↩︎\nAm I projecting based on my own attempt at adversarial collaboration? Absolutely! Thanks for asking, I’m okay!↩︎\n",
    "preview": "posts/2023-03-05-multilevel-censored-location-scale-models/multilevel-censored-location-scale-models_files/figure-html5/BS_0_8_WS_0__means-22-1.png",
    "last_modified": "2023-03-24T15:19:35+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-02-21-reanalysis-sexual-attraction-to-visual-sexual-stimuli-in-association-with-steroid-hormones-across-menstrual-cycles-and-fertility-treatment/",
    "title": "Reanalysis: Sexual attraction to visual sexual stimuli and hormones",
    "description": "Analyzing the public data shared along with a recent publication by Schön et al. (2023): Sexual attraction to visual sexual stimuli in association with steroid hormones across menstrual cycles and fertility treatment",
    "author": [
      {
        "name": "Ruben C. Arslan",
        "url": "https://rubenarslan.github.io"
      }
    ],
    "date": "2023-02-22",
    "categories": [
      "hormones",
      "hibar"
    ],
    "contents": "\n\nContents\nAccuracy of our estradiol and progesterone imputations\nSlightly different analyses\nMultilevel generalizability\nA multivariate model\nA location-scale model\nGroup mean centering\nLatent group mean centering\nImputations and lag\nLatent lag\nBringing it all together\n\n“Conclusion”\nThings I didn’t do or that still confuse me\n\n\n\n\n\nShow code\n\n# importing the data downloaded from the supplementary here https://www.sciencedirect.com/science/article/pii/S0306453023000380#sec0115\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\ncycles <- rio::import(\"ScienceDirect_files_21Feb2023_09-06-38.857/mmc3/SPSS_Dataset_Cycle_1_2.sav\")\n# cycles %>% names()\n\n# cycles %>% select(starts_with(\"Z\"))\ncycles_long <- cycles %>% pivot_longer(starts_with(\"Z\")) %>% \n  separate(name, c(\"cycle\", \"time\", \"name\"), extra = \"merge\") %>% \n  pivot_wider()\n\n# unique(cycles_long$cycle)\n# unique(cycles_long$time)\ncycles_long <- cycles_long %>% \n  mutate(fc_day = as.numeric(recode(time, \"T1\" = \"4\", \n                         \"T2\" = \"13\",\n                         \"T3\" = \"21\",\n                         \"T4\" = \"28\")) - 1)\ncycles_long$fc_day %>% table(exclude=NULL)\n\n.\n  3  12  20  27 \n180 180 180 180 \n\nShow code\n\ncycles_long <- cycles_long %>% \n  mutate(logOESTR = log(OESTR), logPROG = log(PROG))\n\nlead2 <- cycles_long %>% select(ID, cycle, fc_day, logOESTR_lag2 = logOESTR, logPROG_lag2 = logPROG) %>% \n  mutate(fc_day = fc_day + 2)\n\ncycles_long <- cycles_long %>% \n  mutate_at(vars(starts_with(\"SR_\")), ~ (. - 50)/20 )\n\ncycles_longer <- cycles_long %>% \n  group_by(ID, cycle) %>% \n  tidyr::expand(fc_day = c(3, 5, 12, 14, 20, 22, 27, 29)) %>% \n  full_join(cycles_long, by = c(\"ID\", \"cycle\", \"fc_day\")) %>% \n  left_join(lead2, by = c(\"ID\", \"cycle\", \"fc_day\")) %>% \n  mutate(fc_day_lag2 = fc_day - 2)\n\n# table(cycles_longer$fc_day)\n# table(cycles_longer$fc_day_lag2)\n\nfc_days <- rio::import(\"https://files.osf.io/v1/resources/u9xad/providers/github/merge_files/fc_days.sav\")\ncycles_longer <- cycles_longer %>% \n  left_join(fc_days, by = c(\"fc_day\" = \"fc_day\")) %>% \n  ungroup()\n\ncycles_longer <- cycles_longer %>% \n  left_join(fc_days %>% rename_with(~ str_c(., \"_lag2\")), by = c(\"fc_day_lag2\" = \"fc_day_lag2\")) %>% \n  ungroup()\n\n\n# ggplot(cycles_long, aes(fc_day, log(OESTR))) + geom_point() + geom_smooth()\n# ggplot(cycles_longer, aes(fc_day, logOESTR_lag2)) + geom_point() + geom_smooth()\n# ggplot(cycles_long, aes(fc_day, log(PROG))) + geom_point() + geom_smooth()\n# ggplot(cycles_longer, aes(log(OESTR), est_estradiol_fc)) + geom_point()\n\n# lm(log(OESTR) ~ est_estradiol_fc, cycles_longer)\n# lm(log(PROG) ~ est_progesterone_fc, cycles_longer)\n\n\nThe following paper was recently published by Schön et al. in Psychoneuroendocrinology: Sexual attraction to visual sexual stimuli in association with steroid hormones across menstrual cycles and fertility treatment, doi:10.1016/j.psyneuen.2023.106060\n\nAbstract\nBackground\nSteroid hormones (i.e., estradiol, progesterone, and testosterone) are considered to play a crucial role in the regulation of women’s sexual desire and sexual attraction to sexual stimuli throughout the menstrual cycle. However, the literature is inconsistent, and methodologically sound studies on the relationship between steroid hormones and women’s sexual attraction are rare.\nMethods:\nThis prospective longitudinal multisite study examined estradiol, progesterone, and testosterone serum levels in association with sexual attraction to visual sexual stimuli in naturally cycling women and in women undergoing fertility treatment (in vitro fertilization, IVF). Across ovarian stimulation of fertility treatment, estradiol reaches supraphysiological levels, while other ovarian hormones remain nearly stable. Ovarian stimulation hence offers a unique quasi-experimental model to study concentration-dependent effects of estradiol. Hormonal parameters and sexual attraction to visual sexual stimuli assessed with computerized visual analogue scales were collected at four time points per cycle, i.e., during the menstrual, preovulatory, mid-luteal, and premenstrual phases, across two consecutive menstrual cycles (n = 88 and n = 68 for the first and second cycle, respectively). Women undergoing fertility treatment (n = 44) were assessed twice, at the beginning and at the end of ovarian stimulation. Sexually explicit photographs served as visual sexual stimuli.\nResults\nIn naturally cycling women, sexual attraction to visual sexual stimuli did not vary consistently across two consecutive menstrual cycles. While in the first menstrual cycle sexual attraction to male bodies, couples kissing, and at intercourse varied significantly with a peak in the preovulatory phase, (all p ≤ 0.001), there was no significant variability across the second cycle. Univariable and multivariable models evaluating repeated cross-sectional relationships and intraindividual change scores revealed no consistent associations between estradiol, progesterone, and testosterone and sexual attraction to visual sexual stimuli throughout both menstrual cycles. Also, no significant association with any hormone was found when the data from both menstrual cycles were combined. In women undergoing ovarian stimulation of IVF, sexual attraction to visual sexual stimuli did not vary over time and was not associated with estradiol levels despite intraindividual changes in estradiol levels from 122.0 to 11,746.0 pmol/l with a mean (SD) of 3,553.9 (2,472.4) pmol/l.\nConclusions\nThese results imply that neither physiological levels of estradiol, progesterone, and testosterone in naturally cycling women nor supraphysiological levels of estradiol due to ovarian stimulation exert any relevant effect on women’s sexual attraction to visual sexual stimuli.\nThe paper caught my attention for two reasons:\nit’s well-done, interesting work, including serum hormones and both a naturally cycling sample as well as a sample of women undergoing ovarian hyperstimulation in preparation for in vitro fertilisation\nthey openly shared their data, which I love to see1.\nSo, naturally, I delved right in.\nAccuracy of our estradiol and progesterone imputations\nAlmost the first thing I wanted to do was to check the accuracy of our imputations for estradiol and progesterone. In our recent paper, we had computed the accuracy of imputing log estradiol and progesterone from menstrual cycle phase. However, because we only had raw data for one serum dataset, we used a statistical approach (approximative LOO) to reduce overfitting. One reviewer was skeptical that we would find such good performance in independent data.\n\n\nShow code\n\no_ests <- broom::tidy(cor.test(cycles_longer$est_estradiol_fc, log(cycles_longer$OESTR)))\np_ests <- broom::tidy(cor.test(cycles_longer$est_progesterone_fc, log(cycles_longer$PROG)))\n\n\nSo, I merged my imputed estradiol and progesterone values on their “time” variable, which, I thought, can be understood as a cycle day counted forward from the last menstrual onset.\ntheir sampling scheduleIn the BioCycle study data, I had found the accuracy to be 0.57 [0.55;0.59]. Here, it was 0.68 [0.64;0.72]. For progesterone, we had reported 0.72 [0.70;0.74] and here I got 0.79 [0.76;0.82].\nThe values here are actually better! They are more in line with our accuracy estimates for backward-counting (.68 & .83). This might be because they do not have strictly days since last menstrual onset here, but rather I back-translated that from their graph of time points. In actual fact, they used some smart scheduling techniques based on LH and sonography. Another difference might be the variance in cycle phase, which they maximized with two measurement occasions close to menstruation, one around ovulation, and one mid-luteal occasion. I could adjust for that, but for now, I mainly take the message that our imputations seem to work pretty well on independent data.\nSlightly different analyses\nReading the paper, I couldn’t help wonder whether slightly different analysis choices would have led to different results. They used generalized estimating equations and it all seemed well-done if slightly different than what I normally do. But from my own experience with this kind of data (mostly unpublished), I’ve come to the conclusion that:\nlogging steroid hormone concentrations is slightly better than not doing so because\nnot logging you have to make arbitrary decisions how to deal with influential ‘outliers’ which are, however, still bioplausible\nthere is some evidence that associations are linear after logging\nexplained variance by cycle phase was slightly bigger in my recent paper\ninteractions between E and P, or their ratio E/P naturally turn into additive (or rather subtractive) terms after logging, reducing model complexity\n\nthat the relationship between steroid hormones and sexual desire is best predicted by log(estradiol/progesterone)\nthere is some evidence that the effect of serum log(estradiol/progesterone) is strongest at a lag of around two days on psychological outcomes, but much of that is based on salivary immunoassays, which I don’t put much stock in\nthat it might be better to leave log(testosterone) out of the equation at first, because it’s plausibly a mediator\nI figured I could probably aggregate across their four outcomes (ratings of stimuli of male faces, bodies, kissing, intercourse).\nI figured their might be substantial heterogeneity in residual variances, as that’s been my experience with visual analogue rating scales\nMultilevel generalizability\nTo determine whether I could aggregate across their four outcomes, I ran a multilevel generalizability analysis. I brought their visual analogue scales from 0 to 100 to approximate unit variance by subtracting 50 and dividing by 20.\n\n\nShow code\n\ncycles_long <- cycles_long %>% mutate(cycle_time = str_c(cycle, time))\ndf <- cycles_long %>% select(ID, cycle_time, starts_with(\"SR_\")) %>% drop_na() %>% as.data.frame\npsych::mlr(df, grp = \"ID\", Time = \"cycle_time\")\n\n\nMultilevel Generalizability analysis   \nCall: psych::mlr(x = df, grp = \"ID\", Time = \"cycle_time\")\n\nThe data had  88  observations taken over  8  time intervals for  3 items.\n\n Alternative estimates of reliability based upon Generalizability theory\n\nRkF  =  0.98 Reliability of average of all ratings across all items and  times (Fixed time effects)\nR1R  =  0.75 Generalizability of a single time point across all items (Random time effects)\nRkR  =  0.96 Generalizability of average time points across all items (Random time effects)\nRc   =  0.41 Generalizability of change (fixed time points, fixed items) \nRkRn =  0.92 Generalizability of between person differences averaged over time (time nested within people)\nRcn  =  0 Generalizability of within person variations averaged over items  (time nested within people)\n\n These reliabilities are derived from the components of variance estimated by ANOVA \n             variance Percent\nID               0.25    0.20\nTime             0.00    0.00\nItems            0.30    0.24\nID x time        0.05    0.04\nID x items       0.41    0.33\ntime x items     0.00    0.00\nResidual         0.22    0.18\nTotal            1.24    1.00\n\n The nested components of variance estimated from lme are:\n         variance Percent\nid        4.3e-01 3.2e-01\nid(time)  1.1e-09 8.4e-10\nresidual  8.9e-01 6.8e-01\ntotal     1.3e+00 1.0e+00\n\nTo see the ANOVA and alpha by subject, use the short = FALSE option.\n To see the summaries of the ICCs by subject and time, use all=TRUE\n To see specific objects select from the following list:\n ANOVA s.lmer s.lme alpha summary.by.person summary.by.time ICC.by.person ICC.by.time lmer long Call\n\n\n\nShow code\n\ncycles_long %>% select(ID, cycle_time, starts_with(\"SR_\")) %>% pivot_longer(-c(ID, cycle_time)) %>% \n  ggplot(aes(value)) + geom_histogram() +\n  facet_grid(cycle_time ~ name)\n\n\n\nFigure 1: Distributions of the outcome visual analogue scale ratings over time. (Z=cycle, T=time point)\n\n\n\nHmm, the generalizability of within person variations averaged over items is zero, so maybe aggregating is not a good idea. However, a multivariate model would allow me to do some partial pooling across outcomes, so went with that.\nA multivariate model\nSo, in the below model, I:\nlogged estradiol and progesterone, this way I did not have to include the estradiol-progesterone ratio in the model\nomitted testosterone, at least as a first step\nallowed slopes to vary by person and cycle\nallowed correlations across outcomes, both for the residuals and the varying slopes and intercepts\n\n\nShow code\n\nlibrary(brms)\nlibrary(cmdstanr)\nlibrary(tidybayes)\nknitr::opts_chunk$set(tidy = FALSE)\noptions(brms.backend = \"cmdstanr\",  # I use the cmdstanr backend\n        mc.cores = 8, \n        brms.threads = 2,           # which allows me to multithread\n        brms.file_refit = \"on_change\", # this is useful when doing iterative model building, though it can misfire, be careful\n        width = 8000) \n\nm1mv0 <- brm(mvbind(SR_Faces, SR_Bodies, SR_Kissing, SR_Intercourse) ~  cycle + (1 |i|ID) + (1 |c|ID:cycle), cycles_long %>% drop_na(logOESTR, logPROG),\n            iter = 6000, file = \"m1mv0\",\n            control = list(adapt_delta = 0.99))\n\nm1mv <- brm(mvbind(SR_Faces, SR_Bodies, SR_Kissing, SR_Intercourse) ~ log(OESTR) + log(PROG) + cycle + (1 + log(OESTR) + log(PROG)|i|ID) + (1 + log(OESTR) + log(PROG)|c|ID:cycle), cycles_long, \n            iter = 6000, file = \"m1mv\",\n            control = list(adapt_delta = 0.99))\n\n\n\nModel output and comparison to null model\n\n\nShow code\n\noptions(width = 8000)\nm1mv\n\n Family: MV(gaussian, gaussian, gaussian, gaussian) \n  Links: mu = identity; sigma = identity\n         mu = identity; sigma = identity\n         mu = identity; sigma = identity\n         mu = identity; sigma = identity \nFormula: SR_Faces ~ log(OESTR) + log(PROG) + cycle + (1 + log(OESTR) + log(PROG) | i | ID) + (1 + log(OESTR) + log(PROG) | c | ID:cycle) \n         SR_Bodies ~ log(OESTR) + log(PROG) + cycle + (1 + log(OESTR) + log(PROG) | i | ID) + (1 + log(OESTR) + log(PROG) | c | ID:cycle) \n         SR_Kissing ~ log(OESTR) + log(PROG) + cycle + (1 + log(OESTR) + log(PROG) | i | ID) + (1 + log(OESTR) + log(PROG) | c | ID:cycle) \n         SR_Intercourse ~ log(OESTR) + log(PROG) + cycle + (1 + log(OESTR) + log(PROG) | i | ID) + (1 + log(OESTR) + log(PROG) | c | ID:cycle) \n   Data: cycles_long (Number of observations: 551) \n  Draws: 4 chains, each with iter = 6000; warmup = 3000; thin = 1;\n         total post-warmup draws = 12000\n\nGroup-Level Effects: \n~ID (Number of levels: 87) \n                                                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(SRFaces_Intercept)                                   0.91      0.18     0.52     1.24 1.00     1754      954\nsd(SRFaces_logOESTR)                                    0.09      0.05     0.01     0.18 1.01      384     1451\nsd(SRFaces_logPROG)                                     0.06      0.03     0.01     0.12 1.00     1262     1162\nsd(SRBodies_Intercept)                                  0.71      0.09     0.52     0.89 1.00     3926     3637\nsd(SRBodies_logOESTR)                                   0.03      0.02     0.00     0.09 1.01      802     1388\nsd(SRBodies_logPROG)                                    0.03      0.02     0.00     0.07 1.00     2272     4007\nsd(SRKissing_Intercept)                                 0.80      0.11     0.60     1.05 1.00     5869     6772\nsd(SRKissing_logOESTR)                                  0.05      0.03     0.00     0.12 1.00     1228     3401\nsd(SRKissing_logPROG)                                   0.03      0.02     0.00     0.07 1.00     2925     5597\nsd(SRIntercourse_Intercept)                             0.69      0.14     0.39     0.97 1.00     4201     4153\nsd(SRIntercourse_logOESTR)                              0.08      0.03     0.01     0.14 1.01     1482     2455\nsd(SRIntercourse_logPROG)                               0.02      0.01     0.00     0.05 1.00     7089     5680\ncor(SRFaces_Intercept,SRFaces_logOESTR)                -0.07      0.27    -0.57     0.47 1.00     2049     5232\ncor(SRFaces_Intercept,SRFaces_logPROG)                  0.06      0.24    -0.44     0.52 1.00     2412     6102\ncor(SRFaces_logOESTR,SRFaces_logPROG)                   0.10      0.27    -0.43     0.59 1.00     2919     5583\ncor(SRFaces_Intercept,SRBodies_Intercept)               0.36      0.16     0.00     0.64 1.00     1142     2225\ncor(SRFaces_logOESTR,SRBodies_Intercept)                0.15      0.24    -0.35     0.59 1.00      876     1793\ncor(SRFaces_logPROG,SRBodies_Intercept)                 0.17      0.23    -0.30     0.59 1.00     1093     2287\ncor(SRFaces_Intercept,SRBodies_logOESTR)                0.01      0.26    -0.50     0.51 1.00    12488     9042\ncor(SRFaces_logOESTR,SRBodies_logOESTR)                 0.02      0.27    -0.49     0.54 1.00     5779     8230\ncor(SRFaces_logPROG,SRBodies_logOESTR)                  0.00      0.27    -0.53     0.53 1.00     8097     8997\ncor(SRBodies_Intercept,SRBodies_logOESTR)              -0.01      0.27    -0.51     0.51 1.00     8516     9109\ncor(SRFaces_Intercept,SRBodies_logPROG)                 0.03      0.25    -0.46     0.51 1.00    11937     9061\ncor(SRFaces_logOESTR,SRBodies_logPROG)                 -0.02      0.27    -0.53     0.51 1.00     6642     8450\ncor(SRFaces_logPROG,SRBodies_logPROG)                   0.02      0.27    -0.50     0.54 1.00     7737     8150\ncor(SRBodies_Intercept,SRBodies_logPROG)                0.10      0.25    -0.40     0.56 1.00    12621     8147\ncor(SRBodies_logOESTR,SRBodies_logPROG)                 0.02      0.27    -0.51     0.54 1.00     6623     8980\ncor(SRFaces_Intercept,SRKissing_Intercept)              0.40      0.16     0.05     0.67 1.00     1375     1418\ncor(SRFaces_logOESTR,SRKissing_Intercept)               0.11      0.24    -0.39     0.56 1.01     1090     1814\ncor(SRFaces_logPROG,SRKissing_Intercept)               -0.17      0.23    -0.59     0.31 1.00     1347     1555\ncor(SRBodies_Intercept,SRKissing_Intercept)             0.19      0.16    -0.14     0.46 1.00     3093     5296\ncor(SRBodies_logOESTR,SRKissing_Intercept)              0.06      0.26    -0.45     0.54 1.00     1436     3259\ncor(SRBodies_logPROG,SRKissing_Intercept)               0.13      0.24    -0.38     0.58 1.00     2175     4124\ncor(SRFaces_Intercept,SRKissing_logOESTR)              -0.03      0.26    -0.53     0.48 1.00     6664     8724\ncor(SRFaces_logOESTR,SRKissing_logOESTR)               -0.08      0.27    -0.57     0.46 1.00     4768     7422\ncor(SRFaces_logPROG,SRKissing_logOESTR)                -0.08      0.27    -0.57     0.45 1.00     5973     8418\ncor(SRBodies_Intercept,SRKissing_logOESTR)              0.15      0.26    -0.38     0.61 1.00    10292     8256\ncor(SRBodies_logOESTR,SRKissing_logOESTR)               0.07      0.27    -0.47     0.58 1.00     7156     8895\ncor(SRBodies_logPROG,SRKissing_logOESTR)                0.06      0.27    -0.49     0.57 1.00     8805     9999\ncor(SRKissing_Intercept,SRKissing_logOESTR)            -0.18      0.28    -0.66     0.40 1.00     4508     8079\ncor(SRFaces_Intercept,SRKissing_logPROG)               -0.07      0.26    -0.54     0.45 1.00    13308     8995\ncor(SRFaces_logOESTR,SRKissing_logPROG)                -0.03      0.27    -0.54     0.51 1.00     8504     9278\ncor(SRFaces_logPROG,SRKissing_logPROG)                  0.04      0.27    -0.50     0.55 1.00     9818     8006\ncor(SRBodies_Intercept,SRKissing_logPROG)              -0.00      0.26    -0.49     0.50 1.00    14921     8442\ncor(SRBodies_logOESTR,SRKissing_logPROG)               -0.01      0.27    -0.52     0.51 1.00     7387     8837\ncor(SRBodies_logPROG,SRKissing_logPROG)                 0.02      0.27    -0.50     0.54 1.00     8177     9815\ncor(SRKissing_Intercept,SRKissing_logPROG)             -0.05      0.26    -0.53     0.47 1.00    13370     9196\ncor(SRKissing_logOESTR,SRKissing_logPROG)              -0.00      0.27    -0.52     0.53 1.00     7398     9032\ncor(SRFaces_Intercept,SRIntercourse_Intercept)          0.02      0.20    -0.36     0.40 1.00     2583     4005\ncor(SRFaces_logOESTR,SRIntercourse_Intercept)          -0.03      0.24    -0.51     0.44 1.00     1166     2638\ncor(SRFaces_logPROG,SRIntercourse_Intercept)           -0.05      0.24    -0.52     0.41 1.00     1573     3470\ncor(SRBodies_Intercept,SRIntercourse_Intercept)         0.21      0.17    -0.15     0.53 1.00     5264     7247\ncor(SRBodies_logOESTR,SRIntercourse_Intercept)          0.05      0.26    -0.45     0.54 1.00     1664     3815\ncor(SRBodies_logPROG,SRIntercourse_Intercept)           0.16      0.25    -0.36     0.61 1.00     2143     3525\ncor(SRKissing_Intercept,SRIntercourse_Intercept)        0.46      0.19     0.03     0.75 1.00     2193     5048\ncor(SRKissing_logOESTR,SRIntercourse_Intercept)         0.19      0.26    -0.35     0.66 1.00     2642     6558\ncor(SRKissing_logPROG,SRIntercourse_Intercept)          0.13      0.26    -0.40     0.60 1.00     3781     7742\ncor(SRFaces_Intercept,SRIntercourse_logOESTR)           0.12      0.23    -0.34     0.54 1.00     4581     6479\ncor(SRFaces_logOESTR,SRIntercourse_logOESTR)            0.07      0.26    -0.44     0.57 1.00     2286     5319\ncor(SRFaces_logPROG,SRIntercourse_logOESTR)             0.01      0.25    -0.48     0.50 1.00     3672     7120\ncor(SRBodies_Intercept,SRIntercourse_logOESTR)         -0.05      0.22    -0.48     0.39 1.00     9113     7600\ncor(SRBodies_logOESTR,SRIntercourse_logOESTR)          -0.04      0.27    -0.55     0.49 1.00     2956     6047\ncor(SRBodies_logPROG,SRIntercourse_logOESTR)            0.10      0.26    -0.44     0.59 1.00     3664     6303\ncor(SRKissing_Intercept,SRIntercourse_logOESTR)         0.29      0.23    -0.21     0.68 1.00     7440     5910\ncor(SRKissing_logOESTR,SRIntercourse_logOESTR)          0.02      0.27    -0.51     0.54 1.00     4481     7853\ncor(SRKissing_logPROG,SRIntercourse_logOESTR)           0.07      0.27    -0.46     0.58 1.00     5425     9041\ncor(SRIntercourse_Intercept,SRIntercourse_logOESTR)    -0.09      0.27    -0.58     0.46 1.00     4465     7950\ncor(SRFaces_Intercept,SRIntercourse_logPROG)           -0.04      0.28    -0.56     0.50 1.00    19037     8436\ncor(SRFaces_logOESTR,SRIntercourse_logPROG)            -0.01      0.27    -0.53     0.51 1.00    16186     8912\ncor(SRFaces_logPROG,SRIntercourse_logPROG)             -0.00      0.28    -0.53     0.53 1.00    14895     8501\ncor(SRBodies_Intercept,SRIntercourse_logPROG)          -0.03      0.28    -0.55     0.50 1.00    20543     8788\ncor(SRBodies_logOESTR,SRIntercourse_logPROG)            0.00      0.28    -0.54     0.54 1.00    10941     8990\ncor(SRBodies_logPROG,SRIntercourse_logPROG)            -0.00      0.28    -0.55     0.53 1.00    11439     9846\ncor(SRKissing_Intercept,SRIntercourse_logPROG)         -0.01      0.27    -0.54     0.50 1.00    16414     9556\ncor(SRKissing_logOESTR,SRIntercourse_logPROG)           0.01      0.28    -0.53     0.54 1.00     9671     9039\ncor(SRKissing_logPROG,SRIntercourse_logPROG)            0.04      0.28    -0.51     0.57 1.00     8979    10350\ncor(SRIntercourse_Intercept,SRIntercourse_logPROG)     -0.01      0.27    -0.53     0.53 1.00    13612    10073\ncor(SRIntercourse_logOESTR,SRIntercourse_logPROG)      -0.04      0.27    -0.55     0.50 1.00    10481    10230\n\n~ID:cycle (Number of levels: 155) \n                                                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(SRFaces_Intercept)                                   0.23      0.14     0.01     0.49 1.00     1060     3003\nsd(SRFaces_logOESTR)                                    0.05      0.02     0.01     0.09 1.01      755     1774\nsd(SRFaces_logPROG)                                     0.04      0.03     0.00     0.09 1.00     2833     5791\nsd(SRBodies_Intercept)                                  0.20      0.08     0.02     0.34 1.00     1123     1537\nsd(SRBodies_logOESTR)                                   0.02      0.01     0.00     0.05 1.00      676     2430\nsd(SRBodies_logPROG)                                    0.03      0.02     0.00     0.07 1.00     1976     5255\nsd(SRKissing_Intercept)                                 0.11      0.07     0.01     0.25 1.00     2373     4992\nsd(SRKissing_logOESTR)                                  0.02      0.01     0.00     0.05 1.00     1464     3148\nsd(SRKissing_logPROG)                                   0.02      0.02     0.00     0.06 1.00     4545     6123\nsd(SRIntercourse_Intercept)                             0.14      0.08     0.01     0.28 1.00     2539     4014\nsd(SRIntercourse_logOESTR)                              0.02      0.01     0.00     0.04 1.00     2537     5787\nsd(SRIntercourse_logPROG)                               0.02      0.02     0.00     0.06 1.00     4235     5040\ncor(SRFaces_Intercept,SRFaces_logOESTR)                -0.06      0.28    -0.60     0.49 1.00     3019     6296\ncor(SRFaces_Intercept,SRFaces_logPROG)                 -0.03      0.28    -0.55     0.50 1.00    11293     8423\ncor(SRFaces_logOESTR,SRFaces_logPROG)                  -0.01      0.27    -0.53     0.51 1.00    12835     8356\ncor(SRFaces_Intercept,SRBodies_Intercept)               0.09      0.27    -0.46     0.56 1.00     2221     4411\ncor(SRFaces_logOESTR,SRBodies_Intercept)                0.10      0.25    -0.42     0.55 1.00     2766     5243\ncor(SRFaces_logPROG,SRBodies_Intercept)                 0.10      0.27    -0.45     0.60 1.00     2287     4660\ncor(SRFaces_Intercept,SRBodies_logOESTR)                0.04      0.27    -0.50     0.55 1.00     5202     7342\ncor(SRFaces_logOESTR,SRBodies_logOESTR)                 0.06      0.27    -0.49     0.55 1.00     5287     6875\ncor(SRFaces_logPROG,SRBodies_logOESTR)                  0.06      0.27    -0.49     0.58 1.00     3829     6984\ncor(SRBodies_Intercept,SRBodies_logOESTR)              -0.04      0.28    -0.59     0.51 1.00     8507     6344\ncor(SRFaces_Intercept,SRBodies_logPROG)                -0.02      0.27    -0.55     0.50 1.00     8783     8879\ncor(SRFaces_logOESTR,SRBodies_logPROG)                 -0.04      0.27    -0.55     0.49 1.00     9136     9456\ncor(SRFaces_logPROG,SRBodies_logPROG)                  -0.02      0.28    -0.54     0.52 1.00     8549     9224\ncor(SRBodies_Intercept,SRBodies_logPROG)               -0.20      0.29    -0.69     0.41 1.00     3898     7763\ncor(SRBodies_logOESTR,SRBodies_logPROG)                -0.11      0.29    -0.63     0.46 1.00     5296     8147\ncor(SRFaces_Intercept,SRKissing_Intercept)              0.03      0.27    -0.50     0.55 1.00     7627     8351\ncor(SRFaces_logOESTR,SRKissing_Intercept)               0.04      0.27    -0.49     0.54 1.00     9729     8352\ncor(SRFaces_logPROG,SRKissing_Intercept)               -0.02      0.27    -0.54     0.51 1.00     7009     7496\ncor(SRBodies_Intercept,SRKissing_Intercept)             0.04      0.27    -0.49     0.54 1.00     8606     9138\ncor(SRBodies_logOESTR,SRKissing_Intercept)              0.04      0.27    -0.50     0.56 1.00     6706     9178\ncor(SRBodies_logPROG,SRKissing_Intercept)              -0.01      0.28    -0.55     0.53 1.00     7502     9014\ncor(SRFaces_Intercept,SRKissing_logOESTR)               0.03      0.26    -0.49     0.53 1.00     4404     7148\ncor(SRFaces_logOESTR,SRKissing_logOESTR)                0.02      0.26    -0.49     0.50 1.00     6463     7900\ncor(SRFaces_logPROG,SRKissing_logOESTR)                -0.04      0.28    -0.56     0.50 1.00     4373     7109\ncor(SRBodies_Intercept,SRKissing_logOESTR)              0.04      0.26    -0.47     0.53 1.00     7013     9063\ncor(SRBodies_logOESTR,SRKissing_logOESTR)               0.04      0.27    -0.49     0.55 1.00     5720     8204\ncor(SRBodies_logPROG,SRKissing_logOESTR)               -0.01      0.27    -0.54     0.51 1.00     5952     7872\ncor(SRKissing_Intercept,SRKissing_logOESTR)            -0.05      0.28    -0.58     0.50 1.00     6173     8371\ncor(SRFaces_Intercept,SRKissing_logPROG)                0.04      0.27    -0.49     0.55 1.00    12145     9111\ncor(SRFaces_logOESTR,SRKissing_logPROG)                 0.06      0.28    -0.48     0.58 1.00    13327     9312\ncor(SRFaces_logPROG,SRKissing_logPROG)                  0.02      0.27    -0.51     0.54 1.00    12235     9822\ncor(SRBodies_Intercept,SRKissing_logPROG)               0.02      0.27    -0.51     0.53 1.00    12509     9268\ncor(SRBodies_logOESTR,SRKissing_logPROG)                0.01      0.28    -0.52     0.54 1.00    10632     9904\ncor(SRBodies_logPROG,SRKissing_logPROG)                -0.04      0.28    -0.56     0.51 1.00     9590     9562\ncor(SRKissing_Intercept,SRKissing_logPROG)             -0.02      0.28    -0.56     0.53 1.00    10356    10061\ncor(SRKissing_logOESTR,SRKissing_logPROG)              -0.03      0.28    -0.56     0.51 1.00     9963     9191\ncor(SRFaces_Intercept,SRIntercourse_Intercept)         -0.07      0.27    -0.58     0.47 1.00     5145     7140\ncor(SRFaces_logOESTR,SRIntercourse_Intercept)          -0.08      0.26    -0.56     0.44 1.00     6862     8269\ncor(SRFaces_logPROG,SRIntercourse_Intercept)           -0.05      0.27    -0.56     0.50 1.00     5857     7855\ncor(SRBodies_Intercept,SRIntercourse_Intercept)        -0.09      0.26    -0.56     0.45 1.00     5792     5964\ncor(SRBodies_logOESTR,SRIntercourse_Intercept)         -0.06      0.27    -0.57     0.49 1.00     6194     8864\ncor(SRBodies_logPROG,SRIntercourse_Intercept)           0.07      0.28    -0.48     0.58 1.00     6205     8620\ncor(SRKissing_Intercept,SRIntercourse_Intercept)        0.09      0.28    -0.46     0.59 1.00     6045     9092\ncor(SRKissing_logOESTR,SRIntercourse_Intercept)         0.15      0.28    -0.42     0.65 1.00     5312     7865\ncor(SRKissing_logPROG,SRIntercourse_Intercept)         -0.00      0.27    -0.52     0.52 1.00     7463    10084\ncor(SRFaces_Intercept,SRIntercourse_logOESTR)          -0.02      0.27    -0.54     0.51 1.00     8691     8116\ncor(SRFaces_logOESTR,SRIntercourse_logOESTR)           -0.01      0.26    -0.51     0.50 1.00     9421     9011\ncor(SRFaces_logPROG,SRIntercourse_logOESTR)            -0.04      0.27    -0.55     0.50 1.00     7492     8835\ncor(SRBodies_Intercept,SRIntercourse_logOESTR)         -0.08      0.27    -0.58     0.47 1.00     7262     8595\ncor(SRBodies_logOESTR,SRIntercourse_logOESTR)          -0.06      0.27    -0.57     0.49 1.00     6478     8494\ncor(SRBodies_logPROG,SRIntercourse_logOESTR)            0.06      0.28    -0.49     0.57 1.00     7140     8383\ncor(SRKissing_Intercept,SRIntercourse_logOESTR)         0.07      0.28    -0.49     0.59 1.00     6995     9369\ncor(SRKissing_logOESTR,SRIntercourse_logOESTR)          0.11      0.28    -0.46     0.62 1.00     6502     9009\ncor(SRKissing_logPROG,SRIntercourse_logOESTR)          -0.00      0.28    -0.54     0.53 1.00     7081     9351\ncor(SRIntercourse_Intercept,SRIntercourse_logOESTR)    -0.06      0.28    -0.60     0.48 1.00     9499     9733\ncor(SRFaces_Intercept,SRIntercourse_logPROG)           -0.00      0.27    -0.53     0.52 1.00    14990     9485\ncor(SRFaces_logOESTR,SRIntercourse_logPROG)             0.01      0.27    -0.51     0.53 1.00    14252     9014\ncor(SRFaces_logPROG,SRIntercourse_logPROG)              0.00      0.28    -0.52     0.54 1.00    12405     8696\ncor(SRBodies_Intercept,SRIntercourse_logPROG)          -0.03      0.27    -0.56     0.50 1.00    13054     9155\ncor(SRBodies_logOESTR,SRIntercourse_logPROG)           -0.02      0.28    -0.55     0.52 1.00    10884     9401\ncor(SRBodies_logPROG,SRIntercourse_logPROG)             0.03      0.28    -0.52     0.56 1.00    10002     9051\ncor(SRKissing_Intercept,SRIntercourse_logPROG)          0.03      0.28    -0.51     0.55 1.00    10663    10129\ncor(SRKissing_logOESTR,SRIntercourse_logPROG)           0.04      0.28    -0.50     0.56 1.00    10282    10091\ncor(SRKissing_logPROG,SRIntercourse_logPROG)            0.03      0.28    -0.50     0.56 1.00     8733     9824\ncor(SRIntercourse_Intercept,SRIntercourse_logPROG)     -0.03      0.27    -0.54     0.50 1.00     9800    10886\ncor(SRIntercourse_logOESTR,SRIntercourse_logPROG)      -0.01      0.28    -0.54     0.53 1.00     8420    10232\n\nPopulation-Level Effects: \n                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nSRFaces_Intercept          -0.04      0.24    -0.51     0.44 1.00     9371     9434\nSRBodies_Intercept         -0.79      0.15    -1.09    -0.49 1.00     9116     9399\nSRKissing_Intercept         0.10      0.19    -0.27     0.48 1.00     9851     9558\nSRIntercourse_Intercept     0.22      0.20    -0.17     0.60 1.00    10865     9431\nSRFaces_logOESTR           -0.01      0.04    -0.09     0.07 1.00    11608     9730\nSRFaces_logPROG            -0.02      0.02    -0.06     0.03 1.00    12397    10268\nSRFaces_cycleZ2            -0.15      0.09    -0.32     0.03 1.00     8394     9534\nSRBodies_logOESTR           0.02      0.02    -0.02     0.07 1.00    13214     9353\nSRBodies_logPROG           -0.03      0.01    -0.06    -0.00 1.00    12169     9525\nSRBodies_cycleZ2           -0.15      0.05    -0.25    -0.05 1.00     9394     9186\nSRKissing_logOESTR          0.07      0.03     0.01     0.13 1.00    11914    10016\nSRKissing_logPROG          -0.03      0.02    -0.06     0.00 1.00    11387     9710\nSRKissing_cycleZ2           0.05      0.05    -0.05     0.16 1.00    10076     9242\nSRIntercourse_logOESTR      0.10      0.03     0.03     0.16 1.00    10294     8333\nSRIntercourse_logPROG      -0.03      0.02    -0.07    -0.00 1.00    11949     9636\nSRIntercourse_cycleZ2       0.00      0.06    -0.10     0.12 1.00    11584     9844\n\nFamily Specific Parameters: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_SRFaces           0.60      0.02     0.55     0.64 1.00     6182     9029\nsigma_SRBodies          0.35      0.01     0.33     0.38 1.00     5629     8346\nsigma_SRKissing         0.46      0.02     0.43     0.50 1.00     6691     8625\nsigma_SRIntercourse     0.49      0.02     0.46     0.53 1.00     7790     8743\n\nResidual Correlations: \n                                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nrescor(SRFaces,SRBodies)            0.30      0.05     0.20     0.39 1.00     9539     9467\nrescor(SRFaces,SRKissing)           0.15      0.05     0.05     0.25 1.00     8908     8678\nrescor(SRBodies,SRKissing)          0.18      0.05     0.08     0.27 1.00     9600     9746\nrescor(SRFaces,SRIntercourse)       0.09      0.05    -0.01     0.19 1.00    10994     9227\nrescor(SRBodies,SRIntercourse)      0.14      0.05     0.04     0.24 1.00    10321     8966\nrescor(SRKissing,SRIntercourse)     0.48      0.04     0.40     0.55 1.00     8200     9036\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nShow code\n\nLOO(m1mv0, m1mv)\n\nOutput of model 'm1mv0':\n\nComputed from 12000 by 551 log-likelihood matrix\n\n         Estimate    SE\nelpd_loo  -1662.1  53.8\np_loo       432.1  19.4\nlooic      3324.1 107.6\n------\nMonte Carlo SE of elpd_loo is NA.\n\nPareto k diagnostic values:\n                         Count Pct.    Min. n_eff\n(-Inf, 0.5]   (good)     431   78.2%   405       \n (0.5, 0.7]   (ok)       102   18.5%   116       \n   (0.7, 1]   (bad)       16    2.9%   31        \n   (1, Inf)   (very bad)   2    0.4%   9         \nSee help('pareto-k-diagnostic') for details.\n\nOutput of model 'm1mv':\n\nComputed from 12000 by 551 log-likelihood matrix\n\n         Estimate    SE\nelpd_loo  -1651.4  53.5\np_loo       509.6  21.8\nlooic      3302.8 107.0\n------\nMonte Carlo SE of elpd_loo is NA.\n\nPareto k diagnostic values:\n                         Count Pct.    Min. n_eff\n(-Inf, 0.5]   (good)     325   59.0%   777       \n (0.5, 0.7]   (ok)       183   33.2%   204       \n   (0.7, 1]   (bad)       39    7.1%   31        \n   (1, Inf)   (very bad)   4    0.7%   13        \nSee help('pareto-k-diagnostic') for details.\n\nModel comparisons:\n      elpd_diff se_diff\nm1mv    0.0       0.0  \nm1mv0 -10.7       7.0  \n\nAs you can see if you expand the detail above, this doesn’t lead to very different conclusions.\nA location-scale model\nSo, on analogue rating scales, you often see substantially heterogeneous variances, this is the case here too. Will accounting for it in a simple location-scale model make a difference? From here on out, I’m going to simplify and only look at one outcome (SR_Intercourse) for now. I’ll also drop the varying slopes by cycle for simplicity.\n\n\nShow code\n\nsds <- cycles_long %>% select(ID, cycle_time, starts_with(\"SR_\")) %>% pivot_longer(-c(ID, cycle_time)) %>% \n  group_by(ID, name) %>% \n  summarise(sd = sd(value)) %>% \n  group_by(name)\n\n\nsds %>% \n  arrange(sd) %>% \n  ggplot(aes(sd)) + \n  geom_histogram() + \n  facet_wrap(~ name, scales = \"free\")\n\n\n\nFigure 2: Heterogenity in standard deviations by person.\n\n\n\n\nModel output\n\n\nShow code\n\nm1intercourse <- brm(SR_Intercourse ~ logOESTR + logPROG + cycle + (1 + logOESTR + logPROG|i|ID), cycles_long, \n            iter = 4000, file = \"m1intercourse\")\n\nm1intercourse_sigma <- brm(bf(SR_Intercourse ~ logOESTR + logPROG + cycle + (1 + logOESTR + logPROG|i|ID),\n                        sigma ~ (1|i|ID)), cycles_long, \n            iter = 6000, file = \"m1intercourse_sigma\", \n            # file_refit = \"always\",\n            control = list(adapt_delta = .99))\nm1intercourse_sigma\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: SR_Intercourse ~ logOESTR + logPROG + cycle + (1 + logOESTR + logPROG | i | ID) \n         sigma ~ (1 | i | ID)\n   Data: cycles_long (Number of observations: 551) \n  Draws: 4 chains, each with iter = 6000; warmup = 3000; thin = 1;\n         total post-warmup draws = 12000\n\nGroup-Level Effects: \n~ID (Number of levels: 87) \n                               Estimate Est.Error l-95% CI u-95% CI\nsd(Intercept)                      0.79      0.17     0.46     1.15\nsd(logOESTR)                       0.08      0.04     0.01     0.17\nsd(logPROG)                        0.02      0.01     0.00     0.05\nsd(sigma_Intercept)                0.39      0.06     0.28     0.51\ncor(Intercept,logOESTR)           -0.19      0.39    -0.74     0.68\ncor(Intercept,logPROG)             0.22      0.41    -0.66     0.86\ncor(logOESTR,logPROG)             -0.07      0.44    -0.82     0.77\ncor(Intercept,sigma_Intercept)    -0.47      0.21    -0.85    -0.02\ncor(logOESTR,sigma_Intercept)      0.14      0.34    -0.57     0.74\ncor(logPROG,sigma_Intercept)      -0.23      0.42    -0.88     0.68\n                               Rhat Bulk_ESS Tail_ESS\nsd(Intercept)                  1.00     3690     4494\nsd(logOESTR)                   1.01      417     1162\nsd(logPROG)                    1.00     6321     7354\nsd(sigma_Intercept)            1.00     4169     7572\ncor(Intercept,logOESTR)        1.00     1096     3026\ncor(Intercept,logPROG)         1.00    13507     8678\ncor(logOESTR,logPROG)          1.00     9315     9202\ncor(Intercept,sigma_Intercept) 1.00     1293     2803\ncor(logOESTR,sigma_Intercept)  1.00     1043     1323\ncor(logPROG,sigma_Intercept)   1.01      637     2282\n\nPopulation-Level Effects: \n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept           0.25      0.19    -0.12     0.62 1.00     9289\nsigma_Intercept    -0.81      0.06    -0.92    -0.70 1.00     5269\nlogOESTR            0.09      0.03     0.03     0.15 1.00     9304\nlogPROG            -0.03      0.02    -0.06    -0.00 1.00     9162\ncycleZ2             0.03      0.04    -0.05     0.11 1.00    17403\n                Tail_ESS\nIntercept           9697\nsigma_Intercept     8109\nlogOESTR            9188\nlogPROG             9117\ncycleZ2             9139\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nNot so!\nGroup mean centering\nSo, actually we expect the effects of estradiol and progesterone to happen on the within-person level. Differences in average levels of E and P could actually confound the relationship we’re interested in. Adjusting for this is possible using various methods (this video gives a great introduction.\nWe can simply subtract the group mean from logOESTR and logPROG.\n\nModel output\n\n\nShow code\n\ncycles_long <- cycles_long %>% group_by(ID) %>% \n              mutate(logOESTRm = mean(logOESTR, na.rm = T),\n                     logPROGm = mean(logPROG, na.rm = T)) %>% \n              mutate(logOESTR_gmc = logOESTR - mean(logOESTR, na.rm = T),\n                     logPROG_gmc = logPROG - mean(logPROG, na.rm = T)) %>% \n              ungroup()\ncycles_long %>% select(starts_with(\"log\")) %>% cor(use = \"pairwise\") %>% round(2)\n\n             logOESTR logPROG logOESTRm logPROGm logOESTR_gmc\nlogOESTR         1.00    0.36      0.39     0.11         0.92\nlogPROG          0.36    1.00      0.08     0.28         0.36\nlogOESTRm        0.39    0.08      1.00     0.28         0.00\nlogPROGm         0.11    0.28      0.28     1.00         0.00\nlogOESTR_gmc     0.92    0.36      0.00     0.00         1.00\nlogPROG_gmc      0.35    0.96      0.00     0.00         0.37\n             logPROG_gmc\nlogOESTR            0.35\nlogPROG             0.96\nlogOESTRm           0.00\nlogPROGm            0.00\nlogOESTR_gmc        0.37\nlogPROG_gmc         1.00\n\nShow code\n\nm1intercoursegmc <- brm(SR_Intercourse ~ logOESTR + logPROG + cycle + (1 + logOESTR + logPROG|i|ID), cycles_long %>% group_by(ID) %>% \n              mutate(logOESTR = logOESTR - mean(logOESTR, na.rm = T),\n                     logPROG = logPROG - mean(logPROG, na.rm = T)) %>% \n              ungroup(), \n            iter = 4000, file = \"m1intercoursegmc\")\nm1intercoursegmc\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: SR_Intercourse ~ logOESTR + logPROG + cycle + (1 + logOESTR + logPROG | i | ID) \n   Data: cycles_long %>% group_by(ID) %>% mutate(logOESTR = (Number of observations: 551) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nGroup-Level Effects: \n~ID (Number of levels: 87) \n                        Estimate Est.Error l-95% CI u-95% CI Rhat\nsd(Intercept)               0.81      0.07     0.69     0.95 1.00\nsd(logOESTR)                0.07      0.04     0.00     0.17 1.00\nsd(logPROG)                 0.02      0.01     0.00     0.05 1.00\ncor(Intercept,logOESTR)     0.37      0.37    -0.53     0.92 1.00\ncor(Intercept,logPROG)      0.09      0.46    -0.82     0.87 1.00\ncor(logOESTR,logPROG)      -0.05      0.50    -0.88     0.87 1.00\n                        Bulk_ESS Tail_ESS\nsd(Intercept)               1728     3282\nsd(logOESTR)                2570     3237\nsd(logPROG)                 4973     4970\ncor(Intercept,logOESTR)     8913     5207\ncor(Intercept,logPROG)     12655     5410\ncor(logOESTR,logPROG)       8904     6668\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.74      0.09     0.56     0.92 1.00      945     2056\nlogOESTR      0.10      0.03     0.03     0.17 1.00    10853     6651\nlogPROG      -0.03      0.02    -0.07     0.00 1.00    11128     5939\ncycleZ2      -0.00      0.05    -0.09     0.09 1.00    12906     6139\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.51      0.02     0.48     0.54 1.00     8185     5933\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nWell, this makes little if any difference, which makes sense considering that there isn’t much between-subject variance in estradiol and progesterone to begin with.\nLatent group mean centering\nBeing a brms lover, I’ve been looking for an excuse to try Matti Vuorre’s implementation of latent group mean centering in brms. So, here goes. Edit: I’ve confirmed through more simulations that this approach does not work.\n\nModel output\n\n\nShow code\n\nlatent_formula <- bf(\n  SR_Intercourse ~ intercept + \n    blogOESTR*(logOESTR - logOESTRlm), # lm = latent mean,\n  intercept + blogOESTR + logOESTRlm ~ 1 + (1 | ID),\n  nl = TRUE\n) +\n  gaussian()\n\np <- get_prior(latent_formula, data = cycles_long) %>%\n  mutate(\n    prior = case_when(\n      class == \"b\" & coef == \"Intercept\" ~ \"normal(0, 1)\",\n      class == \"sd\" & coef == \"Intercept\" ~ \"student_t(7, 0, 1)\",\n      TRUE ~ prior\n    )\n  )\n\nfit_latent <- brm(\n  latent_formula,\n  data = cycles_long,\n  prior = p,\n  iter = 4000,\n  cores = 8, chains = 4, threads = 2,\n  backend = \"cmdstanr\",\n  control = list(adapt_delta = 0.99),\n  file = \"brm-fit-latent-mean-centered3\"\n)\n\nfit_latent\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: SR_Intercourse ~ intercept + blogOESTR * (logOESTR - logOESTRlm) \n         intercept ~ 1 + (1 | ID)\n         blogOESTR ~ 1 + (1 | ID)\n         logOESTRlm ~ 1 + (1 | ID)\n   Data: cycles_long (Number of observations: 562) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nGroup-Level Effects: \n~ID (Number of levels: 87) \n                         Estimate Est.Error l-95% CI u-95% CI Rhat\nsd(intercept_Intercept)      0.57      0.21     0.07     0.86 1.03\nsd(blogOESTR_Intercept)      0.08      0.03     0.01     0.14 1.02\nsd(logOESTRlm_Intercept)     1.25      1.30     0.04     5.08 1.02\n                         Bulk_ESS Tail_ESS\nsd(intercept_Intercept)       255      400\nsd(blogOESTR_Intercept)       347     1040\nsd(logOESTRlm_Intercept)      224      111\n\nPopulation-Level Effects: \n                     Estimate Est.Error l-95% CI u-95% CI Rhat\nintercept_Intercept      0.36      0.24    -0.09     0.83 1.01\nblogOESTR_Intercept      0.06      0.04    -0.01     0.13 1.01\nlogOESTRlm_Intercept     0.05      1.01    -1.94     2.04 1.00\n                     Bulk_ESS Tail_ESS\nintercept_Intercept       444      389\nblogOESTR_Intercept       748      618\nlogOESTRlm_Intercept     1151     1038\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.51      0.02     0.48     0.54 1.00     8720     5997\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nEdit: Here’s an approach that does work.\n\nModel output\n\n\nShow code\n\ncycles_long <- cycles_long %>% group_by(ID) %>% \n              mutate(logOESTR2 = logOESTR,\n                     seOE = sd(logOESTR, na.rm = T)/sum(!is.na(logOESTR)),\n                     seP = sd(logPROG, na.rm = T)/sum(!is.na(logOESTR)),\n                     logPROG2 = logPROG)\n\nfit_latent <- brm(\n  bf(SR_Intercourse ~ logOESTR + logPROG + \n       mi(logOESTR2) + mi(logPROG2) +\n    cycle + (1 + logOESTR + logPROG|i|ID)) +\n  bf(logPROG2 | mi(seP) ~ 1 + (1|ID)) +\n  bf(logOESTR2 | mi(seOE) ~ 1 + (1|ID)) + \n    set_rescor(FALSE), data = cycles_long, \n            iter = 4000, file = \"m1intercoursegmcmi\")\nfit_latent\n\n Family: MV(gaussian, gaussian, gaussian) \n  Links: mu = identity; sigma = identity\n         mu = identity; sigma = identity\n         mu = identity; sigma = identity \nFormula: SR_Intercourse ~ logOESTR + logPROG + mi(logOESTR2) + mi(logPROG2) + cycle + (1 + logOESTR + logPROG | i | ID) \n         logPROG2 | mi(seP) ~ 1 + (1 | ID) \n         logOESTR2 | mi(seOE) ~ 1 + (1 | ID) \n   Data: cycles_long (Number of observations: 551) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nGroup-Level Effects: \n~ID (Number of levels: 87) \n                                                    Estimate\nsd(SRIntercourse_Intercept)                             0.68\nsd(SRIntercourse_logOESTR)                              0.07\nsd(SRIntercourse_logPROG)                               0.02\nsd(logPROG2_Intercept)                                  0.07\nsd(logOESTR2_Intercept)                                 0.08\ncor(SRIntercourse_Intercept,SRIntercourse_logOESTR)     0.04\ncor(SRIntercourse_Intercept,SRIntercourse_logPROG)      0.06\ncor(SRIntercourse_logOESTR,SRIntercourse_logPROG)      -0.04\n                                                    Est.Error\nsd(SRIntercourse_Intercept)                              0.19\nsd(SRIntercourse_logOESTR)                               0.04\nsd(SRIntercourse_logPROG)                                0.01\nsd(logPROG2_Intercept)                                   0.05\nsd(logOESTR2_Intercept)                                  0.05\ncor(SRIntercourse_Intercept,SRIntercourse_logOESTR)      0.43\ncor(SRIntercourse_Intercept,SRIntercourse_logPROG)       0.48\ncor(SRIntercourse_logOESTR,SRIntercourse_logPROG)        0.49\n                                                    l-95% CI u-95% CI\nsd(SRIntercourse_Intercept)                             0.28     1.04\nsd(SRIntercourse_logOESTR)                              0.01     0.15\nsd(SRIntercourse_logPROG)                               0.00     0.05\nsd(logPROG2_Intercept)                                  0.00     0.20\nsd(logOESTR2_Intercept)                                 0.00     0.19\ncor(SRIntercourse_Intercept,SRIntercourse_logOESTR)    -0.69     0.85\ncor(SRIntercourse_Intercept,SRIntercourse_logPROG)     -0.84     0.87\ncor(SRIntercourse_logOESTR,SRIntercourse_logPROG)      -0.88     0.87\n                                                    Rhat Bulk_ESS\nsd(SRIntercourse_Intercept)                         1.00     1937\nsd(SRIntercourse_logOESTR)                          1.02      216\nsd(SRIntercourse_logPROG)                           1.00     3806\nsd(logPROG2_Intercept)                              1.00     4910\nsd(logOESTR2_Intercept)                             1.00     2290\ncor(SRIntercourse_Intercept,SRIntercourse_logOESTR) 1.00      947\ncor(SRIntercourse_Intercept,SRIntercourse_logPROG)  1.00     8227\ncor(SRIntercourse_logOESTR,SRIntercourse_logPROG)   1.00     6669\n                                                    Tail_ESS\nsd(SRIntercourse_Intercept)                             1606\nsd(SRIntercourse_logOESTR)                               832\nsd(SRIntercourse_logPROG)                               3384\nsd(logPROG2_Intercept)                                  3494\nsd(logOESTR2_Intercept)                                 2522\ncor(SRIntercourse_Intercept,SRIntercourse_logOESTR)     1876\ncor(SRIntercourse_Intercept,SRIntercourse_logPROG)      4660\ncor(SRIntercourse_logOESTR,SRIntercourse_logPROG)       5849\n\nPopulation-Level Effects: \n                          Estimate Est.Error l-95% CI u-95% CI Rhat\nSRIntercourse_Intercept       0.24      0.21    -0.17     0.65 1.00\nlogPROG2_Intercept            1.69      0.06     1.57     1.82 1.00\nlogOESTR2_Intercept           5.86      0.03     5.79     5.92 1.00\nSRIntercourse_logOESTR        0.25      0.31    -0.36     0.87 1.00\nSRIntercourse_logPROG        -0.07      0.26    -0.58     0.45 1.00\nSRIntercourse_cycleZ2        -0.00      0.05    -0.09     0.09 1.00\nSRIntercourse_milogOESTR2    -0.15      0.32    -0.80     0.46 1.00\nSRIntercourse_milogPROG2      0.04      0.27    -0.50     0.56 1.00\n                          Bulk_ESS Tail_ESS\nSRIntercourse_Intercept       5048     5438\nlogPROG2_Intercept           10305     6300\nlogOESTR2_Intercept          10605     6361\nSRIntercourse_logOESTR        1668     2497\nSRIntercourse_logPROG          863     1624\nSRIntercourse_cycleZ2        11095     5934\nSRIntercourse_milogOESTR2     1657     2569\nSRIntercourse_milogPROG2       864     1578\n\nFamily Specific Parameters: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat\nsigma_SRIntercourse     0.50      0.02     0.47     0.54 1.00\nsigma_logPROG2          1.43      0.04     1.34     1.52 1.00\nsigma_logOESTR2         0.77      0.02     0.73     0.82 1.00\n                    Bulk_ESS Tail_ESS\nsigma_SRIntercourse     4409     4243\nsigma_logPROG2         11362     5936\nsigma_logOESTR2        10387     6745\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nImputations and lag\nTo get at the question, whether estradiol and progesterone have time-delayed effects on sexual desire, we would ideally like to have measured serum steroids a few days ahead. Unfortunately, this wasn’t done here (they did measure serum steroids on some other days, but did not share those data).\nA simple solution would be to substitute in my imputed hormones for the days two days prior to the rating task.\n\nModel output\n\n\nShow code\n\nm1_lagi <- brm(SR_Intercourse ~ est_estradiol_fc_lag2 + est_progesterone_fc_lag2 + cycle + (1 + est_estradiol_fc_lag2 + est_progesterone_fc_lag2|i|ID), cycles_longer, \n            iter = 6000, file = \"m1mv_lagi\",\n            control = list(adapt_delta = 0.99))\nm1_lagi\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: SR_Intercourse ~ est_estradiol_fc_lag2 + est_progesterone_fc_lag2 + cycle + (1 + est_estradiol_fc_lag2 + est_progesterone_fc_lag2 | i | ID) \n   Data: cycles_longer (Number of observations: 570) \n  Draws: 4 chains, each with iter = 6000; warmup = 3000; thin = 1;\n         total post-warmup draws = 12000\n\nGroup-Level Effects: \n~ID (Number of levels: 88) \n                                                    Estimate\nsd(Intercept)                                           0.72\nsd(est_estradiol_fc_lag2)                               0.17\nsd(est_progesterone_fc_lag2)                            0.03\ncor(Intercept,est_estradiol_fc_lag2)                   -0.26\ncor(Intercept,est_progesterone_fc_lag2)                -0.11\ncor(est_estradiol_fc_lag2,est_progesterone_fc_lag2)     0.10\n                                                    Est.Error\nsd(Intercept)                                            0.34\nsd(est_estradiol_fc_lag2)                                0.07\nsd(est_progesterone_fc_lag2)                             0.02\ncor(Intercept,est_estradiol_fc_lag2)                     0.47\ncor(Intercept,est_progesterone_fc_lag2)                  0.49\ncor(est_estradiol_fc_lag2,est_progesterone_fc_lag2)      0.47\n                                                    l-95% CI u-95% CI\nsd(Intercept)                                           0.10     1.40\nsd(est_estradiol_fc_lag2)                               0.05     0.32\nsd(est_progesterone_fc_lag2)                            0.00     0.09\ncor(Intercept,est_estradiol_fc_lag2)                   -0.85     0.79\ncor(Intercept,est_progesterone_fc_lag2)                -0.89     0.84\ncor(est_estradiol_fc_lag2,est_progesterone_fc_lag2)    -0.79     0.89\n                                                    Rhat Bulk_ESS\nsd(Intercept)                                       1.00     1663\nsd(est_estradiol_fc_lag2)                           1.02      651\nsd(est_progesterone_fc_lag2)                        1.00     1404\ncor(Intercept,est_estradiol_fc_lag2)                1.01      711\ncor(Intercept,est_progesterone_fc_lag2)             1.00     4776\ncor(est_estradiol_fc_lag2,est_progesterone_fc_lag2) 1.00     5148\n                                                    Tail_ESS\nsd(Intercept)                                           3732\nsd(est_estradiol_fc_lag2)                               1688\nsd(est_progesterone_fc_lag2)                            2613\ncor(Intercept,est_estradiol_fc_lag2)                    2065\ncor(Intercept,est_progesterone_fc_lag2)                 7047\ncor(est_estradiol_fc_lag2,est_progesterone_fc_lag2)     7309\n\nPopulation-Level Effects: \n                         Estimate Est.Error l-95% CI u-95% CI Rhat\nIntercept                    0.38      0.22    -0.03     0.82 1.00\nest_estradiol_fc_lag2        0.17      0.05     0.06     0.27 1.00\nest_progesterone_fc_lag2    -0.05      0.02    -0.10    -0.01 1.00\ncycleZ2                     -0.03      0.05    -0.12     0.07 1.00\n                         Bulk_ESS Tail_ESS\nIntercept                   19032     9683\nest_estradiol_fc_lag2       14482    10340\nest_progesterone_fc_lag2    18940     8961\ncycleZ2                     25540     7844\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.51      0.02     0.47     0.54 1.00     4467     7815\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nShow code\n\nm1_i <- brm(SR_Intercourse ~ est_estradiol_fc + est_progesterone_fc + cycle + (1 + est_estradiol_fc + est_progesterone_fc|i|ID), cycles_longer, \n            iter = 6000, file = \"m1_i\",\n            control = list(adapt_delta = 0.9))\nm1_i\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: SR_Intercourse ~ est_estradiol_fc + est_progesterone_fc + cycle + (1 + est_estradiol_fc + est_progesterone_fc | i | ID) \n   Data: cycles_longer (Number of observations: 570) \n  Draws: 4 chains, each with iter = 6000; warmup = 3000; thin = 1;\n         total post-warmup draws = 12000\n\nGroup-Level Effects: \n~ID (Number of levels: 88) \n                                          Estimate Est.Error l-95% CI\nsd(Intercept)                                 0.61      0.30     0.06\nsd(est_estradiol_fc)                          0.13      0.07     0.01\nsd(est_progesterone_fc)                       0.04      0.03     0.00\ncor(Intercept,est_estradiol_fc)              -0.13      0.48    -0.85\ncor(Intercept,est_progesterone_fc)           -0.10      0.48    -0.87\ncor(est_estradiol_fc,est_progesterone_fc)     0.16      0.46    -0.74\n                                          u-95% CI Rhat Bulk_ESS\nsd(Intercept)                                 1.28 1.00     1653\nsd(est_estradiol_fc)                          0.27 1.00      812\nsd(est_progesterone_fc)                       0.11 1.00     1392\ncor(Intercept,est_estradiol_fc)               0.83 1.00     1217\ncor(Intercept,est_progesterone_fc)            0.84 1.00     3423\ncor(est_estradiol_fc,est_progesterone_fc)     0.91 1.00     3410\n                                          Tail_ESS\nsd(Intercept)                                 1468\nsd(est_estradiol_fc)                          1813\nsd(est_progesterone_fc)                       3218\ncor(Intercept,est_estradiol_fc)               1719\ncor(Intercept,est_progesterone_fc)            5540\ncor(est_estradiol_fc,est_progesterone_fc)     5403\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat\nIntercept               0.15      0.23    -0.30     0.59 1.00\nest_estradiol_fc        0.20      0.06     0.08     0.31 1.00\nest_progesterone_fc    -0.04      0.02    -0.08     0.01 1.00\ncycleZ2                -0.03      0.05    -0.12     0.06 1.00\n                    Bulk_ESS Tail_ESS\nIntercept              14741     9070\nest_estradiol_fc       10393     4474\nest_progesterone_fc    10516     2249\ncycleZ2                18084     8375\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.51      0.02     0.48     0.55 1.00     5034     2081\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nDirectionally, the same-day imputed hormones has a slightly stronger relationship with SR_Intercourse for oestradiol, and the two-day lag imputation has a slightly stronger relationship with progesterone. Not much that can be concluded at this sample size though.\nLatent lag\nJust using the imputations leaves money on the table though. Next, I thought I would use the strong relationship between imputed hormones and measured hormones to impute the missing values two days prior (and thereby carry forward the inherent uncertainty in imputation plus the individual differences, what little there are).\nI thought I needed only to use the syntactic sugar for missing variables in brms (mi()). After some reshaping magic, I thought I had it, but, nope, it took forever to fit2. And I’ve never seen that many warnings from a Stan model before. I did not succeed in fixing them with the usual tricks (more informative priors, inits, playing with control parameters).\nEdit: Sleeping on it, the solution came to me in a dream.3. That solution did not completely fix the model either though. What did it was rereading the brms vignette on missing values and noticing that Paul adds the | mi() also for the main response. This is necessary so brms won’t drop the rows in which the response is missing. I think you can get away with not doing so, if there is overlap, but in my case there was zero overlap (all values that had an outcome did not have lagged steroid measures). So, I added | mi() to by response SR_Intercourse.\n\nModel output\n\n\nShow code\n\nmis_imp_formula = bf(SR_Intercourse | mi() ~ mi(logOESTR_lag2) + mi(logPROG_lag2) + cycle + (1|ID)) +\n  bf(logOESTR_lag2 | mi() ~ est_estradiol_fc_lag2 + (1|ID)) +\n  bf(logPROG_lag2 | mi() ~ est_progesterone_fc_lag2 + (1|ID)) +\n    set_rescor(FALSE)\n\np <- get_prior(mis_imp_formula, data = cycles_longer) %>%\n  mutate(\n    prior = case_when(\n      class == \"b\" & coef == \"Intercept\" ~ \"normal(0, 2)\",\n      class == \"b\"  ~ \"normal(0, 1)\",\n      class == \"sd\" & coef == \"Intercept\" ~ \"student_t(3, 0, 0.5)\",\n      TRUE ~ prior\n    )\n  )\n\n\nm1lag <- brm(\n  mis_imp_formula,\n  cycles_longer, \n  iter = 4000, \n  init = 0,\n  file = \"m1_impute_latent\",\n  control = list(adapt_delta = 0.99, max_treedepth = 15),\n  prior = p\n  )\n\n\nInstead, I took a leaf out of Matti Vuorre’s book and tried my hand at the nonlinear formula syntax. I find this much less convenient to specify and harder to think about4.\nIt worries me that the results of the latent lag model are more like the results of the imputations without lag than of those with lag. So maybe I didn’t specify the nonlinear model correctly.\nEdit: I slept on it and I did not, so I’ve cut it here. You can see it on Github if you wish.\nBringing it all together\n\n\nShow code\n\ndraws <- bind_rows(\n  latent_impute_lag2 = m1lag %>% gather_draws(`bsp_SRIntercourse_milog.+`, regex = T) %>% mutate(.variable = str_replace(str_replace(.variable, \"bsp_SRIntercourse_mi\", \"b_\"), \"_lag2\", \"\")),\n  imputed_lag2 = m1_lagi %>% gather_draws(`b_est.+`, regex = T) %>% mutate(.variable = str_replace(str_replace(.variable, \"est_estradiol_fc_lag2\", \"logOESTR\"), \"est_progesterone_fc_lag2\", \"logPROG\")),\n  imputed = m1_i %>% gather_draws(`b_est.+`, regex = T) %>% mutate(.variable = str_replace(str_replace(.variable, \"est_estradiol_fc\", \"logOESTR\"), \"est_progesterone_fc\", \"logPROG\")),\n  latent_group_mean_centered = fit_latent %>% gather_draws(`b_SRIntercourse_log[A-Z]+`, regex = T) %>% mutate(.variable = str_replace(str_replace(.variable, \"b_SRIntercourse_\", \"b_\"), \"_Intercept\", \"\")),\n  group_mean_centered = m1intercoursegmc %>% gather_draws(`b_log.+`, regex = T),\n  location_scale = m1intercourse_sigma %>% gather_draws(`b_log.+`, regex = T),\n  multivariate = m1mv %>% gather_draws(`b_SRIntercourse_log.+`, regex = T) %>% mutate(.variable = str_replace(.variable, \"b_SRIntercourse_\", \"b_\")),\n  raw = m1intercourse %>% gather_draws(`b_log.+`, regex = T), .id = \"model\") %>%   mutate(model = fct_inorder(factor(model)))\ndraws <- draws %>% group_by(model, .variable) %>% \n  mean_hdci(.width = c(.95, .99)) %>% \n  ungroup()\n\nggplot(draws, aes(y = .variable, x = .value, xmin = .lower, xmax = .upper,\n                  color = model)) +\n  geom_pointinterval(position = position_dodge(width = .4)) +\n  geom_vline(xintercept = 0, linetype = 'dashed') +\n  scale_color_discrete(breaks = rev(levels(draws$model))) +\n  theme_bw() +\n  theme(legend.position = c(0.99,0.99),\n        legend.justification = c(1,1))\n\n\n\nFigure 3: Non-varying slopes for log estradiol and log progesterone\n\n\n\n“Conclusion”\nIn summary, I would say my different analyses did not yield very different conclusions at this sample size. If the authors had shared even more data, maybe slightly cooler reanalyses would be possible. Who knows maybe that twofold change in the effect size for estradiol when bringing in imputations and lags is real.\nI ended up not bothering to bring it all together in one model, but would be interested to see what happens if you, dear reader, give it a go. Given the rest of the literature, I still put stock in a peri-ovulatory sexual desire peak, but I think this is more evidence that we all should design studies to detect small effects (here and most elsewhere in psychology) and effect heterogeneity (especially here).\nCycle researchers have recently started sharing data more widely. It’s cool to see this catch on even in medicine and I hope it continues.5 I think there are interesting substantive and statistical questions both remaining to be answered in this research.\nThings I didn’t do or that still confuse me\nI didn’t do any model comparisons here.\nI never brought it all together in one multivariate location-scale model with imputations and lags\nI didn’t do any group mean centering at the level of the cycle.\nIt confuses me that the results of the latent lag model are more like the results of the imputations without lag than of those with lag.\n\ntheir previous publications on different outcomes in the same study unfortunately didn’t do so↩︎\nwhich, according to the first folk theorem of statistical computing, means there’s something wrong with my model↩︎\nI had confused which imputations to use as predictors and should have used the lagged ones.↩︎\nIf I wanted to be mean, I’d say it feels a little like MPlus with those very strict rules about how variables and parameters can be named.↩︎\nIt would be extra cool if preregistration catches on there outside the narrow remit of clinical trials too.↩︎\n",
    "preview": "https://ars.els-cdn.com/content/image/1-s2.0-S0306453023000380-gr1.jpg",
    "last_modified": "2023-03-15T14:54:53+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-20-opportunity-equality-and-genetics-session-2/",
    "title": "Opportunity, equality, and genetics, seminar session 2",
    "description": "Notes on my seminar (German title \"Chancen, Gleichheit und Genetik\") oriented around K. Paige Harden's book \"The Genetic Lottery\" (2021).",
    "author": [
      {
        "name": "Ruben C. Arslan",
        "url": "https://rubenarslan.github.io"
      }
    ],
    "date": "2022-10-20",
    "categories": [
      "teaching",
      "opportunity-equality-genetics"
    ],
    "contents": "\n\nContents\nHow to discuss\nComprehension questions\nQuiz\nPreview\n\nSession 2 is still mainly input from me, with some discussion. We talk about how to have discussions on the controversial topics of the book, I respond to comprehension questions about the chapter, show them the results of their quiz.\nHow to discuss\nI put up a few principles up for discussion (assume good faith, avoid overloaded words, stay friendly, watch how much you’re speaking). Then, we play a game, a confirmation bias exercise. Last year, I learned that most of them knew the 2-4-6 Wason sequences game (where people ask in a confirmatory way and grow overconfident about the rule), so this year, I modified it a little (letters instead of numbers, more complex rule). They shout possible 5-letter-words and I tell them whether they fit the rule or not. I got the desired result (overconfidence about the rule after confirmatory questions), though my game was maybe a little overengineered.\nI use this game as a jumping off point for talking about human (ir)rationality. I tell them that some of the “biases, fallacies, priming, nudging” work hasn’t survived the replication crisis and that some of the newer work on fake news, filter bubbles, the post-truth age, the backfire effect also is full of holes. Still, confirmation bias is very replicable, as the exercise just showed. I namecheck Mercier & Sperber’s argumentative theory of reasoning as one compelling explanation.\nI then show them a graph of their political leanings from the anonymous quiz, which in Leipzig reliably shows that they lean left. I argue that to sharpen our arguments, we need some friction, otherwise we’re at risk of confirmation bias and ending up with weak arguments for predetermined conclusions. If we want sharp arguments that will survive a debate with someone further away on the political spectrum, we need to do better.\nHow? I planned to tell them about how the role of “advocatus diaboli” can help us keep this in check, as it did for the Catholic church, who ended up with an excess of saints and miracles after doing away with their advocatus diaboli. Unfortunately, that stylized fact doesn’t quite hold up to scrutiny (see tweet thread below). So, instead I told them about my journey of discovering that it’s more complicated, after of course seeking initially merely to confirm rather than disprove the story I heard. I guess it works on a meta level?\n\n\nA story I've heard repeated (e.g. in Zenko's book \"Red Team\"): when the Cath church got rid of devil's advocates, the number of saints rose sharply.Problem: I tried to find a quantitative treatment, but found only news items.Self-made graph from Wikipedia's list of blesseds 1/n pic.twitter.com/LQJGAxIyxU\n\n— Ruben C. Arslan (@rubenarslan) October 18, 2022\n\nAs part of this, I remind them that I want to be criticized as well, and that I have set up a way for them to contact me anonymously.\nThis part is a bit long. I don’t know if I need all of it, but the discussions last year were pretty healthy and my tendency is to keep the spiel.\nComprehension questions\nIn this part, I try to answer to comprehension questions they raised as part of their responses to the quiz. For chapter 1, a lot of people always ask about what is basically “happiness” economics. Harden challenges the ‘classic’ Kahneman & Deaton 2010 study which purportedly found a limit after which more money (in income) is no longer associated with more happiness. First, they answer a small survey about the limit where they think happiness no longer increases (or whether they believe in a relationship at all). The modal result is usually something like 80,000€ a year, but this year it was more like 100,000€ a year. Inflation? :-)\nThen, I show them the graph from K&D and then graphs from the international replication by Jebb, Tai, Diener, & Oishi (2018). These both show some sort of flattening or even a peak of happiness (with log income on the X axis). In discussion, they come up with all sorts of reasons why that may be.\nI ask them whether it fits their impression that most people stop hustling after their first million. Then, I show them the following clip:\n\n\nThen, I show them Matthew Killingsworth (2021)1, who found no peak, no flattening, just a plain old loglinear relationship. I walk them through the result, the explanation for the previous results (ceiling effects, insensitive at high levels of positive affect). Then, I show them the graph with income not on a log scale and ask them what it implies for the effect of redistribution on average happiness.\nQuiz\nHere, I just pick some items that many got wrong and try to explain the correct answer with some recycled slides from my lecture. Because they all did their B.Sc. in a bunch of different places, the quantity and quality of their previous genetics lectures varies from “none” to “in depth knowledge of the virtues of the candidate gene paradigm” to “twin studies”. I don’t have the ambition to lay all the groundwork myself, because the book does it well, but of course it’s nice to add some graphs and other illustrative examples.\nPreview\nTo anticipate the next chapter, I close with a video of a Galton board/bean machine plus a picture of a lognormal variant (since we just talked about income). Then, some pictures of basketballers and a scatterplot of height for MZ/DZ twins, because Harden will go on to talk about Shawn Bradley, a very tall basketballer who has been genotyped.\n\nso jealous of this name for a man with this research topic↩︎\n",
    "preview": "https://pbs.twimg.com/media/FfVg9isWIAAGV-Z?format=png&name=medium",
    "last_modified": "2022-10-20T22:34:14+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-19-opportunity-equality-and-genes-a-seminar/",
    "title": "Opportunity, equality, and genes, a seminar on The Genetic Lottery",
    "description": "Notes on my seminar (German title \"Chancen, Gleichheit und Genetik\") oriented around K. Paige Harden's book \"The Genetic Lottery\" (2021).",
    "author": [
      {
        "name": "Ruben C. Arslan",
        "url": "https://rubenarslan.github.io"
      }
    ],
    "date": "2022-10-19",
    "categories": [
      "teaching",
      "opportunity-equality-genetics"
    ],
    "contents": "\n\nContents\nThe course\nStudent’s tasks\nReadings\nQuizzes and questionnaires\nUsual schedule\n\nFirst session\n\nLast autumn, I decided, on impulse, to base a Master’s level seminar in personality psychology around Paige Harden’s book The Genetic Lottery. I had only recently obtained a review copy, but I already thought it could be good material for a discussion-oriented seminar based on the many reviews which disagreed with the book and each other. Also, I had enjoyed much of Harden’s writing on her (now-defunct?) blog.\n\n\n\nFigure 1: Book Cover\n\n\n\nIn my opinion and according to the student evaluations, the seminar was a success. Some colleagues asked me about my syllabus, but I didn’t have anything in English. This year, I thought I’d try to keep notes on this blog. Maybe it’ll be helpful to others considering this/a similar format. Or to me, next year.\nThe course\nThe course is open to Master’s level psychology students, though one particularly bright B.Sc. student attended last year too. It’s a 2.5 hour session with a break in the middle.\nStudent’s tasks\nread the book and some additional material, mandatory weekly quiz, not graded\ngive a 10 minute talk on background literature (a paper cited in the book or a paper that could have been cited in the book, I will probably keep replacing papers with newer ones given the rapid developments in genetics), not graded\nsubmit questions for three additional papers\nwrite an essay based on the discussion in the seminar (usually for the topic of the talk, but can differ), graded\nReadings\nThe students read one chapter from the book per week. I added one session with a chapter on sex and gender from Kevin Mitchell’s book Innate, because Harden skips this topic entirely.1\nI also added three videos from Matt McGue’s Coursera course (on eugenics, the John-Joan twins/David Reimer, & PKU) because he can add a bit of history that neither me or Harden lived through. Then, I have them read two blog posts by Scott Siskind on 5-HTTLPR (post cited in the book) and on ADHD (because it makes some discussion-worthy points about equality of opportunity/medically compensating for disadvantages when it comes to a normally distributed trait).\nQuizzes and questionnaires\nAfter the first session, I have them fill out a questionnaire which contains some items from studies discussed in the book\nthe LEGIT questionnaire (estimate h2 for various traits)\nattitude questions (on genes, luck, egalitarianism, their connections)\none question on political leaning left-right\nestimation questions about child mortality and extreme poverty\na brief quiz on genetics so they can self-assess how much they know\nThen, for each session, there’s a brief quiz for them to check their comprehension of the book (and my notes in the margin). In the same quiz, they can submit comprehension questions to me.\nUsual schedule\n13:15-13:20 Organisational matters and short impulse from me\n13:20-13:40 Comprehension questions about the chapter\n13:40-13:50 Impulse talk 1\n13:50-14:00 Discussion, questions 1\n14:00-14:20 Break\n14:20-14:30 Impulse talk 2\n14:30-14:40 Discussion, questions 2\n14:40-15:00 Discussion in small groups\n15:00-15:25 Joint discussion\n15:25-15:35 Teaser/preview next week from me\n15:35 End/assigned tasks for next week\nFirst session\nIn the first session, I introduce the author, the authors of the other readings, talk a little about eugenics and “this debate” in the German context (the book is quite US-centric). I talk about why I think it’s an apt course in the personality module. Then, I introduce the impulse talk papers and have them pick favorites via simpleassign.com.\nI end with this video. Harden cites the study from which it’s derived (Brosnan & de Waal, 2003) in her first chapter:\n\nEven monkeys have a sense of fairness. If two capuchin monkeys are “paid” in cucumber slices for performing a simple task, they will both happily pull levers and munch on their cucumber snacks. Start paying just one monkey in grapes, however, and watch the other monkey throw the cucumber back in the experimenter’s face with the indignation of Jesus flipping the tables of the moneychangers.\n\nI ask the students to think about whether this video shows a sense of fairness or even “inequity aversion” in academese. This time, one noted that you could just call it “jealousy”. After all, the grape-ionnaire monkey is not indignant at all. Once we reach this point, I show a slide with tweets by several primatologists.\n\n\nCute videos are mind viruses that produce \"zombie ideas\" (i.e. ideas that just won't die, no matter how many times you kill them). https://t.co/CAEcOk2uFL\n\n— Claudio Tennie (@CTennie) November 20, 2019\n\n\n\nProblems with the logic of the paradigm:https://t.co/JaetGhAcOaFailed replications (review): https://t.co/m79JvgFtigFailed replication and alternative explanation for effect (example): https://t.co/s34WqpKZMrand another (chimps): https://t.co/kq1osGrjCP\n\n— Prof Nichola Raihani (@nicholaraihani) November 22, 2019\n\nIn short, many primatologists hate this video and study. There are conceptual issues.2 There are design issues.3 It doesn’t replicate.4 It fits Jeremy Freese’s immortal words “more vampirical than empirical, unable to be killed by mere evidence”.\nNow, this study is no cornerstone in Harden’s book. She concludes the section as follows.\n\nAs human adults, we share with our children and our primate cousins an evolved psychology that is instinctively outraged by unfairness. Right now, such outrage is bubbling all around us, threatening to boil over at any moment. In 2019, the three richest billionaires in the US possessed more wealth than the poorest 50 percent of the country. Like capuchin monkeys being paid in cucumbers when their neighbor is being paid in grapes, many of us look at the inequalities in our society and think: “This is unfair.”\n\nSo, it’d be interesting if a sense of fairness is so basic, so widespread, fundamental. But of course, if capuchin monkeys and children don’t share our moral views (and they don’t on things like dominance hierarchies and violence), we don’t have to change our moral views.5.\nI use this teaser to say: Even Paige Harden6 gets things wrong and to turn this into more than reading a pop science book, we need to do read critically and do our own research.\n\nEven though it fits in perfectly in my opinion given that biological sex is randomly assigned, genetic, and we have lots of work unpacking the problem that identifying a causal effect doesn’t mean we understand the mechanism, or that a gene having a causal effect implies only biological processes). I presume she simply had her fill of explosive topics, but in my experience I can use SRY to build intuitions about other genes↩︎\nIs it inequity aversion if the better-paid doesn’t care?↩︎\nMonkeys switched from a model to an observer role, this seems to have induced the frustration.↩︎\nAfter correcting the design mistake↩︎\nOr we shouldn’t I guess. The kind of men who go for the alpha male world view rarely reference alpha male capuchin monkeys as their model for some reason. Too cute probably.↩︎\nwho I’ve previously built up as pretty cool↩︎\n",
    "preview": "https://pup-assets.imgix.net/onix/images/9780691190808.jpg?w=1500&auto=format",
    "last_modified": "2022-10-20T00:22:10+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-09-23-hibar-effects-of-acetaminophen-on-risk-taking/",
    "title": "HIBAR: Effects of acetaminophen on risk taking (with virtual balloons)",
    "description": "Had I Been A Reviewer.",
    "author": [
      {
        "name": "Ruben C. Arslan",
        "url": "https://rubenarslan.github.io"
      }
    ],
    "date": "2020-09-23",
    "categories": [
      "HIBAR",
      "re-analysis",
      "risk taking"
    ],
    "contents": "\n\n\n\nFigure 1: An analogue of the Balloon Analogue Risk Task (BART). From the Internet Archive Book Images\n\n\n\nUpdate March 4, 2021\nI exchanged a few emails with Baldwin Way. After two responses, he stopped replying.\nHe did not make the “preregistration” public, after I repeatedly explained the problem (and why adding me as a contributor was not a real solution).\nHe gave an explanation why the sample sizes deviated from plans (logistic reasons with subject recruitment) and explicitly denied optional stopping.\nHe said that they have updated their IRB to be able to share data in the future.\nHe said that this study was a result of emptying their file drawer on acetaminophen and risk preference (apparently, they consider the study on ibuprofen and risk preference to belong in a different file drawer).\nSeveral of my concerns below are not allayed by his responses, and I’m further concerned that Way does not seem motivated to fix unambiguous errors (non-public preregistration that was not frozen, deviation from preregistered analysis) or respond to a critic. His final sentence to me was that I should try running acetaminophen studies myself, but given the presented evidence, I currently feel like this would be a waste of time.\nOriginal post\nA journalist is calling me later today to talk about this recently published paper, so I read it.\nAlthough I have a research interest in risk taking, I had not read the recent flurry of papers that link acetaminophen (paracetamol/Tylenol) to various psychological outcomes.\n\n\n\nFigure 2: The many risks of balloonery. From Punch in the Internet Archive Book Images\n\n\n\nHere’s the abstract of the paper:\n\nAcetaminophen, an analgesic and antipyretic available over-the-counter and used in over 600 medicines, is one of the most consumed drugs in the USA. Recent research has suggested that acetaminophen’s effects extend to the blunting of negative as well as positive affect. Because affect is a determinant of risk perception and risk taking, we tested the hypothesis that acute acetaminophen consumption (1000 mg) could influence these important judgments and decisions. In three double-blind, placebo-controlled studies, healthy young adults completed a laboratory measure of risk taking (Balloon Analog Risk Task) and in Studies 1 and 2 completed self-report measures of risk perception. Across all studies (total n = 545), acetaminophen increased risk-taking behavior. On the more affectively stimulating risk perception measure used in Study 2, acetaminophen reduced self-reported perceived risk and this reduction statistically mediated increased risk-taking behavior. These results indicate that acetaminophen can increase risk taking, which may be due to reductions in risk perceptions, particularly those that are highly affect laden.\n\nWell, because of my own research and studies like Frey et al. 2017, I have my doubts that changes on the BART will generalize to changes in real world risk taking. I think it’s a somewhat silly task, where the optimal behaviour is to inflate several balloons 64/128 times (unlike real balloons, the balloon has a 1/128 chance of bursting at the first press, which goes to 100% chance of bursting at the 128th press). In the study, they played this task for “imaginary money”, which they “lost” when the balloon burst, so high scores mainly mean people pressed a button a lot of times. Unsurprisingly, people rarely “pump” (press a button) the optimal number of times, perhaps because it is boring to do so. Ironically, the BART is in wide use, because it is supposed to measure the affectively-laden side of risk taking (rather than supposedly even more boring lotteries). To be honest, I would not bet money that the BART even predicts how much air people put in a real balloon, let alone bigger life choices.\nThey also look at self-reported perceptions of risk on the DOSPERT and in a inventory by Finucane, finding mixed results.\nBut before asking whether the results will generalize to real world behavior, I should first try to judge whether the results are even likely to replicate.\n\n\n\nFigure 3: The many risks of balloonery, part 2. From the Internet Archive\n\n\n\nAn initial overview doesn’t inspire confidence:\nThe two significant p values for the BART are on the uncanny mountain (BART S1: 0.023, S2: 0.033, S3: 0.84, Combined: 0.024). Such p values should be rare, but they are common in this study.\nOnly one study, Study 1, was preregistered. Studies 2 and 3 were not, even though they were highly similar.\nThe preregistration link on OSF for Study 1 is not accessible.\nSeveral other outcomes (Columbia Card Sorting, Iowa Gambling, etc.) are mentioned in the text, but “will be reported separately”.\nThe data are not public, even though they are not sensitive and can easily be anonymized (the authors did not obtain the necessary consent).\nThere are multiple outcomes and multiple hypotheses relating to these outcomes (pertaining to mean differences and correlations), but no accounting for multiple testing.\nThe DOSPERT outcome is present and significantly different in S2 (P = 0.002). Study 3 “replicates and extends” Study 2, but the DOSPERT is not reported.\nSeveral apparent mediators (emotional experience during the BART) that did not show significant differences were relegated to the supplement.\nI emailed the last author, Baldwin Way, to get access to the preregistration, which he granted (the link still isn’t public).\nAs it turns out:\nThe preregistration wasn’t formally registered with OSF (i.e. the version was not frozen). This is not an uncommon error, but we can look at file timestamps.\nThe date the preregistration was uploaded (2015-01-26) was after the study’s start date (according to BW: Study 1 was run between Jan 15th and April 17th 2015). Normally, when we say a study was preregistered, we mean pre data collection, not during.\nThe preregistered sample size, 120, is lower than the final S1 sample size (140, after exclusions). Of course, that raises the spectre of optional stopping, i.e., that the authors consciously or unconsciously made the data collection’s end contingent on the significance of the results. The authors write that they aimed for 200 participants in both S2 and S3 based on S1’s results. In this context, it is interesting that the sample size for S3 (the nonsignificant result) exceeded 200 (214 after exclusions) and the sample size for S2 (barely significant) fell short of it (188 after exclusions).\nThey preregistered that they would examine whether the correlation between risk and benefit perception in their inventories would change as a function of acetaminophen usage. To test this, they tested for moderator effect in a linear regression. This is not the same thing as testing for a difference in correlations, see this explainer.\nI emailed the last author some questions to ask for explanations of some of these deviations.\nDoing some further digging, I found that the first author’s master’s thesis reports a third study that tested whether these effects extended to ibuprofen (but not the study 2 reported here).\nTo my mind, the empirical evidence isn’t very strong and it’s unfortunate that the authors’ studies left open several researcher degrees of freedom, making it hard to take their p values at face value.\nI don’t know if this study is characteristic of the broader literature (see here for a critical take), but I think if I avoid paracetamol in the future, the liver damage will weigh heavier on my mind than the supposed psychological effects.\nIf someone wants to do further research on this, especially with undergraduates, I’d recommend asking them whether they drank the night before or are in other pain. We would not want to claim that paracetamol affects real world risk taking, when it really just affects whether we’re willing to hear a loud bang while hungover.\nFurther reading:\nHere’s a critical commentary on another acetaminophen study by Jonathon McPhetres: https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00705/full\nThe authors rebut some, but not the most damning criticisms, namely that the sample was small, the evidence weak and the study not preregistered: https://www.frontiersin.org/articles/10.3389/fpsyg.2020.02099/full\n\n\n\n",
    "preview": "posts/2020-09-23-hibar-effects-of-acetaminophen-on-risk-taking/pole_balloon.png",
    "last_modified": "2021-03-04T14:37:19+01:00",
    "input_file": {},
    "preview_width": 525,
    "preview_height": 391
  },
  {
    "path": "posts/2020-06-23-mis-allocated-scrutiny-in-science-a-quick-simulation/",
    "title": "Mis-allocated scrutiny: a quick simulation",
    "description": "Below I document my simulation code underlying an upcoming blog post at The 100% CI.",
    "author": [],
    "date": "2020-06-23",
    "categories": [
      "meta science",
      "reproducibility",
      "mistakes",
      "quick job"
    ],
    "contents": "\nYou can read the blog post about mis-allocated scrutiny at the 100% CI. This is just where I document the R code for my stupid little simulation.\n\n\n# load packages\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)\n\nParameters\n\n\nn_papers <- 10000\nn_journals <- 15\nn_revisions <- 3\nsubmission_limit <- 10\n\nSpawn papers and journals\n\n\n\nset.seed(1610)\njournals <- tibble(\n  journal = 1:n_journals,\n  threshold = rnorm(n_journals, sd = 1.8),\n  fame = threshold + 0.3 * rnorm(n_journals),\n  submitted = 0,\n  accepted = 0,\n  reviewer_count = case_when(\n    fame > 2 ~ 5,\n    fame > 1 ~ 4,\n    fame > 0 ~ 3,\n    fame > -1 ~ 2,\n    TRUE ~ 1\n  )\n) %>% arrange(desc(fame)) %>%\n  mutate(journal = 1:n_journals) %>% \n  as.data.frame()\n\npapers <- tibble(\n  paper = 1:n_papers,\n  fitness = rnorm(n_papers, sd = 1.5),\n  reviews = 0,\n  revisions = 0,\n  submissions = 0,\n  submission_sequence = \"\",\n  published = FALSE,\n  journal = NA_real_,\n  first_choice = NA_real_,\n  authors_own_assessment = NA_real_,\n  editors_assessment = NA_real_,\n  most_recent_assessment = NA_real_\n) %>% \n  as.data.frame()\n\nkable(table(journals$reviewer_count), \n      caption = \"How many journals recruit how many reviewers on average?\")\n\nTable 1: How many journals recruit how many reviewers on average?\n\n\nVar1\n\n\nFreq\n\n\n1\n\n\n2\n\n\n2\n\n\n4\n\n\n3\n\n\n3\n\n\n4\n\n\n3\n\n\n5\n\n\n3\n\n\ndecisions <- tibble(\n  did_not_submit = 0,\n  desk_rejection = 0,\n  reject_after_reviews = 0,\n  revise_and_resubmit = 0,\n  accepted = 0\n) %>% \n  as.data.frame()\n\n\nget_reviews <- function(fitness, n = 1) {\n  error <- 1/(sqrt(1+n))\n  sqrt((1 - error)) * fitness + (sqrt(error)) * rnorm(length(fitness))\n}\n\nMain simulation loop\n\n\n# library(profvis)\n# profvis({\nfor(p in 1:n_papers) {\n  papers[p, \"authors_own_assessment\"] <- 1 + get_reviews(papers[p, \"fitness\"], 1)\n  # submission loop\n  for(j in 1:n_journals) {\n    if(papers[p, \"authors_own_assessment\"] <\n       (journals[j, \"fame\"] - 1.8)) {\n      decisions$did_not_submit = decisions$did_not_submit + 1\n      # WOULD NOT EVEN SUBMIT\n    } else {\n      # SUBMIT\n      papers[p, \"submissions\"] <- papers[p, \"submissions\"] + 1\n      papers[p, \"submission_sequence\"] <- paste0(\n        papers[p, \"submission_sequence\"], j, \", \")\n      journals[j, \"submitted\"] = journals[j, \"submitted\"] + 1\n      if(is.na(papers[p, \"first_choice\"])) {\n        papers[p, \"first_choice\"] <- j\n      }\n\n      # EDITOR REVIEWS\n      papers[p, \"editors_assessment\"] <- get_reviews(papers[p, \"fitness\"], 1)\n      papers[p, \"reviews\"] <- papers[p, \"reviews\"] + 1\n\n      if(papers[p, \"editors_assessment\"] <\n         (journals[j, \"threshold\"] - 1.5)) {\n        # DECISION: DESK REJECTION\n        decisions$desk_rejection = decisions$desk_rejection + 1\n        papers[p, \"fitness\"] <- papers[p, \"fitness\"] + \n          0.05/papers[p, \"submissions\"]\n        papers[p, \"revisions\"] <- papers[p, \"revisions\"] + 1\n      } else {\n        # SENT FOR REVIEW\n        # revision loop\n        for(r in 1:n_revisions) {\n          papers[p, \"most_recent_assessment\"] <-\n            get_reviews(papers[p, \"fitness\"], journals[j, \"reviewer_count\"])\n\n          if(papers[p, \"most_recent_assessment\"] <\n                    (journals[j, \"threshold\"] - 0.5)) {\n            # DECISION: REJECT AFTER REVIEWS\n            decisions$reject_after_reviews = decisions$reject_after_reviews + 1\n            papers[p, \"reviews\"] <- papers[p, \"reviews\"] + journals[j, \"reviewer_count\"]\n            papers[p, \"revisions\"] <- papers[p, \"revisions\"] + 1\n            # diminishing returns\n            papers[p, \"fitness\"] <- papers[p, \"fitness\"] + \n              0.1/papers[p, \"submissions\"]\n            break\n          } else if(papers[p, \"most_recent_assessment\"] <\n                    (journals[j, \"threshold\"])) {\n            # DECISION: MAJOR REVISION/R&R\n            decisions$revise_and_resubmit = decisions$revise_and_resubmit + 1\n            papers[p, \"reviews\"] <- papers[p, \"reviews\"] + journals[j, \"reviewer_count\"]\n            papers[p, \"revisions\"] <- papers[p, \"revisions\"] + 1\n            papers[p, \"fitness\"] <- papers[p, \"fitness\"] + \n              0.3/papers[p, \"submissions\"]\n          } else if(papers[p, \"most_recent_assessment\"] >=\n             journals[j, \"threshold\"]) {\n            # DECISION: ACCEPTED/MINOR REVISION\n            decisions$accepted = decisions$accepted + 1\n            papers[p, \"reviews\"] <- papers[p, \"reviews\"] + journals[j, \"reviewer_count\"]\n            papers[p, \"published\"] <- TRUE\n            journals[j, \"accepted\"] = journals[j, \"accepted\"] + 1\n            papers[p, \"journal\"] <- journals[j, \"journal\"]\n            break # acceptance\n          }\n        } # end revision loop\n      }\n    }\n    if (papers[p, \"published\"]) {\n      break # done\n    } else if (papers[p, \"submissions\"] >= submission_limit) {\n      break # give up\n    }\n  } # end journal loop\n} # end paper loop\n# })\n\nInspect results\n\n\n\ntheme_set(theme_minimal())\noptions(digits = 2)\nkable(decisions, \n      caption = \"How common are certain editorial decisions?\")\n\nTable 2: How common are certain editorial decisions?\n\n\ndid_not_submit\n\n\ndesk_rejection\n\n\nreject_after_reviews\n\n\nrevise_and_resubmit\n\n\naccepted\n\n\n17244\n\n\n31843\n\n\n26577\n\n\n7195\n\n\n8729\n\n\nkable(table(papers$submissions), \n      caption = \"How often do papers get submitted?\")\n\nTable 2: How often do papers get submitted?\n\n\nVar1\n\n\nFreq\n\n\n1\n\n\n230\n\n\n2\n\n\n288\n\n\n3\n\n\n695\n\n\n4\n\n\n715\n\n\n5\n\n\n1642\n\n\n6\n\n\n887\n\n\n7\n\n\n1408\n\n\n8\n\n\n887\n\n\n9\n\n\n1261\n\n\n10\n\n\n1987\n\n\nkable(table(papers$published, exclude=NULL), \n      caption = \"How many do not end up published, i.e. authors give up after 8 tries?\")\n\nTable 2: How many do not end up published, i.e. authors give up after 8 tries?\n\n\nVar1\n\n\nFreq\n\n\nFALSE\n\n\n1271\n\n\nTRUE\n\n\n8729\n\n\npapers_in_journals <- papers %>% left_join(journals)\npapers_in_journals %>% select(fitness, fame, submissions, reviews) %>% \n  cor(use = 'pairwise') %>% round(2) %>% \n  kable(caption = \"How do paper fitness, journal fame, paper's number of submissions and paper's accumulated number of reviews intercorrelate?\")\n\nTable 2: How do paper fitness, journal fame, paper’s number of submissions and paper’s accumulated number of reviews intercorrelate?\n\n\n\n\nfitness\n\n\nfame\n\n\nsubmissions\n\n\nreviews\n\n\nfitness\n\n\n1.00\n\n\n0.89\n\n\n-0.58\n\n\n0.02\n\n\nfame\n\n\n0.89\n\n\n1.00\n\n\n-0.59\n\n\n-0.17\n\n\nsubmissions\n\n\n-0.58\n\n\n-0.59\n\n\n1.00\n\n\n0.55\n\n\nreviews\n\n\n0.02\n\n\n-0.17\n\n\n0.55\n\n\n1.00\n\n\nggplot(papers_in_journals, aes(journal, reviews)) +\n  geom_jitter(alpha = 0.2) +\n  geom_pointrange(stat = 'summary', color = \"blue\") +\n  ggtitle(\"Mis-allocated scrutiny\", subtitle = \"Simulated data\") +\n  xlab(\"Journal rank\") +\n  ylab(\"Accumulated reviews\")\n\n\njournals %>% mutate(acceptance_rate = round(accepted/submitted,2)) %>% \n  kable(caption = \"How many papers are submitted to each journal and how many are accepted?\",\n        digits = 1)\n\nTable 2: How many papers are submitted to each journal and how many are accepted?\n\n\njournal\n\n\nthreshold\n\n\nfame\n\n\nsubmitted\n\n\naccepted\n\n\nreviewer_count\n\n\nacceptance_rate\n\n\n1\n\n\n2.9\n\n\n2.8\n\n\n4891\n\n\n132\n\n\n5\n\n\n0.0\n\n\n2\n\n\n2.8\n\n\n2.7\n\n\n5358\n\n\n112\n\n\n5\n\n\n0.0\n\n\n3\n\n\n2.2\n\n\n2.1\n\n\n7112\n\n\n407\n\n\n5\n\n\n0.1\n\n\n4\n\n\n2.0\n\n\n1.6\n\n\n7856\n\n\n329\n\n\n4\n\n\n0.0\n\n\n5\n\n\n1.2\n\n\n1.5\n\n\n7651\n\n\n1459\n\n\n4\n\n\n0.2\n\n\n6\n\n\n1.4\n\n\n1.2\n\n\n6723\n\n\n377\n\n\n4\n\n\n0.1\n\n\n7\n\n\n0.8\n\n\n0.9\n\n\n6704\n\n\n1117\n\n\n3\n\n\n0.2\n\n\n8\n\n\n0.8\n\n\n0.7\n\n\n5681\n\n\n600\n\n\n3\n\n\n0.1\n\n\n9\n\n\n0.1\n\n\n0.4\n\n\n5260\n\n\n1620\n\n\n3\n\n\n0.3\n\n\n10\n\n\n0.4\n\n\n-0.1\n\n\n3772\n\n\n512\n\n\n2\n\n\n0.1\n\n\n11\n\n\n0.3\n\n\n-0.2\n\n\n2653\n\n\n309\n\n\n2\n\n\n0.1\n\n\n12\n\n\n-0.5\n\n\n-0.3\n\n\n2205\n\n\n835\n\n\n2\n\n\n0.4\n\n\n13\n\n\n-0.9\n\n\n-0.6\n\n\n1067\n\n\n426\n\n\n2\n\n\n0.4\n\n\n14\n\n\n-2.6\n\n\n-2.1\n\n\n498\n\n\n474\n\n\n1\n\n\n0.9\n\n\n15\n\n\n-2.2\n\n\n-2.2\n\n\n23\n\n\n20\n\n\n1\n\n\n0.9\n\n\n\n\n",
    "preview": "posts/2020-06-23-mis-allocated-scrutiny-in-science-a-quick-simulation/mis-allocated-scrutiny-in-science-a-quick-simulation_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2020-06-24T16:37:58+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-09-03-hibar-sexdifferentiated-changes-in-sexual-desire-predict-marital-dissatisfaction/",
    "title": "HIBAR: Sex‑Differentiated Changes in Sexual Desire Predict Marital Dissatisfaction",
    "description": "Had I Been a Reviewer. Actually, I was a reviewer. Huh! So, how did this turn out?",
    "author": [
      {
        "name": "Ruben C. Arslan",
        "url": "https://rubenarslan.github.io"
      }
    ],
    "date": "2019-09-03",
    "categories": [
      "HIBAR",
      "post-publication review",
      "sexual desire",
      "marriage",
      "relationships"
    ],
    "contents": "\nMcNulty, Maxwell, Meltzer, & Baumeister make use of two cohorts of newlyweds to find out whether discrepancies in sexual desire contribute to reducing marital satisfaction. The study is based on the same data as McNulty et al. 2016 and tries to answer a very similar question. The only difference is that here we are looking at sexual desire (which was measured less often) rather than sexual satisfaction and frequency as the predictor, and there’s an added consideration of childbirth and stress as mediating factors. I reviewed a previous version of this manuscript at another journal, so this blog post is an edited version of that review minus the points the authors addressed1.\nIn all, I think the results are plausible and the data are rich, but I wanted to think through some important limitations of their data (some of them discussed at length, some less so) to figure out how I think about the results.\n\nThe Authors’ Abstract\n\nSex is critical to marriage. Yet, there are several reasons to expect spouses to experience declines in the desire for sex over time, and the rates of any declines in sexual desire may differ for men and women. We used two multi-wave, longitudinal studies to test whether male and female members of newlywed couples experienced different rates of change in sexual desire, whether any such changes were accentuated by childbirth, and whether any such changes had implications for marital satisfaction. In both studies, spouses provided multiple reports of sexual desire, marital satisfaction, and childbirth. Results demonstrated that women’s sexual desire declined more steeply over time than did men’s sexual desire, which did not decline on average. Further, childbirth accentuated this sex difference by partially, though not completely, accounting for declines in women’s sexual desire but not men’s. Finally, declines in women’s but not men’s sexual desire predicted declines in both partners’ marital satisfaction. These effects held controlling depressive symptoms and stress, including stress from parenthood. The current findings offer novel longitudinal evidence for sex-differentiated changes in sexual desire and therefore suggest an important source of marital discord.\nThe three key limitations:\n\n\n\nFigure 1: A closer look. From the Internet Archive Book Images\n\n\n\nData start at marriage. At this point most relationship will have been going on for a (variable) while.\nNo data on hormones (age trends, menstrual cycle change), hormonal contraception, pregnancy, and breast feeding.\nSystematic attrition. Given that the goal of the study is to predict marital dissatisfaction, and that it followed newlyweds for 4.5 years, it baffles me that the authors do not discuss what happened to marriages that ended in divorce and how that relates to dropout (which was substantial).\nRelationships don’t start with marriage anymore\nOne key limitation is not addressed by design. Many couples never marry, and most couples don’t marry right after meeting. We may be looking at a very heterogeneous group here with respect to relationship duration. Why is that important? For example, couples who marry after having been together for longer may be more likely to have children. We would then spuriously conclude effects of childbirth that are in reality driven by pre-existing differences in relationship duration. The authors did not share data on relationship duration preceding marriage. Their data on newlyweds are obviously valuable, but for answering the specific research question posed, I’d hazard starting with unmarried couples would give us clearer answers.\nHormones\n\n\n\nFigure 2: Mother and Baby. From the book Woman in Girlhood, Wifehood, Motherhood in the Internet Archive Book Images\n\n\n\nAt the end, the authors say that hormonal fluctuations may be proximal mechanisms by which desire changes.\nYet, they do not discuss menstrual cycles, hormonal contraception, pregnancy and breastfeeding. All of these entail hormonal changes. Let’s think this through: If newlywed women are more likely to be already pregnant at the first timepoint, changes in sexual desire after birth might rather reflect a shift from pregnancy back to breastfeeding and/or regularly ovulating or hormonal contraception.2\nHormonal contraception causes small decreases in sexual desire on average. If newlywed women are more likely to have gone off the pill, they may temporarily have higher sexual desire in the first wave, and then decrease again after a return to hormonal contraception after birth.\nMore mechanistically, given that we found that women experiences peaks in sexual desire before ovulation, it may have been more likely that women were asssessed around ovulation during the first wave and less likely after childbirth (given that both breastfeeding and combined hormonal contraceptives can suppress ovulation).\n\n\n\nFigure 3: What comes before childbirth? From the book Woman in Girlhood, Wifehood, Motherhood in the Internet Archive Book Images\n\n\n\nFinally, with the age range of their sample, I don’t think anyone entered menopause during the study period, but I still would have liked to see their analyses adjusted for age.\nAre the effects reported by the authors small enough to be fully explained by these slightly roundabout explanations? In standard deviations, the effect is approximately a decrease of 0.18 across both studies, if I understood their table correctly. I don’t know how many women change their contraceptive method after marriage in their sample, nor do they report when and how many couples had children, so there are a lot of unknowns here.\nCombined with the problem that relationships don’t start at marriage, I do wonder if we were really shown evidence of a linear decline in sexual desire or whether we just started following couples right after a small uptick in sexual desire after the honeymoon (for hormonal and many other plausible reasons). I think getting this right makes a real difference to the counseling of couples.\nSystematic attrition\nI don’t really know what the right approach would be here, but certainly the authors should have mentioned how they dealt with missing data (FIML? Listwise deletion?) and I don’t think adjusting for the number of waves does much good. Presumably, dissatisfied couples are more likely to drop out, because they divorce or separate, so a censored model could be appropriate for the marital dissatisfaction analyses. In all, I think these problems could lead to an underestimation of effects, but I could be wrong about this because I don’t know the details of the model the authors fit.\nOverinterpretation\nIn all, the conclusions were tempered a lot between the version I saw and this one; in fact, I feel like I can see the signature of certain well-known reviewers in the limitations section. But these two slipped through:\n\nquality close relationships are a significant source of mental and physical health\n\nThis is an unsupported causal claim. The correlation may well be due to reverse causation or unobserved third variables. It would have be supported by something stronger than meta-analyses of correlations (e.g., propensity score matching). I’d add that divorce does not imply that people will necessarily end up lonely; they may end up happier. There’s still friends, new partners, or remarriage. To make the claim you’re trying to make, it would be more apt to cite negative consequences of divorce for well-being and mortality, rather than show positive correlates of marriage and assume that those whose marriages end will be alone.\n\ndesire for sex dwindles among newly married women but not men\n\nGiven the small effect size, the word “dwindles” is misleading.\nReproducibility\nGiven that the last two papers I did HIBARs on provided open data and one provided open analysis scripts, I have to point out that I really felt the lack of reproducibility for this paper. The data is not open and there is no syntax for the quite complex models. To my mind, this makes issues with things like missing data handling much more pressing, because I and others cannot easily re-analyse the data.\nThere is still at least one inconsistent p value in here (as identified by Statcheck.io), even after I recommended Statcheck in my review. I could not reproduce the authors measure of within-subject variability (presumably because I could not figure out how they went about it), although I could reproduce the directional result using my own approach.\nThere is no codebook for the two studies, so I have to rely on various descriptions of the data strewn across multiple papers to find out what they did and did not measure.\nThey report Cronbach’s α, but no retest reliabilities, nor reliabilities of change3; both of which of would be relevant to the question of whether they had adequate power. My admonition that they needed to provide more detail for their power analysis (e.g., predicted effect sizes) for it to make sense was apparently dealt with by omitting the power analysis entirely.\nFigure 1 just shows simple means with standard errors. A spaghetti plot (showing trajectories for all couples) or a smoothed spline over time superimposed on raw data would have done justice to the data.\n\nFurther, given the birth of children is consistently linked to marital satisfaction, a potential confound, we controlled marital satisfaction in these analyses as a time-varying covariate.\n\nMarital satisfaction is not just a potential confound, it’s also a potential outcome of lower sexual desire. Adjusting for it is hence not straightforward in a timeseries with long lags like this. I would like to see the results without this adjustment.\nThe authors use the infamous terms “marginally significant” and “trended toward significance”. As many others have pointed out, p values aren’t Geiger counters that tell you when you’re approaching the truth.\nConclusion\n\n\n\nFigure 4: This book on Girlhood, Wifehood, Motherhood seems a tad outdated, but at least it has sections on courtship before marriage, menstruation, ovulation, and menopause (and the benefits of bathing).\n\n\n\nWhile I found the paper interesting in general, I took less away from this than I could have. Of course, not everyone has a focus on hormones in their work, but given the limitations discussed above I cannot integrate their findings with what we already know about sexual desire and hormones. I hope future work revisits these issues.\n\nfor example, they pulled in the sexual frequency and satisfaction data here to link the two papers a bit more↩︎\nThe authors never discuss collecting information on whether women were pregnant when measured, although I assume they could back-calculate that from children’s birth dates (if those were collected) or approximately infer it depending on whether any children were born between waves (they definitely collected that).↩︎\nCronbach’s alpha is not a sufficient measure of reliability to report for longitudinal measures such as these. I recommend reporting multilevel generalizability (Shrout and Lane 2012), as implemented e.g. in the psych package by Revelle. Especially the coefficient reliability of change is interesting for the analyses reported. Also, simply reporting that coefficients were “more than .90” is too imprecise.↩︎\n",
    "preview": "posts/2019-09-03-hibar-sexdifferentiated-changes-in-sexual-desire-predict-marital-dissatisfaction/book_cover.png",
    "last_modified": "2021-03-05T12:50:29+01:00",
    "input_file": {},
    "preview_width": 673,
    "preview_height": 1019
  },
  {
    "path": "posts/2019-06-20-correction-to-arslan-et-al-2019-using-26000-diary-entries-to-show-ovulatory-changes-in-sexual-desire-and-behavior/",
    "title": "Correction to Arslan et al. (2019).",
    "description": "Some additional information on the correction notice for _Using 26,000 diary entries to show ovulatory changes in sexual desire and behavior_ that appeared today.",
    "author": [
      {
        "name": "Ruben C. Arslan",
        "url": "https://rubenarslan.github.io"
      }
    ],
    "date": "2019-06-20",
    "categories": [
      "correction",
      "ovulation"
    ],
    "contents": "\nTable of Contents\nCorrectionFigure 1 and case numbers\neffsize package bug\nProgramming error for a moderator variable\n\nOther post-publication feedback (not part of the correction)Figure 5\nFollowing the preregistration\n\nConclusion\nWe (me, Katharina Schilling, Tanja M. Gerlach, & Lars Penke) recently published a diary study on ovulatory changes in the Journal of Personality and Social Psychology.\nUnfortunately, we made a few mistakes in reporting the study. The correction appeared today in JPSP. Because corrections have to be quite short, we will use this blog post to give a little more detail.\nAccording to our assessment, the mistakes, although annoying and preventable, changed nothing substantive. I have taken to adding automated testing to my data cleaning code and instituted a bug bounty policy to reduce the odds of such errors in my future work.1\nAfter expanding on the correction, we will also respond to some criticisms that we do not think are errors in our work, but differences in interpretation.\n\n\n\nFigure 1: Several bugs in our code. From the Internet Archive Book Images\n\n\n\nCorrection\nFigure 1 and case numbers\nWe regret the following errors and inconsistencies in our published paper. Between our initial submission and our revision, we had made a small adjustment to the code for our exclusion criteria and neglected to update Figure 1 and Table 3 (because we did not notice that we had a few more participants and days). This led us to report an incorrect, lower number of total participants (1043 instead of 1054) for the robustness checks. The number of days were also off by a few hundred, as well as various sample means. The substantive results (model coefficients etc.) were reported correctly and with correct case numbers (in the online supplement).\nThe preregistered work is unaffected by this error. A corrected Figure 1 also shows two exclusion criteria (hypothesis guessing and long interruptions of the diary) that were mentioned on the supplementary website, but missing from Figure 1. A corrected figure can be found here and in the updated article.\neffsize package bug\nWe reported inflated effect sizes for the Hedges’ g differences between hormonal contraceptive users and non-users in Table 1. After re-analysing data for the correction, we suddenly got different effect sizes. It turned out there was a bug in the effsize package for Hedges’ g computation that had been fixed in a newer version.\nIn all, we reported larger effect size differences between our naturally cycling group and our hormonal contraception quasi-control group; they were more comparable than Table 1 made them seem.\nProgramming error for a moderator variable\nI made a programming error when aggregating the variable “partner’s attractiveness relative to self”. Specifically, I accidentally sorted values because of a typo in the data.table syntax. This led to nonsense values (women’s values were jumbled). A reader who re-analysed our data found it, for which are grateful. Fixing this error led to the following changes:\n\n\n\nFigure 2: We needed some help to get this one. From the Internet Archive Book Images\n\n\n\nIn the preregistered analyses, the moderation of fertile window effects on extra-pair desire and behaviour was no longer non-significant in the opposite direction of the prediction, but non-significant in the predicted direction (p = 0.23).\nIn the robustness analyses, the predicted interaction was significant for extra-pair desire and behaviour (p = 0.00565) and partner mate retention (p = 0.0014).\nOur preregistered tests, following the literature at the time, had not permitted slopes for menstruation and the fertile window to vary by woman, even though fitting a cross-level moderation essentially stipulates that varying slopes must exist (an internal conceptual inconsistency).\nModels with varying slopes indeed fit better for all outcomes. We reported robustness checks with varying slopes for all main effects, but we had not done so for our moderators tests, because we found no evidence of moderation and the check would have only made the test more conservative. Given that correcting the error led to a nominally significant result, we also tested a model, allowing for slopes to vary. In this model, the predicted interaction was non-significant for extra-pair desire (p = 0.085). The predicted interaction for partner mate retention in the robustness check would have been significant (p = 0.0072) according to our threshold of .01 for the preregistered tests, but still potentially consistent with sampling error given that 24 moderator effects had been tested (four moderators, three outcomes, two subsamples) were tested for essentially one hypothesis.\nThis programming error, though severe, did not affect the preregistered results. In our robustness checks, the error led to some changes in nominal significance, but the overall pattern still cannot be seen as evidence for the predicted moderation pattern.\nOther post-publication feedback (not part of the correction)\nFigure 5\nDan Engber helpfully pointed out that the caption for Figure 5 could have been clearer. The figure was intended to show differences in patterns across the cycle. To this end, we standardised differences within variables and hormonal contraceptive status (“within-subject change” in the figure caption). This focuses the eye on the differences in changes for HC users and non-users. In Figure 3, we also showed the mean differences. An alternative version of Figure 5, including mean differences between HC users and non-users, can be found online.\n\n\n\nFigure 3: These aren’t true bugs, but still good to discuss. From the Internet Archive Book Images\n\n\n\nFollowing the preregistration\nWe were criticised for not following our preregistration to the letter. It was our intention to be faithful to the preregistration as much as possible and transparent about the deviations that we considered reasonable and necessary. We think we succeeded in doing so and that problems raised by the critic are mainly problems of explicitness and interpretation.\nIt was our first preregistration (in 2014), we had no models for how to preregister correlational work with many simultaenous (but not all related) hypothesis tests. It was also our first menstrual cycle study and my first repeated measures study. For this reason, we relied on expert opinion to design, for example, our exclusion criteria.\nThis process led to a few suboptimalities (still an incomplete list, I am sure):\nOur exclusion criteria were overly strict and would have led to excluding most of the women for no good reason (according to our effect size estimates, excluded women were not more likely to be anovulatory).\nWe preregistered the use of windowed fertility predictors, which throw away most of the informative variation in fertility and reduce the number of usable days.\nWe preregistered no strategy to deal with multiple testing, although we had multiple outcomes (some of which were highly correlated).\nWe preregistered several moderators that were all designed to test the same hypothesis, instead of the strongest possible specification.\nWe did not preregister how we would aggregate some of the more complex items in the data.\nWe preregistered a scale optimisation algorithm based on Cronbach’s alpha, which is not the best way to estimate reliability for multilevel data like ours\nWe think we transparently reported how we chose to deal with these problems. We did not make any decisions to arrive at foregone conclusions; instead, we think we had good reasons for non-arbitrary decisions.\nOperationalisation of hypothesis 2.2\nThe reader alerted us that our hypothesis _H.2.2. Moderation or shift hypotheses: The ovulatory increase in women’s extra-pair desires and reported male mate retention behavior is strongest (and the in-pair desire increase is weakest) for women who perceive their partners as low in sexual attractiveness relative to long-term partner attractiveness. could also be interpreted to mean a different statistical model than the one we fitted.\nWe interpreted it as meaning that women who have a partner who is high in long-term attractiveness but low in short-term attractiveness would show ovulatory increases in extra-pair desire, whereas all other women would not. Basically, women who have a partner who is a “provider” but does not have “good genes” would be interested in extra-pair men; other women would not be.\nWe saw this in contrast to the simpler model, which we also fit, with only short-term attractiveness as the moderator. The reader interpreted it as meaning that we should adjust for long-term attractiveness to remove a “positivity bias” and test only the interaction between the fertile window and short-term attractiveness. Previous work had sometimes tested such a model and sometimes a difference score.\nAlthough we reported them, we recommend not interpreting difference scores such as this (or the relative attractiveness variable above) in isolation, because they assume that women with partners who are attractive for both long- and short-term relationships behave the same way as women with partners who are not attractive for either long- or short-term relationships. We think this is not what the verbally specified theory predicts, but of course verbal specifications can be debated because they often leave some room for ambiguity.\nIn our preregistered analyses, none of these alternative specifications would have yielded a significant effect, except one significant result in the opposite direction for in-pair desire. However, in our robustness checks, the interaction for this alternative specification would have been significant (p = 0.006). Again, allowing for slopes to vary rendered this interaction nonsignificant at .01 (p = 0.045).\nOverall, as we had already stressed in our discussion, it would be premature to conclude an absence of moderation: confidence intervals were too wide to rule out potentially relevant effect sizes and patterns were often in the predicted form for extra-pair desire (but not for in-pair desire). But neither should these models, which were suggested after seeing the results for other models, be seen as evidence for moderation, given the number of tests performed. If a prediction from the literature is supported in preregistered tests, checks like ours can show robustness to relaxing or tightening assumptions. The evidence for the predicted moderators is clearly not robust in our data. More data is needed to reach adequate power for more informative tests of moderation patterns, and is indeed forthcoming. Maybe more importantly, theories need to be clearer, so that they can specify severe tests. We found this difficult to do at the time of planning the study.\nOperationalisation of preregistration regarding hormonal contraceptive users\nLastly, we did not preregister that we would use hormonal contraception (HC) users as a quasi-control group for the naturally cycling group. Consistent with this, our preregistered tests compared fertile window changes with zero, not with the baseline change for HC users. However, we reported the latter comparison as well, in the preregistered analysis section. We mainly did this to show that despite the fact that we only had an ad-hoc strategy to deal with multiple testing, we never found an ovulatory change among hormonal contraceptive users (for whom ovulation is suppressed). We thought reporting the quasi-control group was one way to show that our ad-hoc strategy was effective.\nWe hope these additional tests, which were in fact always consistent with our preregistered tests, did not lead to confusion regarding our preregistration. The choice of additionally presenting these analyses did not affect our conclusions and was not made conditional on the results.\nConclusion\nWe are glad the paper led to animated post-publication discussion and are grateful to all who pointed out errors or ways the paper could be improved. We will implement the suggestions and lessons when publishing the results from the second, larger cycle study we conducted after this one.\nIn all, the conclusions of our paper remain the same, although quite a few numbers changed a little. The replicability of the ovulatory change literature still seems decidedly mixed. Our work was not (and was never meant to be) the last word on moderators of cycle changes.\nI also stopped using data.table in favour of dplyr which has a more explicit syntax.↩\n",
    "preview": "https://live.staticflickr.com/3862/14751616975_c06d479338_o_d.jpg",
    "last_modified": "2020-06-06T06:58:54+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-06-19-tympanic-temperature-and-social-connectedness/",
    "title": "HIBAR: Tympanic temperature and social connectedness",
    "description": "Re-examining a reported association between physical temperature and social connectedness.",
    "author": [
      {
        "name": "Ruben C. Arslan",
        "url": "https://rubenarslan.github.io"
      }
    ],
    "date": "2019-06-19",
    "categories": [
      "HIBAR",
      "re-analysis",
      "temperature",
      "experience sampling"
    ],
    "contents": "\nThere was recently a bit of a tussle in the literature about the question whether physical warmth prime social social warmth. A nonreplication of Williams & Bargh (2008) by Chabris, Heck, Mandart, Benjamin, & Simons (2018) did not support that holding a hot coffee cup would make people judge others as warmer, among other things. Bargh & Melnikoff (2018) responded and pointed out that the larger question about the connection between social and physical warmth no longer rested on their initial small study.\nAmong other studies, they cited the Human Penguin Project and a study by Inagaki & Human (2019) in which daily tympanic (in the ear) temperature readings where correlated with feelings of social connectedness in an experience sampling design.\n\n\n\nFigure 1: Human penguins? From the Internet Archive Book Images\n\n\n\nI decided to look the experience sampling study up, because I wanted to know how the authors had dealt with the well-known diurnal changes in body temperature and the circamensal rhythm, in which naturally cycling1 women experience increases in body temperature after ovulation.\nThe authors seemed to be aware of these issues (e.g., they excluded pregnant women and women who used hormonal birth control), but took a fairly strictly correlational approach to the data. The literature discussed was all about high temperatures increasing social warmth though. However, their design was used to remove between-person confounds (such as age and pregnancy), so they do seem to want to lay the groundwork for causal claims.\n\n\n\nFigure 2: Diurnal variation in temperature. From the Wikipedia\n\n\n\nThe authors analyses left me wishing for more though. I thought I could potentially exclude a confound of post-ovulatory change in temperatures by looking at within-day variation and that I could maybe adjust for time of day to rule out a common cause confounder of both temperature and feelings of connectedness. The authors simply wrote “there are no hypothesized effects related to time of day in the current study.”\nTo my great pleasure, I found the authors had uploaded a processed subset of their data to the Open Science Framework.\nAs far as I can tell from the also provided R source code, this is the final dataset used for analysis (I can reproduce their Table 1).\n\n\n\nThere are 6633 observations in the dataset from 212 people.\nThe authors had participants measure their temperature twice 3 minutes apart, but did not respond the correlation between the two measures. I graphed it.\n\n\n\nFigure 3: Two temperature readings from the right ear 3 minutes apart.\n\n\n\nThere were three surprising things about this graph for me.\nIt’s quite noisy (r=0.88)—as a psychologist myself I always kind of expect physiological measures to have better reliability (even though I know that need not be the case).\nIt is bunchy. Either the thermometers reported readings only to a tenth of a degree or people only reported tenths or the authors rounded the data. This seems like a low standard for accuracy for a scientific study (for comparison, women who measure basal body temperatures for contraception usually track hundredths of a degree).\nSome people had temperatures which should have made them too comatose to enter them in a survey. The minimum value recorded for the average was 30.05, the maximum was 38.85. The authors reported only excluding “Two participants with tympanic readings that were consistently outside the normotensive range and were therefore suspected to be ill were excluded from final analyses, leaving a final sample of 211 participants.”. However, their abstract says “in the nonfebrile range”. But 38.8 °C is in the febrile range. And 30 °C is not febrile, but should be excluded as an outlier because of likely measurement error/participant being a zombie.\nExclusions\nI restricted the range of the data to what Wikipedia calls normal range (36.5–37.5 °C) plus/minus 0.2 for a rough standard error in measurement for each measurement.\n\n\n\n740 measurements were excluded. The correlation changed a little (r=0.83).\n\n\n\nFigure 4: Restricted data. Two temperature readings from the right ear 3 minutes apart.\n\n\n\nI also wondered about the diurnal variation. The authors did not share time of day or time since waking, but they shared the number of the within-day assessment.\n\n\n\nFigure 5: Diurnal variation. Means + SEs.\n\n\n\nRe-analysis results\n\nMultilevel regression results\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: tempavg ~ (1 | ID) + (1 | ID:Day)\n   Data: temp\n\nREML criterion at convergence: 213.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.5004 -0.6159  0.0072  0.6322  3.7343 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID:Day   (Intercept) 0.005167 0.07188 \n ID       (Intercept) 0.030825 0.17557 \n Residual             0.050922 0.22566 \nNumber of obs: 5893, groups:  ID:Day, 1463; ID, 212\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(>|t|)    \n(Intercept)  36.9041     0.0126 208.2758    2928   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: connected ~ temp_b + temp_w + (1 | ID) + (1 | ID:Day)\n   Data: temp\n\nREML criterion at convergence: 17416.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.3529 -0.4451  0.0970  0.5186  4.3220 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID:Day   (Intercept) 0.1351   0.3676  \n ID       (Intercept) 0.9166   0.9574  \n Residual             0.9027   0.9501  \nNumber of obs: 5893, groups:  ID:Day, 1463; ID, 212\n\nFixed effects:\n              Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)   28.38771   13.60878  212.19305   2.086   0.0382 *  \ntemp_b        -0.62335    0.36876  212.17414  -1.690   0.0924 .  \ntemp_w         0.24957    0.05661 5665.95669   4.409 1.06e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr) temp_b\ntemp_b -1.000       \ntemp_w  0.000  0.000\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nconnected ~ temp_b + temp_b_day + temp_w_day + (1 | ID) + (1 |  \n    ID:Day)\n   Data: temp\n\nREML criterion at convergence: 17418.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.3458 -0.4433  0.0959  0.5169  4.3173 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID:Day   (Intercept) 0.1354   0.3679  \n ID       (Intercept) 0.9165   0.9573  \n Residual             0.9025   0.9500  \nNumber of obs: 5893, groups:  ID:Day, 1463; ID, 212\n\nFixed effects:\n              Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)   28.38532   13.60835  212.21364   2.086   0.0382 *  \ntemp_b        -0.77081    0.38954  263.68655  -1.979   0.0489 *  \ntemp_b_day     0.14752    0.12547 1339.02065   1.176   0.2399    \ntemp_w_day     0.27567    0.06344 4458.60405   4.346 1.42e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n           (Intr) temp_b tmp_b_\ntemp_b     -0.947              \ntemp_b_day  0.000 -0.322       \ntemp_w_day  0.000  0.000  0.000\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nconnected ~ temp_b + temp_b_day + temp_w_day + factor(WithinDayAssessment) +  \n    (1 | ID) + (1 | ID:Day)\n   Data: temp\n\nREML criterion at convergence: 17426.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.4094 -0.4465  0.0991  0.5113  4.2991 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID:Day   (Intercept) 0.1349   0.3673  \n ID       (Intercept) 0.9168   0.9575  \n Residual             0.9022   0.9498  \nNumber of obs: 5893, groups:  ID:Day, 1463; ID, 212\n\nFixed effects:\n                               Estimate Std. Error         df t value\n(Intercept)                    28.21859   13.61064  212.17769   2.073\ntemp_b                         -0.76601    0.38959  263.59833  -1.966\ntemp_b_day                      0.14594    0.12541 1340.27676   1.164\ntemp_w_day                      0.25343    0.06430 4459.65903   3.941\nfactor(WithinDayAssessment)2    0.02306    0.03955 4706.24655   0.583\nfactor(WithinDayAssessment)3    0.06896    0.04001 4716.92339   1.724\nfactor(WithinDayAssessment)4    0.11034    0.04016 4718.30844   2.748\nfactor(WithinDayAssessment)5    0.04046    0.04084 4753.66239   0.991\nfactor(WithinDayAssessment)6   -0.05942    0.14702 5319.09589  -0.404\nfactor(WithinDayAssessment)7    0.01317    0.57594 5214.95864   0.023\nfactor(WithinDayAssessment)8   -0.92273    0.99542 5168.25697  -0.927\n                             Pr(>|t|)    \n(Intercept)                   0.03935 *  \ntemp_b                        0.05032 .  \ntemp_b_day                    0.24475    \ntemp_w_day                   8.23e-05 ***\nfactor(WithinDayAssessment)2  0.55988    \nfactor(WithinDayAssessment)3  0.08483 .  \nfactor(WithinDayAssessment)4  0.00602 ** \nfactor(WithinDayAssessment)5  0.32183    \nfactor(WithinDayAssessment)6  0.68609    \nfactor(WithinDayAssessment)7  0.98175    \nfactor(WithinDayAssessment)8  0.35399    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) temp_b tmp_b_ tmp_w_ f(WDA)2 f(WDA)3 f(WDA)4\ntemp_b      -0.947                                             \ntemp_b_day   0.000 -0.322                                      \ntemp_w_day   0.001 -0.001  0.001                               \nfctr(WtDA)2 -0.005  0.005 -0.004 -0.100                        \nfctr(WtDA)3 -0.004  0.002  0.000 -0.140  0.521                 \nfctr(WtDA)4 -0.006  0.005 -0.004 -0.115  0.516   0.516         \nfctr(WtDA)5 -0.002 -0.003  0.011 -0.030  0.499   0.497   0.494 \nfctr(WtDA)6 -0.002  0.005 -0.010 -0.017  0.141   0.141   0.142 \nfctr(WtDA)7  0.000 -0.003  0.008 -0.005  0.035   0.035   0.034 \nfctr(WtDA)8  0.000 -0.004  0.011 -0.013  0.021   0.022   0.019 \n            f(WDA)5 f(WDA)6 f(WDA)7\ntemp_b                             \ntemp_b_day                         \ntemp_w_day                         \nfctr(WtDA)2                        \nfctr(WtDA)3                        \nfctr(WtDA)4                        \nfctr(WtDA)5                        \nfctr(WtDA)6  0.139                 \nfctr(WtDA)7  0.034   0.032         \nfctr(WtDA)8  0.020   0.019   0.056 \n\nInterestingly, the effects actually get much stronger when excluding these measurements. Their within-person change score for temperature has an effect size of .13, after these exclusions, it’s .24. I also tried centering the temperature by day (to get a within-day change measure that should be independent of ovulatory change and other daily change), and the estimate was .27. I also adjusted for within-day assessment, this did not change the within-day temperature effect much.\nStill, a plot showed that the effect may still be driven by values which are more than 0.5 degrees away from the person mean. This stuff makes me worry about correlated measurement error.\n\n\n\nFigure 6: Values that are more than .5 degrees away from the person mean, drive the association.\n\n\n\nReplication\nI tried replicating the association with another, larger dataset that I have access to with daily basal body temperature. Results descriptively went in the opposite direction for outcomes like feeling sociable or supportive (non-sig. neg. effects of within-person temperature), or withdrawn (positive effects).\nSummary\nOutlying values in the data should have been excluded. The article should probably be corrected. I can replicate the effects based on their own data, associations get even stronger. I cannot shake the feeling that the authors did not do a good enough job to rule out “boring” common cause confounders like time of day or physical activity. The authors stuck to presenting the data as correlations, but people only care about the data because of the implied causal path temperature -> connectedness. If it was people exercising in team sports -> connectedness and exercise -> temperature, few readers would care.\n\n\n\nFigure 7: Penguins are simply way classier than us. From Wikipedia\n\n\n\n\nnot using hormonal contraceptives, premenopausal, not pregnant or breastfeeding↩︎\n",
    "preview": "posts/2019-06-19-tympanic-temperature-and-social-connectedness/tympanic-temperature-and-social-connectedness_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-03-04T14:37:54+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-06-14-hibar-how-methods-and-practices-changed-after-the-replication-crisis-in-social-psychology/",
    "title": "HIBAR: How methods and practices changed after the replication crisis in social psychology",
    "description": "Had I Been a Reviewer. A post-publication peer review with some added figures.",
    "author": [
      {
        "name": "Ruben C. Arslan",
        "url": "https://rubenarslan.github.io"
      }
    ],
    "date": "2019-06-14",
    "categories": [
      "HIBAR",
      "re-analysis",
      "post-publication review",
      "meta science",
      "bibliometrics"
    ],
    "contents": "\nEdit: Authors’ response\nThe authors of the study have responded to the points raised here. You can read their response here. We had a little bit of follow-up discussion on Twitter. In all, I found this was a productive exchange and I’m happy the authors took the time to respond in such detail.\n\n\n\n\n\n\nFigure 1: Change in proportion of studies that are online over time by journal. Bootstrapped means and 95% CIs.\n\n\n\nSassenberg and Ditrich published a paper in Advances in Methods and Practices in Psychological Science in May. It’s on a topic I care about deeply, namely the impact of changes in academic culture on research quality. Specifically, the authors were interested whether social psychologists have responded to the replication crisis in their subdiscipline and subsequent cries for higher methodological rigour (especially higher statistical power) by switching to less effortful methods of data collection (self-report).\nI was not a reviewer of the paper, but given that I’ve already re-analyzed the N-pact paper, it felt only appropriate to do the same with this paper. I decided to do this post in the format of HIBAR.1 I think it’s an important topic and the authors collected valuable data, which surely took a lot of coding effort. The authors, regrettably, did not share any figures for their data. Their findings, which are easily summarised graphically, may therefore become less widely known. So, I made some figures from the open data (above and below).\nI frequently hear arguments of the form “calls for rigour in some practices will just lead to less rigour in other areas”, “labour-intensive research will go extinct if higher sample sizes are required” from senior researchers. These arguments are often used to urge caution in response to calls for reform. They may end up being interpreted as advocacy for the status quo.\nEmpirical evidence that given consistent publication pressure, researchers urged to increase their sample sizes will do less rigorous research in other ways is thus worrying.2\nMajor points\nOmitted variables\nThe authors mention a number of coded variables analysed that they say are not relevant for the questions addressed here. I disagree with this assessment. The broader question is whether low-effort methods of data collection such as self-report and online research have replaced high-effort methods. However, the differences in effort for running an effective online study (especially when first learning about online research) versus running a lab study on undergraduates are smaller and more arguable than the differences in effort for running a study online vs. on a population-representative sample or a community sample. The same holds true for self-report and reaction time measures (both not very high-effort) versus e.g. genetic, or endocrinological data, intelligence testing, or observer coding. So, as a reader I would like to know whether self-report and online research replaced other low-effort or high-effort modes of data collection. The researchers disclose that they also coded study design, exclusion of participants, student vs. nonstudent sample, mediation analysis, behavioral measures, response times, memory measures, performance measures, coding of written materials, and physiological measures. Given that the authors give no strong rationale for excluding this data from analyses and given the absence of a preregistration, the omission of these data seems unjustifiable and should be rectified to give readers a fuller picture.\nWasteful sample size transformation\nThe authors winsorize sample size. I do not think this is an appropriate treatment of this continuous variable. Yes, there are outlying values, but these are presumably not the result of miscoding, but the result of massive heterogeneity. A study of 10000 people really is a hundred times as big as a study of 100 people. There are better ways to deal with non-normally distributed statistics without discarding information. As an example, I chose to plot the logarithmised sample sizes below.\nFigures\nFigures are essential for communicating data and effect sizes effectively. I took the liberty of generating a few figures below. What I would like to see in addition is figures on the omitted variables. Further inferential testing is, in my opinion, not necessary. We are interested in some measure of overall rigour, but there will be no agreeable way to aggregate the different variables into one measure of rigour, so the best we can do is present the data and discuss it.\n\n\n\nFigure 2: Change in proportion of studies that use only self-report over time by journal. Bootstrapped means and 95% CIs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Change in median sample size over time by journal\n\n\n\n\n\n\nFigure 4: Change in sample size over time by journal. Sample sizes were logarithmised with base 10. Bootstrapped means and 95% CIs.\n\n\n\n\n\n\nFigure 5: Change in number of studies per article over time by journal. Bootstrapped means and 95% CIs.\n\n\n\nDid submitters or editors and reviewers change their behaviour?\nThe authors frame their results as evidence of changes in the behaviour of research teams. Arguably, science reform is mediated in many places by editors and reviewers. Maybe just as many small-sample studies are being done, but they do not get published in these journals. To begin to answer this question, it is interesting to see (emphasis on see, below) how the different outcome measures associate. The authors should discuss this question more explicitly and discuss the correlations. Can they be used to make the case that researchers in the same team trade off sample size for self-report? Or are we seeing an increase in self-report measures only among online (Mturk?) researchers, while other researchers independently increase their sample sizes without changing their methods?\n\n\n\nFigure 6: Sample size by self-report. Bootstrapped means and 95% CIs.\n\n\n\n\n\n\nFigure 7: Sample size by online. Bootstrapped means and 95% CIs.\n\n\n\nBibliographic data\nThe published data does not contain bibliographic information on the papers. This makes it impossible to check the accuracy of codings, to re-use and extend the data (by, for example, looking up DOIs and fetching citation counts in a few years). If the authors did this to preserve researcher anonymity, I want to strongly argue that this is misguided when it comes to published literature.\nCausal inference\nThe researchers give the standard disclaimer that they have examined only correlations. This would become more vivid if they discussed other known time trends that could confound their results, such as the rise of Mturk. They could also discuss ways in which a stronger causal inference would be possible. Are there subdisciplines akin to social psychology in their use of small samples that were not hit by the reform calls as early (developmental, evolutionary?) which we might examine additionally? Is the question important enough that we should advocate for large-scale experimentation with journal policies?\nMinor points\nThe authors say that their analysis explains 31% of the variance in the online variable. This is a dichotomous variable, so you cannot report an R2. Same for self-report. Please run logistic regressions and report pseudo R2s (if you must).\nI took the liberty of generating a human- and machine-readable codebook for the data, see below.\nSummary\nGiven the presented data, I am not convinced that the researchers have shown that calls for increased rigour in terms of sample size have led to decreased rigour in measurement. To get a fuller sense of valid information, it would also have been interesting to look at other measures of rigour, such as the number of items, reliability, and whether the measure was ad-hoc. This cannot be done with the existing data. What the authors can do, is to fully present the data they have collected, including data on other measurement methods. As a final note, I am not aware that many voices in the reform movement called for more studies per article, yet we see this trend. This might serve as a vivid example that there are always many things going on simultaneously when just examining trends over time.\nCodebook\n\n\n\n\n\n\n\n\nMetadata\nDescription\n\nDataset name: Research in social psychology changed\n\n\nThe dataset has N=1300 rows and 6 columns. 458 rows have no missing values on any column.\n\n\nMetadata for search engines\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate published: 2021-03-04\n\n\n\n\n\n\nx\npaperID\nJournal\nJahr\nStudynum\nSample\nonline\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodebook table\n\n\n\n\n\n\n\n\n\n\nJSON-LD metadata\nThe following JSON-LD can be found by search engines, if you share this codebook publicly on the web.\n{\n  \"name\": \"Research in social psychology changed\",\n  \"datePublished\": \"2021-03-04\",\n  \"description\": \"The dataset has N=1300 rows and 6 columns.\\n458 rows have no missing values on any column.\\n\\n\\n## Table of variables\\nThis table contains variable names, labels, and number of missing values.\\nSee the complete codebook for more.\\n\\n|name     |label                                  | n_missing|\\n|:--------|:--------------------------------------|---------:|\\n|paperID  |unique paper identifier                |         0|\\n|Journal  |journal in which article was published |         0|\\n|Jahr     |year of publication                    |         0|\\n|Studynum |number of studies per paper            |       842|\\n|Sample   |sample size                            |         0|\\n|online   |online data connection                 |         0|\\n\\n### Note\\nThis dataset was automatically described using the [codebook R package](https://rubenarslan.github.io/codebook/) (version 0.9.2).\",\n  \"keywords\": [\"paperID\", \"Journal\", \"Jahr\", \"Studynum\", \"Sample\", \"online\"],\n  \"@context\": \"http://schema.org/\",\n  \"@type\": \"Dataset\",\n  \"variableMeasured\": [\n    {\n      \"name\": \"paperID\",\n      \"description\": \"unique paper identifier\",\n      \"@type\": \"propertyValue\"\n    },\n    {\n      \"name\": \"Journal\",\n      \"description\": \"journal in which article was published\",\n      \"value\": \"1. JESP,\\n2. JPSP,\\n3. PSPB,\\n4. SPPS\",\n      \"maxValue\": 4,\n      \"minValue\": 1,\n      \"@type\": \"propertyValue\"\n    },\n    {\n      \"name\": \"Jahr\",\n      \"description\": \"year of publication\",\n      \"value\": \"0. 2009,\\n1. 2011,\\n2. 2016,\\n3. 2018\",\n      \"maxValue\": 3,\n      \"minValue\": 0,\n      \"@type\": \"propertyValue\"\n    },\n    {\n      \"name\": \"Studynum\",\n      \"description\": \"number of studies per paper\",\n      \"@type\": \"propertyValue\"\n    },\n    {\n      \"name\": \"Sample\",\n      \"description\": \"sample size\",\n      \"@type\": \"propertyValue\"\n    },\n    {\n      \"name\": \"online\",\n      \"description\": \"online data connection\",\n      \"value\": \"0. no,\\n1. yes\",\n      \"maxValue\": 1,\n      \"minValue\": 0,\n      \"@type\": \"propertyValue\"\n    }\n  ]\n}`\n\n\n\n\n\n (Had I been a Reviewer)↩︎\nIt seems easier to change standards at journals than to decrease publication pressure and competititon throughout academia.↩︎\n",
    "preview": "posts/2019-06-14-hibar-how-methods-and-practices-changed-after-the-replication-crisis-in-social-psychology/hibar-how-methods-and-practices-changed-after-the-replication-crisis-in-social-psychology_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-03-04T14:38:25+01:00",
    "input_file": {},
    "preview_width": 3456,
    "preview_height": 768
  },
  {
    "path": "posts/2019-04-21-who-initiates/",
    "title": "Who initiates sex?",
    "description": "A chicken-and-egg-question?",
    "author": [
      {
        "name": "Ruben C. Arslan",
        "url": "https://rubenarslan.github.io"
      }
    ],
    "date": "2019-04-26",
    "categories": [
      "sex diary",
      "open science",
      "sexual activity",
      "sex",
      "quick job"
    ],
    "contents": "\nI’m tempted to say I started writing this post over Easter, because the question “who initiates sexual behaviour in relationships” seems like a chicken-and-egg-question. There are no easy answers. Of course, societal expectations and relationship habits imply that we cannot take whatever answers we find to mean that this translates to who wants (certain) sex (acts) more. And of course, none of this gets any easier given that in our dataset women reported their perceptions of both their partner and themselves—we did not hear from the partners.\n\n\n\nFigure 1: Figure from “Mrs. Basley’s poultry book; tells you what to do and how to do it; the chicken business from first to last including 1001 questions and answers, relative to up-to-date poultry culture” in the Internet Archive Book Images\n\n\n\nBut this is just a blog post, so I’ll ask you to kindly keep these caveats in mind.1\n\n\n\nTo look at who initiates sex more often, I’ll restrict the diary dataset to the 628 women who filled out the diary on more than 30 days and who were in a heterosexual relationship. I’ll also restrict it to days on which women had seen their partners at least briefly. We asked women in heterosexual relationships to endorse the following specific, awkwardly pointed statements. Because we were trying not to bore our participants out of their minds we only asked these questions on 30% of days. This still lets me examine 7303 days, with sexual activity on 3054 of those days.\nMy partner initiated sexual acts with me.\nI initiated sexual acts with my partner.\nSingle women and women who had non-heterosexual relationships filled out detailed questions about their love lives as well, but we’ll leave that for another day.\nWomen and men\n\n\n\nFigure 2: As you can maybe already see, women reported their partners initiated sex more often, on average.\n\n\n\n\n\n\nFigure 3: By looking at this in two dimensions, we can see a bit more detail. On most days, women report equal initiative by both, but on many days they also report initiative from the partner, when they themselves showed zero initiative. It becomes apparent that stretching the response options out from zero to four may not have made that much of a difference.\n\n\n\n\n\n\nFigure 4: Let’s simplify it down to yes or no. I counted everything higher than zero as a yes. That’s probably not the best approach to ordinal data and definitely not a good approach to consent.\n\n\n\nIndividual differences\nI don’t want to neglect individual differences, but visualising them well is hard! Therefore, I hid these attempts here.\n\n\n\nFigure 5: Simplifying it gives us a chance to make a tapestry of the individual differences in these patterns I’m restricting it to women with a lot of data here, so we can still see. Each four-coloured square is one couple/woman. White squares should really be dark blue (zero counts), but I couldn’t quickly make ggplot2 do my bidding.\n\n\n\n\n\n\nFigure 6: We can also look at the number of days on which both, either, or neither initiated sex in a table. Each color is one woman/couple.\n\n\n\n\nTable 1: We can look at the five women those whose initiative exceeded their partners’ most, and those five women whose initiative was lower than their partners most often.\nboth_init\npartner_more\npartner_less\npartner_same\npartner_same_and_nonzero\n7\n0\n7\n14\n1\n7\n0\n5\n19\n2\n7\n6\n10\n11\n4\n20\n2\n7\n17\n12\n6\n7\n2\n14\n2\n11\n13\n4\n6\n4\n9\n14\n0\n7\n3\n3\n11\n1\n10\n3\n5\n24\n0\n2\n1\n\nBy time\nIn the last post, we looked at sexual activity across the day and week. So, you have some idea when sex happens. But who initiates sex on these days? Unsurprsingly, it seems both (or more in some cases) parties are initiating sex more on the weekend.\n\n\n\nFigure 7: Are their partners more likely to initiate sex during the week than our participants? The difference does not seem large.\n\n\n\nA different look at the same data\n\n\n\nFigure 8: Okay, so I have a problem with letting go of redundant plots. So what?\n\n\n\nWhat about time of day? In the last post we saw that women reported slightly lower enjoyment for morning sex on average. Here, we see a corresponding gap in initiation. However, we also see a gap for daytime sex and we saw no enjoyment gap there. I’m very interested in your theories (or literature) why these gaps turn out differently.\n\n\n\nFigure 9: Interesting difference! It may seem odd that average initiation goes down in the evening, even though people have more sex in the evening, but remember that I can only plot sex according to time conditional on having sex. I don’t know exactly what everyone is doing at night, but maybe sex ‘just happens’ more often? Or the kind of people who only have sex in the evenings are less likely to strongly endorse the initiation items. Complicated, huh? I’m lucky this is just a blog post.\n\n\n\nA different look at the same data\n\n\n\nFigure 10: Another redundant plot, but IT LOOKS NICE.\n\n\n\nWho initiates which acts?\nNow, we did not ask for each sexual act who initiated it. That kind of play-by-play just did not seem like a reasonable thing to ask. Still, it might be fun to look at the initiation on each day by the kind of sex people had on that day.\n\n\n\nFigure 11: First, we can look at raw means.\n\n\n\nNow, looking at raw means ignores that certain sex acts are more likely to be performed in the evenings or on weekends and that they co-occur. We already saw these factors have something to do with initiation as well. I’m not sure what the best model is to take all this into account. I’m happy for better suggestions!\nSome activities trade ranks, but fellatio and BDSM (submissive)2 stay at the top. Fellatio is the only act where we see almost no gap in initiation.\n\n\n\nFigure 12: What do we get when we adjust for these other factors and between-individual differences in an ad-hoc model?\n\n\n\n\n\n\nFigure 13: We can also limit the data to only days on which no sexual acts were combined.\n\n\n\nSo, this is a first look at the question who initiates (what kind of) sex. I had a busy week, so this is just quick and dirty.3 I plan to analyse sexual initiation across the menstrual cycle at some point, so it was good to do some sanity checks with this data.\nSelf-reported initiation—not total nonsense, maybe?\nPart of the reason for doing this blog post was seeing whether the items we used give reasonable results. I did not want to bore you with these sanity checks, but I’m also not good at deleting graphs either.\n\n\n\nFigure 14: How would have thought? Initiating sex predicts more sex acts on that day.\n\n\n\n\n\n\nFigure 15: And sexual initiation in the absence of partner initiation is associated with masturbation\n\n\n\n\n\n\nFigure 16: Initiating sex has a stronger relationship with own libido than partner initiation\n\n\n\n# A tibble: 2 x 2\n  who                       `cor(value, high_libido)`\n  <chr>                                         <dbl>\n1 sexual_initiation_partner                     0.453\n2 sexual_initiation_self                        0.609\n\n\n\n\nFigure 17: Initiating sex has a stronger relationship with with wanting to satisfy own sexual needs than partner initiation\n\n\n\n\n\n\nFigure 18: Initiating sex has a stronger relationship with with wanting to be desired than partner initiation\n\n\n\n\n\n\nFigure 19: Self- and partner initiation are similarly associated with wanting to satisfy partner’s sexual needs.\n\n\n\n\n\n\nFigure 20: Guess who enjoys partner-initiated sex? Partners.\n\n\n\n\n\n\nFigure 21: However, own enjoyment is not higher when having initiated sex.\n\n\n\n\n\n\nFigure 22: Neither is happiness.\n\n\n\nLest I sell the data short, the initiation questions do show reasonable associations with a bunch of other questions we asked, see Appendix. However, I have tagged this post a “quick job”, so it is excluded from by Bug Bounty program. I still appreciate hearing about errors of course.↩\nThis response meant engaging in some BDSM play in a submissive role. It was also possible to specify BDSM play in a dominant role, but that fell below my arbitrary cutoff of at least 30 observed acts↩\nBut let me know if you want to see a graph I could add.↩\n",
    "preview": "posts/2019-04-21-who-initiates/who-initiates_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2020-06-06T06:58:54+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-04-08-sex-by-day-and-by-night/",
    "title": "Sexual activity by time of day",
    "description": "Examining when women in our diary study were sexually active.",
    "author": [
      {
        "name": "Ruben C. Arslan",
        "url": "https://rubenarslan.github.io"
      }
    ],
    "date": "2019-04-12",
    "categories": [
      "sex diary",
      "open science",
      "sexual activity",
      "sex"
    ],
    "contents": "\nWe collected detailed data on sexual activity in our diary study. After a lot of discussion in the team, we agreed on the following method:\nWe asked women whether they had been sexually active (including masturbation, and caresses1)\nIf they said yes, we asked how often\nIf they gave a number higher than two, we asked them to describe only the first two activities\nThen, for each sexual activity, we asked\nWhen?\nWith whom?\nWhat did you do?\nIf they mentioned a partner, we asked about contraception.\nIf they mentioned masturbation, we asked about sexual fantasies\nWho/what did you think about?\n\nWe then asked about own satisfaction and partner satisfaction, and whether it made them happy.\n\nA big part of our discussion was which sexual activities we should list. A very long list of activities seemed a bit intimidating, plus many sexual activities in German have either weirdly academic or vulgar names, but nothing that seemed usable. We didn’t want to exclude non-genital sexual contact from consideration, but also didn’t want to ask our participants to report every goodbye kiss or hug. In the end, we opted for a subset of activities (including kissing and cuddling) and the ability to write in unlisted options, which we coded afterwards. We think this worked quite well, but it is possible that the activities we listed were “normalised” more than unlisted activities. I’m curious how others go about this.2\nBecause we tried to impose as few restrictions as possible, cleaning this data was a bit cumbersome though (people have sex in manifold ways!). To motivate myself to do it, I wanted to blog a few descriptive statistics. I don’t have a sexology background, so I don’t know what is well-known to experts in the field and what isn’t. Of course, we mainly plan to use the data to study sex across the menstrual cycle, but I think there are many interesting aspects to the data unrelated to that.\n\n\n\nHere, I am looking at all data (1345 women over 61,365 days, ~45 days per woman). First off, how many sexual acts do women report across days?\n\n\n\nFigure 1: Number of sexual acts on each diary day. X-axis is log1p-scaled.\n\n\n\nThis looks good. Women rarely report more than two acts a day, so we are not missing much in our survey. On average, people in our sample engaged in some sexual activity (including kissing and cuddling) on 31% of days.3 Excluding kissing and cuddling, women in our sample either masturbate or have some sort of sex on 28% of days. Broken down even further, women in our sample have sex with a partner on 16% of days and masturbate on 13% of days. In the following, I will exclude kissing and cuddling, although it makes little difference for the results.\nOf course, there are individual differences in these frequencies. In yellow, you see women in relationships, in blue single women.4\n\n\n\nFigure 2: Sexual act frequency differences\n\n\n\nMost readers probably know people are more sexually active on weekends:\n\n\n\nFigure 3: More sex on the weekends\n\n\n\nWhat’s up with Monday though? Because women filled out our diary from 17:00-03:00, they also reported on sex the previous night. Maybe some of the increase we’re seeing is due to this? I excluded sex on the previous night in the below graph.\n\n\n\nFigure 4: Without sex the previous night\n\n\n\nDetailed break-up by time of day\n\n\n\nFigure 5: Sexual activity by time of day\n\n\n\nIt seems the Monday bump is just Sunday night sex (see also the details graph, if you are interested). We can also move all last night’s sexual activity to the day before and see whether Mondays look more like we expect Mondays to look.5\n\n\n\nFigure 6: Sex and time of sex across the week. Y axis shows the count of sex acts on that day at that time as a percentage of the total number of diary days divided by 7. We can see that daytime sex increases\n\n\n\nLooks reasonable. So far, I have lumped all sexual activity, partnered and non-partnered.\n\n\n\nFigure 7: The weekend bump appears only for partnered sexual activity. But there is an interesting decrease in masturbation before falling asleep on Thursday, Fridays, and Saturdays.\n\n\n\nSo, maybe it’s not mainly about having more time on the weekend, but rather about coordination? We have a lot of data about whether people are living together, in the same city, or in a long-distance relationship.\n\n\n\nFigure 8: Long-distance relationships make the week-end gap worse, but even couples who live together have more sex on week-ends.\n\n\n\n\n\n\nFigure 9: The week-end gap is smaller, but not gone for couples who spend the more days a week together.\n\n\n\nWe can also relate the timing of sex to own/partner satisfaction and happiness. For example, we saw above that women report masturbating in the morning and during the night quite rarely, whereas partnered sexual activity was comparatively more common then.\n\n\n\nFigure 10: Women’s own satisfaction and happiness and their partner’s satisfaction as a function of time of day. The scale went from 0 [not at all] to 4 [very much so].\n\n\n\nIn fact, women report being less satisfied with and happy about morning sex.6 The difference is small and there could be a ton of factors (from circadian hormone change, to different activities by time of day, to differing partner attentiveness, to feeling rushed before work, and so on).\nWe have data on aspects of the sexual activity other than time, so we can potentially pull apart different influences on happiness and enjoyment (although still with much less than experimental rigour).\nThis is an incredibly rich dataset, so I am very interested in suggestions what else to look at and how.\nPS\nIn follow-up discussions, it occurred to me that I never included two of the most obvious graphs. Luckily, I can remedy that now.\n\n\n\nFigure 11: People have the most sex in the evenings. On week-ends, they also get a chance during the day and in the mornings.\n\n\n\n\n\n\nFigure 12: We can show it even more strongly by collapsing the two evening and the two morning categories\n\n\n\nprobably the German word Zärtlichkeiten sounds less weird than this translation. Or more so.↩\nI’ll blog about this list of activities another time.↩\nIf this seems low to you: as you can see in the aside, most women did not report kissing and cuddling if there was not also some other sexual contact.↩\nI restricted the dataset to the 878 women who had participated for at least 40 days to make sure we could estimate the frequency with some accuracy.↩\nSteering clear of any Garfield references here.↩\nThey say the same thing about their partner’s enjoyment.↩\n",
    "preview": "posts/2019-04-08-sex-by-day-and-by-night/sex-by-day-and-by-night_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2020-06-06T06:58:55+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-04-02-measuring-contraception/",
    "title": "Measuring contraception",
    "description": "How we measured methods of contraception, reasons for using different contraceptives, and fertility awareness.",
    "author": [
      {
        "name": "Ruben C. Arslan",
        "url": "https://rubenarslan.github.io"
      }
    ],
    "date": "2019-04-02",
    "categories": [
      "sex diary",
      "open science",
      "contraception"
    ],
    "contents": "\nIn our sex diary study, one of the most important components was our contraception questionnaire. Surprisingly, to us, we did not find a good, open standard questionnaire that fit our needs.1 We needed something that would measure\nnot only one main, but all methods of contraception\nfertility awareness2 independently of whether it was used for contraceptive purposes, i.e. including apps like Clue\nthe reasons why people mix contraceptive methods\nthe exact contraceptive pill, intra-uterine device, ring, patch etc.3\nwhat fertility-awareness method users do when fertile\nall potential contraceptive methods that we could imagine\nWhy do we need such detailed information on contraception to study ovulatory effects?\nCombined hormonal contraceptives suppress ovulation—we could use them as a quasi-control group for our last study. However, progestogen-only contraceptives do not reliably suppress ovulation.\nSome fertility awareness method users are sexually abstinent in the fertile window. When we are interested in whether ovulation whets the appetite for sex and changes sexual behaviour, we should probably exclude these women or at least treat them separately.\nAwareness of the cycle phase could lead to different responses. Potentially, women actually feel or report feeling more desirable, or more sexual when they believe they are fertile—independently of, or on top of any hormonally caused changes.4\nToday, I want to show the questions we chose and share the questionnaire. We’re still in the market for improvements. At the same time, I want to encourage others to more fully report contraception information in their menstrual cycle studies. When you are reporting effects on sexual desire or activity, we need to know more than that participants were not on the pill and not pregnant. I’ll try to show why in the coming blog posts.\nTranslation details\nTo be able to show the descriptive graphs below, we translated the questions.5\n\n\n\nContraception or not?\nWe began by asking whether women used contraception at all. This time, we only asked women who were not pregnant. Next time, we plan to ask all women about contraception because even pregnant women could e.g. use condoms to prevent sexually transmitted infections, urinary tract infections. However, so that it does not like a lack of forethought, we should probably optionally display some explanation to this effect for e.g. menopausal and pregnant women, because some women found it odd to be asked these questions. In this study, we gave women a free text field to write in if they wanted to specify other reasons why they didn’t use contraception.\n\n\n\nFigure 1: Pregnant women were excluded from the contraception questionnaire.\n\n\n\n\n\n\nFigure 2: Different degrees of using contraception\n\n\n\nMethods\nNext, we asked all women who used contraception which methods they used. Multiple methods could be checked. As you can see, condoms and the the pill were the two most common methods. A lot of women also combined both. Next, you might be surprised at the number of women who report using coitus interruptus (in the lower left graph), the pull-out method. However, more women combined this with condoms or a fertility awareness method than used only coitus interruptus. Next were intra-uterine devices and hormonal contraceptives other than the pill. We will look at these in detail next.\n\n\n\nFigure 3: Methods of contraception and how they are combined. In this UpSet plot, we see how common each contraceptive is (on its own or in combination with others) in the small bar chart in the lower left. We see which combinations are common by checking which circles are connected in the panel at the bottom and checking the height of the intersection bar.\n\n\n\nAs you can see, the non-pill hormonal methods were mainly NuvaRing (a estrogen/progestogen vaginal ring) and Mirena (a progestogen-only intra-uterine contraceptive).\n\n\n\nFigure 4: Other hormonal contraceptives\n\n\n\nCombining methods\nAbove, we saw that many women combine multiple contraceptives. We had a list of reasons that we considered likely, but many women also gave reasons we hadn’t included.\n\n\n\nFigure 5: Why combine several contraceptives?\n\n\n\nThe most common reason was as a fallback method during fertile days (i.e., for women using fertility awareness methods). Many used condoms in addition to the pill to also prevent STDs, but many also used the pill and condoms to decrease conception risk even further. Fallback methods were also common (e.g. condoms as a fallback to the pill, or pull-out as a fallback to condoms). A substantial number of women also used different methods for different partners, and a small minority also said they used condoms when their partner has a cold.\nFrom the reasons we hadn’t included in our list, we should include at least the following in the future:6\nfallback in case the pill might not have worked (e.g., stomach flu, throwing up, travel, medication)\npill against acne/hypermenorrhoe/menstrual pain/other reasons, another method for contraception\ncondoms for hygienic reasons\ncondoms to avoid urinary tract infections\naccording to mood\nFertility awareness\nA special case of combining contraceptive methods are the fertility awareness methods. How many of our participants are sexually abstinent during the fertile phase, how many switch to other methods? As you can see, most switch to other methods. Total sexual abstinence is rare, although quite a few report having penetrative sex less often, or not at all.\n\n\n\nFigure 6: What do fertility awareness method users do during their fertile phase?\n\n\n\nMethods and reasons for combining methods in two big graphs\nWe can of course also merge the reasons for combining contraceptives with the contraceptive information itself.\n\n\n\nFigure 7: Which contraceptives are combined for which reasons?\n\n\n\n\n\n\nFigure 8: Which contraceptive method do fertility awareness method users switch to during their fertile phase?\n\n\n\nThe pill\nTo help our participants identify their pill, we used a searchable drop-down that showed the brand name of the pill together with a picture of the packaging. Most women found their pill in our list; the others were asked to enter name and details (progestogen type and amount, estrogen amount) into text fields.\n\n\n\nFigure 9: Common oral contraceptive pills. Only those used by at least 10 women shown.\n\n\n\n\n\n\nFigure 10: Micrograms of estrogen in a pill (unified to average content over 21 days, without pill break)\n\n\n\nAwareness\nQuite a few women used some sort of cycle app.\n\n\n\nFigure 11: Cycle app users\n\n\n\n\n\n\nFigure 12: Cycle apps. Only those used by at least 10 women shown.\n\n\n\nWe looked at all of these apps and coded their purpose. Some of them are just pill reminders, some additionally allow you to track symptoms, whereas others really lead to fertility awareness (i.e., tell you when they estimate you will be fertile).\n\n\n\nWe are really glad we asked about apps. As you can see below, the vast majority of cycle awareness app users do not use them as a primary method of contraception; most use condoms and IUDs.\n\n\n\nFigure 13: Cycle awareness app users’ contraceptive methods.\n\n\n\nEstrogen and progestogen\nAs you could see above, our participants used a wide variety of contraceptives. Among them were quite a few progestogen-only contraceptives (hormonal IUDs, such as Mirena, patches and depots, and of course various mini-pills, such as Jaydess, Cyprella, Desirett, Cerazette, etc.). When we add all this together, we see that a substantial minority uses progestogen-only contraceptives.\n\n\n\nSummary\n\n\n\nFigure 14: To summarise all this for our purposes, we constructed the above categories. In A, we do not yet include information on cycle tracking apps and on the different hormones in contraceptives. In B, we account for this but lump women who combine hormones and barrier methods with other hormonal contraception users. To do so, we differentiate two hormonal methods, awareness methods, condoms, non-hormonal IUDs, and no contraception. The rest are lumped. For women who combine several contraceptives, the order in the preceding sentence determines precedence.\n\n\n\n\n\n\nSo, these were the main questions from our questionnaire. I didn’t get into how we combined this with questions about the period (menarche/menopause, regularity, cycle length, etc.), family planning, and so on.\nI added the entire list of questions, including skipping logic (which takes some forethought) below. You can export these to Excel and use them directly in formr.org. This is already an adapted version of the questionnaire we used and I’m sure it can be improved further. What questions are you asking? What can we improve in our next study?\nOur questions\n\n\n\n\nAcknowledgements\nThanks to Tessa Cappelle and Kim Gloystein for help translating the questionnaire. Thanks to Julie Driebe and Iro Eleftheriadou who helped me clean and code the contraception questionnaire data. The questionnaire was designed by our study team (Julie Driebe, Tanja M. Gerlach, Lars Penke, Julia Jünger (now: Julia Stern), Julia Ostner).\nWe checked a few familiar demographic surveys, which erred on the side of asking fewer questions, and the psychological literature, which erred on the side of not reporting exactly which questions were asked↩\nCounting days, measuring temperature, mucus↩\nso we could differentiate between contraceptives with different kinds and amounts of progestogens and different amounts of estrogens↩\nIn our last study, we only found some descriptive differences between non-hormonal methods. The only significant difference was between hormonal and non-hormonal methods, but our sample size for fertility awareness methods was small and potentially women who used condoms also used a fertility tracking app for fun, but not contraception. Descriptively, fertile window increases in extra-pair desire were larger for women using awareness methods than for barrier method (condoms, mostly) users, but the opposite pattern held for in-pair desire, and we found no apparent difference for self-perceived desirability.↩\nIf you ever find yourself in the situation of wanting to re-label items in your language with English labels, you can follow along the code for this post on Github. It’s really simple as long as you have a well-structured item sheet.↩\nTaken from the written responses to the other reasons prompt.↩\n",
    "preview": "posts/2019-04-02-measuring-contraception/measuring-contraception_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2020-06-06T06:58:54+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-03-13-intra-individual-feedback/",
    "title": "Intra-individual feedback",
    "description": "Thinking through an intra-individual menstrual cycle feedback.",
    "author": [
      {
        "name": "Ruben C. Arslan",
        "url": "https://rubenarslan.github.io"
      }
    ],
    "date": "2019-03-19",
    "categories": [
      "sex diary",
      "open science",
      "feedback",
      "visualization"
    ],
    "contents": "\nIn our first sex diary study, we had no real budget, so we encouraged users to participate by promising them detailed feedback. This worked great1 and was well-received by participants. In the second study, we could pay participants up to 45€, but of course I wanted to one-up the feedback from the first study, given that we were using formr.org and I’m super proud of its feedback capabilities.\nSpecifically, I wanted to also include intra-individual feedback on menstrual cycle changes. After all, we had collected up to 70 days worth of data from each woman and asked them quite many questions. Even if the results wouldn’t always be diagnostic (there may well be cycles changes that are missed), we could at least show that. After all, many women have theories about how they personally change over the cycle.2 Most women do not explicitly track or notice ovulation.3 So, for most it is easier to notice menstrual changes rather than ovulatory changes. Our feedback might actually uncover something about them that they did not already know better themselves. I would not say this about most personality feedback based on self-report.\nDecisions\nWhen designing feedback plots that will be generated live without a human in the loop, you have to make many decisions.\nDo you require a minimum of data?\n(How) do you display uncertainty?\nDo you show the raw data?\nWhich outcomes are interesting?\nWe pretty much free-styled these decisions. Given that my collaborators in Göttingen might run such studies again, I thought I’d show what we did and ask for reader input. We didn’t require a minimum of data, but we showed uncertainty and raw data instead. I’ve gotten the feedback from my collaborators at Clue that they would not show their users such complex plots though, so I’m very interested in simplifying them or the messages they contain.\n\n\n\n\n\n\nFigure 1: An example feedback plot. On the X axis you see the days until the next menstruation. On the Y axis, you see the outcome (here, high sex drive). The line is a locally weighted regression smooth with a 95% confidence interval.\n\n\n\nPlanned revisions\nOf course, for many women the answers from the plots were pretty equivocal, like this plot.\nI should not have used LOESS, but a GAM with a cyclic spline s(menstrual_onset_days_until, bs = \"cc\"), given that it is, you know, a cycle. I did not know that then. Thanks to Mike Lawrence for pointing this out in response to our first study’s graphs.\n\n\n\nFigure 2: The same graph as above with cyclic splines.\n\n\n\nI should have included menstruation dates from our screening and follow-up surveys, not just the diary. I forgot or didn’t get the code working in time. This meant that some days on either side of the diary could not be included. Doing this for this participant, we get 20 days more and a clearer pattern.\n\n\n\n\n\n\nFigure 3: The same graph as above with all days included.\n\n\n\nI’m not happy about how we showed the fertile window. In our research work, we use a continuous measure of conception risk and of course we cannot be sure that we estimate the day of ovulation precisely with counting data. So, our precise fertile window above actually went against our own philosophy of showing uncertainty.\n\n\n\nFigure 4: The same graph as above with a more continuous fertile window.\n\n\n\nLastly, showing an intra-individual plot is great, but in the end, we are giving particpants something they could have found for themselves by tracking their cycle and psychological changes. And it is simply interesting to compare yourself to others.\nTo this end, it would have been cool to show norm data as well. We did not have such data yet because the feedback was generated live, while the study was running. Now, I can add data for other women as well. I’ll show the code for the “final” plot function here.4.\n\n\ndiary_comparable <- diary %>% \n  group_by(session, cycle_nr) %>% \n  filter(!is.na(prc_stirn_b), \n         hetero_relationship == 1,\n         RCD > -30, reasons_for_exclusion == \"\",\n         between(cycle_length, 27, 30))\n\nplot_menstruation = function(data, y, ylab, ylim = c(0, 4)) {\n  if (!is.null(ylim)) {\n    ymax = ylim[2]; ymin = ylim[1]\n  } else {\n    ymax = max(data %>% select_(y), na.rm = T)\n    ymin = min(data %>% select_(y), na.rm = T)\n  }\n  alphas <- s3_daily %>% \n    ungroup() %>% \n    select(menstrual_onset_days_until, prc_stirn_b) %>% \n    distinct() %>% \n    filter(prc_stirn_b > 0.1) %>% \n    deframe()\n\n  rects <- alphas %>% \n    imap(~ annotate('rect', \n                    xmin = as.numeric(.y), xmax = as.numeric(.y) + 1, \n                    ymin = ymin, ymax = ymax + 1, \n                    fill = '#37af9bAA', color = NA, alpha = .x))\n  \n  ggplot(data, aes_string(x = \"menstrual_onset_days_until\", y = y)) +\n    rects +\n    annotate('rect', xmin = -6, xmax = 0, \n             ymin = ymin, ymax = ymax + 1, fill = '#ed9383AA', color = NA) +\n    annotate('text', label = 'fertile\\nwindow', x = -19.5, \n             y = ymax, size = 4, hjust = 0) +\n    annotate('text', label = 'premenstrual\\nphase', x = -5.5, \n             y = ymax, size = 4, hjust = 0) +\n    geom_jitter(aes_string(shape = \"menstruation_labelled\", \n                           color = \"menstruation_labelled\"), size = 1.5) +\n    scale_color_manual(\"Menstruation\", \n                       values = c(\"no\" = \"black\", \"probably\" = \"#b86147\", \n                                  \"yes\" = \"#cf6030\")) +\n    scale_shape_manual(\"Menstruation\", \n                       values = c(\"no\" = 16, \"probably\" = 17, \"yes\" = 17)) +\n    geom_smooth(aes(group = 3), data = diary_comparable %>% \n                  filter(hormonal_contraception == \"FALSE\"), \n                method = \"gam\", formula = y ~ s(x, bs = 'cc'), \n                color = \"red\", fill = \"red\", alpha = 0.1) +\n    geom_smooth(aes(group = 2), data = diary_comparable %>% \n                  filter(estrogen_progestogen == \"progestogen_and_estrogen\"), \n                method = \"gam\", formula = y ~ s(x, bs = 'cc'), \n                color = \"black\", fill = \"black\", alpha = 0.1) +\n    geom_smooth(aes(group = 1), \n                method = \"gam\", formula = y ~ s(x, bs = 'cc'), \n                color = \"#ee00ee\", fill = \"#ee00ee\", alpha = 0.1) +\n    ylab(ylab) + \n    xlab(\"Days until next menstruation\") +\n    theme(legend.title = element_text(size = 10), \n          legend.text = element_text(size = 8)) +\n    coord_cartesian(ylim = ylim)\n}\nplot_menstruation(s3_daily, \"high_libido\", \"High sex drive\")\n\n\n\n\nFigure 5: The same graph as above with the average curves. The black curve shows women who use combined hormonal contraceptives. The red curve shows women who don’t use hormonal contraception.\n\n\n\nAs you can see, this woman, whether by chance or because of meaningful individual differences5, seems to show a larger-than-average increase in libido in the fertile window.\nSummary\nThe last graph does a better job than the original. It shows the uncertainty for the participant (including the fertile window estimate), includes more of their data, uses cyclic splines, and shows the normative pattern.\nI’m sure it could still be improved. For example, the splines for the rest of the sample ignore the multilevel structure of the data. I have fit models with brms that could do a better job of this, but I’ll save those for another day. Do you have any other suggestions?\nMore feedback\nI didn’t want to bore my audience too much, but if anyone is curious and doesn’t mind reading German, I uploaded an example of a full feedback for this participant. It includes personality feedback, notes on how to interpret the cycle graphs, and several interactive graphs (on time use on weekends versus weekdays, and on mood across the period of the entire diary). You can find the code here. I did not share the data to protect participant privacy, but it may still be useful if you want to make your own feedback plots.\n\nOk, we didn’t randomise this, so I don’t actually know whether it wasn’t something else - we really went all-in on recruiting too.↩︎\nWe asked about these theories after the diary concluded, but that is a story for another day.↩︎\nWith the exception of the large minority, ca. 30%, of women who experience mittelschmerz, or ovulation pain.↩︎\nYou can find the intermediate steps on Github.↩︎\nI plan to rigorously test such questions soon.↩︎\n",
    "preview": "posts/2019-03-13-intra-individual-feedback/intra-individual-feedback_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2021-03-04T14:39:31+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1344
  },
  {
    "path": "posts/2019-03-12-moderation-vs-distributional-regression/",
    "title": "Do single mothers have a stronger influence on their kids?",
    "description": "And how would you test that?",
    "author": [
      {
        "name": "Ruben C. Arslan",
        "url": "https://rubenarslan.github.io"
      }
    ],
    "date": "2019-03-13",
    "categories": [
      "simulation",
      "data-generating model",
      "brms"
    ],
    "contents": "\nYesterday, I sat down with Lisa Reiber, who is doing her master’s thesis with me. She is working on the question of the transmission of risk preferences in families. Many of the potential predictions we hope to test involve mean level changes (e.g., children of divorcees will be more risk-averse because they learned the world is less certain). In other cases, once we get down to which models we want to test, it is a lot less clear.\nOne question which I have struggled with in the past often comes up when the phrase “strength of influence” comes up. For example, some theories say that mothers have a stronger influence on their children’s traits if the fathers are absent. Verbally, I have also seen this expressed “play a bigger role”. Intuitively, how would you test this?\nWe decided we should make a quick data-generating model to clarify what we mean when we talk about this.1\nWe start by generating families, half of which are divorced. Mothers and fathers aren’t mating assortatively in this example.\n\n\nlibrary(tidyverse)\nlibrary(brms)\ntheme_set(theme_bw())\nfam <- tibble(\n  mother = rnorm(1000),\n  father = rnorm(1000),\n  divorce = rep(c(0,1), each = 500)\n)\n\n\n\nSo far, so good. We have a 1000 couples - let’s make children. We’ll assume all transmission is through parenting, which these hypothetical parents share equally, not genetics. Hence, in two-parent families, they share parenting equally and both have the same influence on their child’s trait.\nIf there was no divorce, we would expect the following. We have equal contributions by each parent (0.6 * parent). Despite all the power of parenting in this hypothetical world, children turn out somewhat differently from their parents, which is reflected by the final term 0.6 * rnorm(1000).\n\n\nfam <- fam %>% \n  mutate(\n   child = 0.6 * mother + 0.6 * father + 0.6 * rnorm(1000)\n  )\n\n\n\nWhat happens in case of divorce? In divorced families, fathers have absolutely no influence.2 One thing that is pretty clear, is that the term for the father’s influence should be zero in our model ((1 - divorce) * 0.6 * father). And isn’t it logical that the mother’s regression weight should hence go up by the same amount (divorce + 1) * 0.6 * mother?\n\n\nfam <- fam %>% \n  mutate(\n    child = (divorce + 1) * 0.6 * mother + (1 - divorce) * 0.6 * father + 0.6 * rnorm(1000)\n  )\n\n\n\nOn thinking about this, we reconsidered. Does having a stronger influence mean that\nwe think risk-averse divorced mothers make their children twice as risk-averse as themselves compared to non-divorced mothers?\nthat there is no difference between divorced and non-divorced mothers when the mothers are average in risk preference?\nThis would mean that divorcee’s children are actually less similar to their mothers in absolute terms. No, we decided by “having a stronger influence” in this case, we actually meant simply that another systematic influence (the father) was removed, so the mothers’ contribution to differences in their children would be bigger and the children would be more similar to them. This is a simpler model. We only say that the contribution by the father is removed (1 - divorce) * 0.6 * father.\n\n\nfam <- fam %>% \n  mutate(\n    child = 0.6 * mother + (1 - divorce) * 0.6 * father + 0.6 * rnorm(1000)\n  )\n\n\n\nIf you think the second model makes more sense, you would probably tend to test an interaction between mother’s trait and divorce. But how would you test the third scenario? When there is distinct groups, I have been trained to think of this as a difference in correlations (similarity, variance explained). I didn’t really have a lot of experience teaching this particular nuance. I think I left my own graduate statistics classes thinking that correlations are basically regressions with the variables standardised.\nNow, really, this is the model we would want to test, the data-generating model.\n\n\nsummary(brm(child ~ mother + divorce * father, data = fam, cores = 4, file = \"dgm\"))\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: child ~ mother + divorce * father \n   Data: fam (Number of observations: 1000) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nPopulation-Level Effects: \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept         -0.01      0.03    -0.06     0.05 1.00     5029\nmother             0.60      0.02     0.56     0.64 1.00     5777\ndivorce           -0.01      0.04    -0.08     0.07 1.00     4637\nfather             0.62      0.03     0.56     0.68 1.00     3222\ndivorce:father    -0.61      0.04    -0.69    -0.53 1.00     3316\n               Tail_ESS\nIntercept          2969\nmother             2920\ndivorce            3167\nfather             2962\ndivorce:father     3243\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.63      0.01     0.60     0.66 1.00     5997     3004\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nNice.3 But in many datasets (and in ours), divorced families will have missing data for the fathers.\n\n\nfam$father <- NA\n\n\n\nSo, we might instead test the following model; after all we think mothers will have greater influence in divorced families and a lot of people seem to test “greater influence” via interaction tests.\n\n\nsummary(brm(child ~ divorce * mother, data = fam, cores = 4, file = \"interaction_model\"))\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: child ~ divorce * mother \n   Data: fam (Number of observations: 1000) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nPopulation-Level Effects: \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept          0.01      0.03    -0.05     0.08 1.00     4345\ndivorce           -0.03      0.05    -0.12     0.07 1.00     4342\nmother             0.60      0.03     0.54     0.67 1.00     3590\ndivorce:mother    -0.00      0.05    -0.09     0.09 1.00     3393\n               Tail_ESS\nIntercept          2856\ndivorce            2679\nmother             2943\ndivorce:mother     2735\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.75      0.02     0.72     0.78 1.00     5118     2475\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nBut the interaction is estimated at zero! A plot to the rescue!\n\n\nggplot(fam, aes(mother, child)) +\n  geom_point(alpha = I(0.1)) +\n  geom_smooth(method = 'lm') +\n  coord_cartesian(c(-3,3), c(-3,3)) +\n  facet_wrap(~ divorce, \n             labeller = labeller(divorce = c(\"0\" = \"Not divorced\", \"1\" = \"Divorced\")))\n\n\n\n\nHere, we are looking at scatter plots of mother and child by marital status. We can see visually that the slopes of the regression lines are the same. However, now we notice that the scatter around the regression line is more dispersed in the non-divorced group. I have to say, I am not sure how easily this sort of thing is noticed in plots with real data, noisier relationships, or if the moderator of influence strength is continuous.\nInstead of regressions, we can also run correlations\n\n\nfam %>%  \n  summarise(cor(child, mother))\n\n\n# A tibble: 1 x 1\n  `cor(child, mother)`\n                 <dbl>\n1                0.630\n\nfam %>% group_by(divorce) %>% \n  summarise(cor(child, mother))\n\n\n# A tibble: 2 x 2\n  divorce `cor(child, mother)`\n*   <dbl>                <dbl>\n1       0                0.567\n2       1                0.725\n\nNow, we see that the correlation between mother and child is indeed stronger in divorced families. However, I never particularly liked this approach to this problem. I find correlations harder to think about in terms of my data-generating model (you’ll notice that the correlations .55 and .70 appear nowhere in the code above). It also becomes difficult when moving to multiple regression, multilevel models, or non-normal data. That’s why I am so happy about brms. It allows me to think about the models I want to fit in almost the same language that I use to think about data-generating models. This greatly reduces cognitive friction for me.\nWhat would this model look like in brms? It’s an example of a distributional regression with unequal variances. The brmsformula function allows us to group multivariable formulas.\n\n\nmodel_formula <- brmsformula(\n  child ~ mother, # the regression of mother on child\n  sigma ~ divorce # sigma is a reserved word. \n  # we are predicting the size of the residual variation \n  # using the divorce variable\n  )\n\n\n\nLet’s run this model.\n\n\nmod <- brm(model_formula, \n  data = fam, cores = 4, \n  file = \"divorce_importance\")\nsummary(mod)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: child ~ mother \n         sigma ~ divorce\n   Data: fam (Number of observations: 1000) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nPopulation-Level Effects: \n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept          -0.00      0.02    -0.05     0.04 1.00     4308\nsigma_Intercept    -0.25      0.03    -0.31    -0.19 1.00     4143\nmother              0.60      0.02     0.56     0.64 1.00     4081\nsigma_divorce      -0.22      0.05    -0.32    -0.13 1.00     4401\n                Tail_ESS\nIntercept           2940\nsigma_Intercept     3360\nmother              3148\nsigma_divorce       3068\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nHere we go. We see clearly that there is less residual variation when the mother is the only parent. We can visualise this too. We have to use “predict” method, because this leads brms to include the residuals (sigma) in the uncertainty intervals. This plot nicely recapitulates our scatter plots from above.\n\n\nconds <- data.frame(divorce = c(0,1))\nrownames(conds) <- c(\"not divorced\", \"divorced\")\nplot(marginal_effects(mod, effects = \"mother\", \n                 method = 'predict',\n                 conditions = conds))\n\n\n\n\nSummary\nIt often helps to generate data according to the model we have in mind. Even such simple simulations can give us a sense of whether we are able to recover our model and sometimes they may lead us to notice that we are using a word like “influence” in a very vague sense and deriving the wrong test because of that. A way that helps me clarify this is to ask whether I am really thinking about another influencing variable that is reduced in importance (here, the father).\nIn personality psychology, I think a lot of us intuitively grok this problem when the two variables are the same thing measured twice (e.g. stability, consistency, items), but even then we sometimes lose sight of it.4 Maybe one reason is that for more complex questions, the right models are harder to fit. That’s where brms comes in handy.\nAcknowledgements\nThanks to Lisa Reiber, Julia Rohrer, and Paul Bürkner for related discussions.\n\nI am very fond of simulating things to figure out stuff that others may learn through math, I just grok it more quickly. Fortunately, it works similarly for Lisa, so we decided to share this simple model.↩︎\nThis is a bit internally inconsistent with them wanting to share parenting equally before, but whatever, custody battles really favour mothers in this hypothetical world.↩︎\nYou can see how neatly it recovers all the parameters in our data-generating model.↩︎\nAnother problem may be that our love-hate relationship with measurement error makes us regard absolute regression coefficients with suspicion, and as fickle.↩︎\n",
    "preview": "posts/2019-03-12-moderation-vs-distributional-regression/moderation-vs-distributional-regression_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2021-03-04T14:31:44+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-03-08-the-golden-age-of-never-finishing-anything/",
    "title": "The Golden Age of Never Finishing Anything",
    "description": "Serialising the results from our sex diary study.",
    "author": [
      {
        "name": "Ruben C. Arslan",
        "url": "https://rubenarslan.github.io"
      }
    ],
    "date": "2019-03-11",
    "categories": [
      "sex diary",
      "open science"
    ],
    "contents": "\nI have too many unfinished projects. Thanks to social media, I know I’m not alone with this problem, but this is probably the only thing I can thank social media for with respect to this.\n\n\n\nFigure 1: Sagrada Família. Unfinished projects can still be pretty neat. From Wikipedia.\n\n\n\nDuring my PhD I (barely) wrapped up two projects with supersized supplementary websites that contained lots of extra work, robustness analyses, so many little sidetracks, footnotes and so on. In one case, it took me so long to finish the project that I felt compelled by my newly gained statistical knowledge and love of brms to do over all main analyses. Although I’m not a perfectionist1, the fact that traditional publication is final and corrections are difficult exacerbated any such tendencies in me.2 In the end, making these websites was fun and I learned a lot, but few people ever really engaged with their contents.3\nThat is okay - not everyone needs to care about the intricacies behind the question whether older fathers have more harmful mutations in their sperm, or the many decisions on can make when calculating a fertile window probability. Nor do most people stay around me at parties when I start talking about a cool R package I found for easily using a computing cluster.\nThen again, some people may care. Potentially related to this, we are currently in the Second Golden Age of Television according to some. Some of this is due to the ability to reach more niche audiences using the internet. Maybe this format just takes advantage of our more limited attention spans. Anyway, it might work for me.\n\n\n\nFigure 2: Tower of Babel by Lucas van Valckenborch (1594). Famously unfinished because of Netflix binging. From Wikipedia.\n\n\n\nIn an effort to stop myself accumulating results sections in need of introduction and discussion, and to get a chance to talk about some of the smaller steps in the process, I decided to start blogging a current project. Fittingly, there is a project that I started during the last year of my PhD. It would have provided ample data for my post-doc in Göttingen, but I moved back to Berlin to work at the MPI. The project suffered owing to the many new exciting collaborations that resulted from this move. We wrote a humongous preregistration for it, but did not give enough thought to the question how we would organize all these results.4\nI plan to blog preliminary5 results from this large sex diary study, but also nifty workflow stuff, lost trails in the literature, struggles with my own preregistration, and so on. I hope this bite size series on this project can start discussing some results sooner. I still hope to publish most of this in traditional journals at some point, but this approach makes make me feel less bad about working on some smaller projects before finishing the write-up of our fifty preregistered hypotheses.\n\n\n\nFigure 3: The Berlin Babel Imitation. When I read about how they have to replace all monitors because they installed them six years before opening, I was painfully reminded of transitioning my years-old code to dplyr 0.8.0. From Wikipedia.\n\n\n\nTry using formr.org or codebook and you’ll find this to be true↩\nNot that the results were anywhere close to perfect.↩\nAnd I’m not saying I know this because I hid hilarious easter eggs in these supplements that no one ever found.↩\nThis experience is one of the reasons I’m a big believer in Registered Reports, which require us to be more realistic about these things.↩\nBut not that preliminary. My bug bounties apply to these posts.↩\n",
    "preview": "https://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Baldomer_Gili_Roig._La_Sagrada_Fam%C3%ADlia%2C_1905_Copia_moderna_del_negatiu_original_de_vidre.jpg/981px-Baldomer_Gili_Roig._La_Sagrada_Fam%C3%ADlia%2C_1905_Copia_moderna_del_negatiu_original_de_vidre.jpg",
    "last_modified": "2020-06-06T06:58:54+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-01-21-genetic-analyses-and-solidarity/",
    "title": "Genetische Analysen implizieren keineswegs Unmenschlichkeit – im Gegenteil",
    "description": "Genetische Unterschiede sind ein Argument für den Sozialstaat",
    "author": [
      {
        "name": "Ruben C. Arslan",
        "url": "https://rubenarslan.github.io"
      },
      {
        "name": "Gert G. Wagner",
        "url": {}
      },
      {
        "name": "Philipp Köllinger",
        "url": {}
      }
    ],
    "date": "2019-01-21",
    "categories": [],
    "contents": "\nMy colleagues and I wrote the following (in German) to announce the release of a new GWAS on risk preferences. We also respond to an article in the Sueddeutsche Zeitung on the heritability of intelligence that we thought was misleading on some major points. Maybe some of those misconceptions and the desire to reject genetic explanations for individual differences come from an assumption that genetic research must lead to a “survival of the fittest” mentality. We disagree. A shorter version appeared in the Sueddeutsche Zeitung today.\ngekürzt erschienen unter dem Titel „Argumente für mehr Solidarität“ in der Süddeutschen Zeitung am 21. Januar 2019, S. 18\nRuben C. Arslan ist Psychologe am Max Planck Institut für Bildungsforschung, Berlin, wo auch der Ökonom Gert G. Wagner tätig ist. Philipp Köllinger ist Professor für Genom-Ökonomie an der Freien Universität Amsterdam. Köllinger und Wagner sind auch Research Fellows am Deutschen Institut für Wirtschaftsforschung (DIW Berlin). Alle Autoren waren 2014 bis 2017 Mitglied in der Forschergruppe „Genetic and Social Causes of Life Chances“ am Zentrum für interdisziplinäre Forschung (ZiF) der Universität Bielefeld.\nMitte Januar 2019 ist in der Fachzeitschrift Nature Genetics ein Aufsatz zu den genetischen Grundlagen der menschlichen Risikobereitschaft online gegangen, an dem zwei der Autoren dieses Beitrags beteiligt sind (u.a. auf Basis der Daten der Berliner Altersstudie). Im Genom von über einer Million Menschen, deren Daten analysiert wurden, wurden 124 Stellen gefunden, die die Risikobereitschaft vorhersagen, die von den analysierten Befragten angegeben wurde. Außerdem wurden entsprechende molekulargenetische Zusammenhänge mit tatsächlichem Verhalten, etwa Rauchen und Trinken, gefunden. Aber von der enormen Streuung der menschlichen Risikobereitschaft kann durch einzelne Gen-Varianten fast nichts statistisch erklärt werden. Die wichtigste Variante erklärt nur 0,02 Prozent der Streuung – statistisch signifikant, also kein reiner Zufall, aber trotzdem wird so gut wie kein Unterschied in der Risikobereitschaft durch eine Stelle im Genom erklärt.\nDurch die Betrachtung von größeren Bereichen des Genoms, die Millionen von einzelnen Gen-Buchstaben enthalten (aus über sechs Milliarden Buchstaben, aus denen ein menschliches Genom besteht), konnten immerhin 1,6 Prozent der Streuung statistisch aufgeklärt werden. Die winzigen Effekte vieler Stellen im Genom summieren sich. Aber auch nun würde es sich nicht lohnen die Speichelproben von Investmentbankern zu analysieren, um herauszufinden, wie risikobereit sie sind. In der Studie wird ausdrücklich festgestellt, dass das Ergebnis der Analyse sich nicht eignet um die Risikobereitschaft eines einzelnen Menschen vorherzusagen – der Prognosefehler wäre viel zu hoch. Wozu also der ganze Aufwand? Was kann man aus solchen Genom-Analysen schlussfolgern und was nicht? Sind derartige genetische Analysen nicht von Grund auf menschenunwürdig und führen in moralische Abgründe? „Nein“ ist unsere im folgenden begründete Antwort. Genetische Erkenntnisse können sogar zur Menschenwürde beitragen, da sie gute Argumente liefern für Solidarität und für Bestrebungen hin zu mehr Chancengleichheit in unserer Gesellschaft.\nErst einmal ist gewissermaßen „innerwissenschaftlich“ festzuhalten, dass die erfolgreiche Erklärung von psychologischen Unterschieden durch molekulargenetisch gemessene Unterschiede die bereits länger bestehenden Kernergebnisse der sogenannten Verhaltensgenetik bestätigt. Diese Disziplin berechnet auf Basis von Studien zur Ähnlichkeit von beispielsweise adoptierten Geschwistern oder separat aufgewachsenen eineiigen Zwillingen die Wichtigkeit von genetischen Unterschieden gezeigt. Demnach sind im statistischen Mittel genetische Unterschiede innerhalb von Familien maßgeblicher für Persönlichkeit und kognitive Leistungsfähigkeit als Unterschiede in der Erziehung zwischen Familien. Eltern mit mehreren Kindern wissen auch wie unterschiedlich ihre Kinder sein können, obwohl ihre familiäre und soziale Umgebung ja ähnlich ist. Aber weder die menschliche Erfahrung und erst recht nicht die neuartigen genetischen Analysen können die Kritiker der Humangenetik überzeugen.\nKürzlich bestritt zum Beispiel der Regensburger Psychologieprofessor Christof Kuhbandner in der Süddeutschen Zeitung (14. Dezember 2018), dass Intelligenz überhaupt in einem nennenswerten Ausmaß erblich sei – und legt nahe, dass Kinder mehr lernen, wenn man sie glauben lässt, dass an Intelligenz nichts Angeborenes ist. Er kritisiert die Aussagekraft der Verhaltensgenetik und prognostiziert, dass das Genom niemals mehr als etwa vier Prozent der Streuung von gemessener Intelligenz vorhersagen können wird. In der Tat sind es aber bereits jetzt bis zu 10 Prozent, die sich molekulargenetisch erklären lassen, d. h. durch die Betrachtung des Genoms im Detail – Buchstabe für Buchstabe. Und selbst wenn es wirklich „nur“ vier Prozent wären, wäre das eine ganze Menge. Wenn jemand bei allen gefundenen Genen die intelligenzzuträglichen Varianten hätte, entspräche das dem Effekt eines zusätzlichen Schuljahrs. Und Kuhbandner unterschätzt die genetische Prognostik systematisch. Er hat zwar recht, dass Gen-Varianten, die mit Hilfe von immer größeren Stichproben von Menschen gefunden werden, immer weniger an Streuung erklären werden (weil man die größeren Effekte bereits mit kleineren Stichproben gefunden hat), aber das bedeutet nicht, dass deswegen die Analyse nicht lohnt. Denn es werden immer mehr einzelne Gen-Buchstaben gefunden werden, die in ihrem Zusammenwirken sehr wohl mehr erklären werden. Dies zeigen Studien beispielsweise zur Genetik von Körpergröße sehr eindeutig. Für diese Bereiche können wir bereits jetzt ein Viertel der Streuung mit Hilfe tausender Gen- Buchstaben erklärt werden. Forscher nehmen an, dass für den IQ über 30% mit riesengrossen Stichproben von Millionen von Menschen aus den Molekülen des Genoms vorhersagbar sein werden. Auch wenn 30 Prozent übertrieben sein mögen, zeigen die Molekulargenetik, dass angeborene Unterschiede der Intelligenz nicht ganz unwichtig sind. Das heisst aber natürlich nicht, dass deswegen die Schlaueren dies ungehemmt ausnützen können sollten. Wir werden darauf am Ende zurückkommen.\nErziehungswissenschaftler wie auch Kuhbandner argumentieren auf einer pädagogischen Ebene. Sie sagen, dass es für Kinder in der Schule besser sein, wenn man gar nicht von angeborenen Unterschieden spricht. Wir behaupten das genaue Gegenteil. Es wäre offenbar ja unverantwortlich und grausam, von einem Kind mit eingeschränkter kognitiver Leistungsfähigkeit soviel zu erwarten wie vom begabten Geschwisterkind. Genauso wäre es grausam jemandem, der mit Mühe den Schulabschluss geschafft zu empfehlen, doch zum Softwarearchitekten umzuschulen, wenn seine berufliche Tätigkeit wegautomatisiert wird. Die Note von „nicht bestanden“ in „noch nicht bestanden“ zu ändern, wie Kuhbander nahe legt, wird auch kaum helfen – neuere Studien können die von Carol Dweck vermarkteten Erfolge des growth mindset jedenfalls nicht unabhängig replizieren.\nSchule bildet ohne Zweifel, aber sie kann die Effekte genetischer Unterschiede nicht völlig einebnen. Man kann auch Menschen durch „Schulung“ nicht beliebig risikofreudig machen, um mehr Unternehmertum zu haben oder sich einiges an sozialer Sicherung zu ersparen, da risikofreudige Menschen große Einkommensschwankungen klaglos hinnähmen.\nBei der modernen genetischen Forschung, wie sie hier berichtet wird, geht es auch nicht nur darum einzelnen Menschen bessere Therapien bei schweren Krankheiten oder maßgeschneiderte Lehr-Konzepte anzubieten (es bleibt ohnehin abzuwarten was da möglich sein wird). Es geht auch darum, und das mag sich am Ende vielleicht als noch wichtiger erweisen, bestimmte gesellschaftliche Strukturen, die immer wieder in Gefahr geraten zerstört zu werden, besser zu begründen und abzusichern als bislang. Also: Man sollte die Evidenz relevanter genetischer Unterschiede nicht leugnen. Sie bedeutet keineswegs, dass daraus ein „Survival of the Fittest“ als normative Leitlinie für das menschliche Zusammenleben folgen muss. Ganz im Gegenteil! Man kann auch ohne weiteres auch argumentieren, dass Nachteile, die einem Menschen durch seine „genetische Ausstattung“ entstehen, von der Gesellschaft zumindest teilweise kompensiert, vielleicht sogar möglichst weitgehend ausgeglichen werden sollten, da ja niemand was für seine Gene kann und sie mit dazu beitragen an welcher Stelle in der Gesellschaft sich jemand findet. Vielfältigkeit der Menschen ist ein Wert an sich. Genetische Vielfalt erhöht die Anpassungsfähigkeit einer Art und Gene, die heutzutage mit Nachteilen verbunden sind, können sich in Zukunft für die Menschheit als lebenswichtig erweisen. In der Tat orientiert sich unsere Steuer- und Sozialpolitik an diesen Überlegungen – in Deutschland und in vielen Ländern in der Welt. Wir wollen die Menschen nicht gleichmachen, sondern wir wollen Nachteile aller Art, mit denen wir ohne eigene Schuld durch das Leben gehen müssen, soweit es vernünftig ist, kompensieren.\nEin Wirtschaftsphilosoph, John E. Roemer, hat diesen Standpunkt mit seiner Theorie zur Chancengleichheit (Equality of Opportunity) auf den Punkt gebracht: individuelle Anstrengung sollte sich lohnen, aber Unterschiede im Einkommen, die sich aufgrund der Chancen ergeben, die einem die Eltern mitgaben (sowohl genetisch wie sozial), sollten durch Steuern und Transfers ausgeglichen werden. Eine progressive Besteuerung hoher Einkommen einerseits und andererseits Transfers an Menschen, die Probleme haben ein ordentliches Einkommen zu erzielen, machen genau dieses. Natürlich nicht perfekt, aber weit jenseits eines ungebremsten individuellen Egoismus, der un-verdiente Vorteile, die jemand mitbekommen hat, ausnützt. Mit anderen Worten: Da eine günstige genetische Ausstattung im Wortsinne un-verdient ist, können die Analysen, die die Bedeutung der Gene für das menschliche Leben zeigen, die Argumente für einen starken Steuer- und Sozialstaat stärken.\n\n\n",
    "preview": {},
    "last_modified": "2020-06-06T06:58:53+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-01-02-are-studies-that-replicate-cited-more/",
    "title": "Revised: Are studies that replicate cited more?",
    "description": "Looking at the RPP to bring data to a discussion",
    "author": [
      {
        "name": "Ruben C. Arslan",
        "url": "https://rubenarslan.github.io"
      },
      {
        "name": "Ioanna Iro Eleftheriadou",
        "url": {}
      }
    ],
    "date": "2019-01-02",
    "categories": [
      "meta science",
      "open science",
      "reproducibility",
      "quick job"
    ],
    "contents": "\nReplication in the Reproducibility Project Psychology and citations\n\n\n\nFigure 1: Does it replicate? From the Internet Archive Book Images\n\n\n\nAfter his talk at the Center for Adaptive Rationality, Stephan Lewandowsky and I had a small discussion whether scientists can actually pick “winners”. The discussion stemmed from a larger discussion about whether we get more research waste, if we replicate first, then publish, or publish first, and then replicate those studies that are found interesting.\nIf I recall correctly, we didn’t really disagree that scientists can tell if things are off about a study, but we did disagree on whether citation indexes such a quality assessment, and is a useful way to find out which studies are worthy of more attention.\nSo, I ran the numbers for one of the few studies where we can find out, the Reproducibility Project: Psychology. I tweeted it back then, but felt like making the graphs nicer and playing with radix on a train ride.\n\n\n\n\n\n\n\n\n\nWe found 167 DOIs, so we had DOIs for all our studies1.\n\n\nscopus_pre2015\nscopus_2018\nscopus_post2015\ngscholar_pre2015\ncrossref_2018\nmixed_post2015\nscopus_pre2015\n1.00\n0.98\n0.93\n0.97\n0.97\n-0.05\nscopus_2018\n0.98\n1.00\n0.98\n0.97\n0.98\n0.04\nscopus_post2015\n0.93\n0.98\n1.00\n0.93\n0.97\n0.13\ngscholar_pre2015\n0.97\n0.97\n0.93\n1.00\n0.97\n-0.15\ncrossref_2018\n0.97\n0.98\n0.97\n0.97\n1.00\n0.10\nmixed_post2015\n-0.05\n0.04\n0.13\n-0.15\n0.10\n1.00\n\nDoes replication in the RPP predict how often a paper is cited?\nNo, not for the citation count recorded in the RPP.\n\n\n\nCall:\nglm(formula = citations_2015 ~ replicated_p_lt_05, family = quasipoisson(), \n    data = .)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-8.331  -4.810  -1.726   3.067  14.583  \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            3.93854    0.09962  39.534   <2e-16 ***\nreplicated_p_lt_05yes -0.08052    0.17203  -0.468    0.641    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 32.61327)\n\n    Null deviance: 2813.6  on 98  degrees of freedom\nResidual deviance: 2806.4  on 97  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\nDoes replication predict 2018 citation counts?\nDetails: I got DOIs, which were missing from the RPP data, by searching Crossref on titles, authors, and dates. I did some checking to see if matches were proper. Next, I got citation counts from Scopus and validated those against those in the RPP and Crossref. Find the improved dataset with DOIs below.\nI used the Crossref API to get DOIs and the Scopus API to get yearly citation counts for the papers contained in the RPP.\nEdit: The SCOPUS citation count up to 2015 was highly correlated with the one stored in the dataset (based on Google Scholar). Rank order were also very similar for citations pre and post 2015 using Scopus, CrossRef, or Google Scholar. However, subtracting CrossRef citation counts from Google Scholar counts amplified error (to get citations after the publication of the RPP) - the correlation with the “citations after 2015 (Scopus)” variable was low. Therefore, the revised version of this blog post uses only the Scopus numbers.\nAgain, there was no association with replication status for 2018 citation counts.\n\n\n\nCall:\nglm(formula = citations_2018 ~ replicated_p_lt_05, family = quasipoisson(), \n    data = .)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-11.005   -6.988   -3.263    4.546   19.781  \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            4.50328    0.10635  42.344   <2e-16 ***\nreplicated_p_lt_05yes -0.04297    0.18138  -0.237    0.813    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 65.37293)\n\n    Null deviance: 5660.0  on 98  degrees of freedom\nResidual deviance: 5656.3  on 97  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\nDoes replication predict subsequent citation counts (ie. 2015-2018)?\nThe correlation between 2018 Scopus and 2015 Google Scholar counts is 0.97, but the means differ (2018 Scopus = 72, Scholar 2015 = 90). Can citations go down? No, but Google Scholar includes more sources than Scopus, leading to the mean being higher. Still, these sources don’t seem to be systematically different, leading to the maintained rank order.\n\n\n\nThis is pretty dirty work, because I’m subtracting citation counts from one source with another, so most papers are cited less in 2018 than in 2015. But haven’t found a quick way to get citation counts in 2015 from rcrossref. I’ve requested the necessary access to Scopus, where I could check, but Elsevier is being annoying.\nAgain, no association. So, assuming the dirtiness of the analysis doesn’t matter, \nThe literature hasn’t reacted at all to the presumably important bit of information that a study doesn’t replicate.\n\n\n\nCall:\nglm(formula = citations_after_2015 ~ replicated_p_lt_05, family = quasipoisson(), \n    data = .)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-7.899  -5.302  -2.963   3.103  13.664  \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           3.662760   0.119244  30.717   <2e-16 ***\nreplicated_p_lt_05yes 0.004458   0.200260   0.022    0.982    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 35.46258)\n\n    Null deviance: 3101.3  on 98  degrees of freedom\nResidual deviance: 3101.3  on 97  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\nWhat about self citations?\nThe RPP emphasised its own overall result. Hence, some nonreplications of specific studies may have gone unnoticed by researchers in the field. But the study authors hardly have this excuse; they knew whether their study was replicated (probably even prior to 2015, but this is hard to figure out). However, there is also no significant difference in self citation count (before or after 2015) by publication status.\n\n\n\nCall:\nglm(formula = self_cites_before_2015 ~ replicated_p_lt_05, family = quasipoisson(), \n    data = .)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-4.1533  -1.8906  -0.3514   1.3266   6.3949  \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            2.15466    0.09269  23.246   <2e-16 ***\nreplicated_p_lt_05yes -0.07880    0.15997  -0.493    0.623    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 4.742497)\n\n    Null deviance: 463.83  on 98  degrees of freedom\nResidual deviance: 462.67  on 97  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\nCall:\nglm(formula = self_cites_after_2015 ~ replicated_p_lt_05, family = quasipoisson(), \n    data = .)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.6939  -2.0400  -0.6064   0.3752   7.5933  \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             1.0934     0.1699   6.437 4.65e-09 ***\nreplicated_p_lt_05yes   0.1954     0.2688   0.727    0.469    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 5.510687)\n\n    Null deviance: 429.28  on 98  degrees of freedom\nResidual deviance: 426.40  on 97  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 6\n\nHow does pre-2015 citation count predict post-2015 citations accounting for replication status?\nA slightly different way of looking at it does not yield different conclusions for me.\n\n\n\nDoes the association differ by journal?\nHard to tell with this little data!\n\n\n\nCall:\nglm(formula = citations_2015 ~ Journal * replicated_p_lt_05, \n    family = quasipoisson(), data = .)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-8.331  -4.332  -1.445   2.532  12.122  \n\nCoefficients:\n                                  Estimate Std. Error t value\n(Intercept)                        3.47197    0.24258  14.313\nJournalJPSP                        0.64544    0.27814   2.321\nJournalPS                          0.49518    0.28506   1.737\nreplicated_p_lt_05yes             -0.18759    0.37517  -0.500\nJournalJPSP:replicated_p_lt_05yes -0.01447    0.50369  -0.029\nJournalPS:replicated_p_lt_05yes    0.36557    0.43739   0.836\n                                  Pr(>|t|)    \n(Intercept)                         <2e-16 ***\nJournalJPSP                         0.0225 *  \nJournalPS                           0.0857 .  \nreplicated_p_lt_05yes               0.6182    \nJournalJPSP:replicated_p_lt_05yes   0.9771    \nJournalPS:replicated_p_lt_05yes     0.4054    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 28.42273)\n\n    Null deviance: 2813.6  on 98  degrees of freedom\nResidual deviance: 2419.1  on 93  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\nConclusion\nSo, are citation counts a poor indicator of quality? The most common reaction I received to these results was saying that the 7 years from the publication of the studies to 2015 are probably not enough for citation counts to become more signal than noise, or at least that the 3 years from the publication of the RPP results to 2018 are not enough. These reactions mostly came from people who did not really believe in citations-as-merit before anyway.\nTo me, if 10 years after publication citations cannot be used to distinguish between studies that replicated and those that didn’t, they’re probably not a useful measure of thoroughness that can be used in assessment, hiring, and so on. They may be a useful measure of other important skills for a scientist, such as communicating their work; they may measure qualities we don’t want in scientists, but it seems they are not useful to select people whose work will replicate. I think that is something we should want to do.\nIn addition, the literature does not react quickly to the fact that studies do not replicate. Given that people also keep citing retracted studies (albeit with a sharp drop), this does not surprise me. It will be interesting to revisit the data in a few years time and see if researchers picked up on replication status then.\nLimitations\nThese were all studies from reputable journals, so we might have some range restriction here. On the other hand, plenty of these studies don’t replicate, and citation counts go from 0 to >300.\nWhich studies keep being cited after not being replicated?\nHover your mouse over the dots to see the study titles.\n\n\n\nWhich authors keep citing their own studies after they do not replicate?\nHover your mouse over the dots to see the study titles.\n\n\n\nList of studies\n\n\n\nAppendix\n\n\n\nThese analyses are based on Chris J. Hartgerink’s script. The data and his script can be found on the OSF. Did I get the right DOIs? There are probably still some mismatches. Titles are not exactly equal for 84 studies, but on manual inspection this is only because Crossref separates out the subtitle, and 150 of 167 titles start exactly the same.\nWere they they all correct? See Appendix↩\n",
    "preview": "posts/2019-01-02-are-studies-that-replicate-cited-more/are-studies-that-replicate-cited-more_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2020-06-06T06:58:54+02:00",
    "input_file": {},
    "preview_width": 1152,
    "preview_height": 768
  },
  {
    "path": "posts/2018-10-26-on-making-mistakes-and-my-bug-bounty-program/",
    "title": "New rational athletics for boys and girls.",
    "description": "Accounting for mistakes in my scientific work and announcing a bug bounty program.",
    "author": [
      {
        "name": "Ruben C. Arslan",
        "url": "https://rubenarslan.github.io"
      }
    ],
    "date": "2018-10-26",
    "categories": [
      "mistakes",
      "bug bounty"
    ],
    "contents": "\nA story of few papers and many corrections\nMy first published paper had a typo in the title. I had gone over every use of the word “parental” and “paternal” in my draft and asked a friend to do the same, because I knew I tended to mix them up. I forgot the title. I emailed PLoS ONE frantically when I noticed, as the paper had not yet been published, but apparently it slipped their mind, and I had to ask again after publication.\nSo, my academic career started with a correction. And it was not going to be the last.\n\n\n\nFigure 1: Figure from the book New rational athletics for boys and girls (1917) in the Internet Archive Book Images\n\n\n\nHere, we accidentally reported a lower cutoff than we actually used owing to some miscommunication to my co-authors on my part.\nAnd recently, my co-authors and I replied to a critical commentary of one my dissertation papers. I actually love this kind of stuff, as I personally feel that I learn the most when seeing a nice debate between two parties of opposite viewpoints.1 However, this was a fundamentally dissatisfying experience because I spent more time battling journal submission forms than counter-arguments, because the journal caused a version problem and then hardly owned up to the mistake (see Appendix).\nI only noticed the version problem because an anonymous submission via my Tell me I’m wrong form alerted me to the fact that I was misquoting the authors. Thank you, anonymous commenter.\nAnd lately, I’ve received some useful post-publication peer review of my ovulatory changes paper. I had put all my code online, and a previous peer reviewer was actually motivated to check it for errors. Others like Dan Engber or Mike Wood just noticed problems right away, by virtue of their different background in journalism and statistics. This is great, but I admittedly dread another corrections process. I would love to work in a system where we publish and update our knowledge base more continuously than through the antiquated system of publication, correction, and retraction. But I don’t and here’s an attempt to reckon with that.\nConsequences\nThis is the story of the mistakes I know about so far. It is not an impressively low rate for a fledgling scientist, but I’m at my limit in terms of reducing my own error rate through best practices on my own.2\nI also know that for every new release of formr.org that involves more than a few lines of code, I make mistakes. With formr.org I usually find out quite early, because users notice the ensuing problems. Some of my scientific data analyses also involve a lot of code. Do I magically make fewer errors when working with R than with PHP and JavaScript?\nNo. If I’m honest with myself, I think there are probably even more errors in my work than the ones that have been found. Unfortunately, psychological science does not have a well-developed error culture and it is rare for people to reanalyse others’ data, even if they build on it.\nCode review\n\n\n\nFigure 2: Watch closely. Figure from the book New rational athletics for boys and girls (1917) in the Internet Archive Book Images\n\n\n\nIt is probably a co-incidence, but my paternal and fitness paper was the only one where one of my co-authors (thanks, Kai) independently re-did some of my central analyses completely independently3 - and it is also the only one where I haven’t yet had to issue a correction, even though I was primarily responsible for analysis.\nSo, I want to further promote code review in my work. I urge it on the people I supervise, I’m collaborating on a draft to advertise the practice. But there is also one other avenue that I want to try.\nA bug bounty program\n\n\n\nFigure 3: Get him. Figure from the Internet Archive Book Images\n\n\n\nSo, I’m announcing a bug bounty program. Briefly, I will pay you to report errors in my work to me (up to 760€). It generally only applies to first-authored scientific work I publish from now on, but my co-author and friend Laura Botzet volunteered her first-authored preprint on birth order effects in Indonesia as a guinea pig. I’m senior author on this paper, and tried my best to check the code (I also wrote some parts myself). Data and code are openly available, so if you feel like playing with a cool dataset and an interesting research question, we encourage you to check and review our code. Please see here for the exact conditions4\nA bad error culture at Proceedings of the Royal Society B\nI was invited to peer review the critique5 and pointed out a few factual errors. As a result, the critique that was accepted for publication already partially addressed some counterpoints, leading to a quite jumbled argument-counterargument sequence.\nThen, the journal messed up. After a first round of peer review and then acceptance, they allowed the authors to revise their critique further. The authors substantially changed the text and added two new arguments. But the journal sent me the older version to comment, not the one they published. As a result, our reply quotes the commenters three times - and none of those quotes are in the published version - we looked quite sloppy.\nMistakes happen (I know!), but it took a while for the journal to acknowledge the mistake, and even longer for them to decide how to deal with it. We then had to submit an erratum (apparently, in their parlance, an erratum is for journal errors, corrections are for author errors). The journal then desk-rejected the erratum, because they felt replying to the newly added arguments didn’t fit in the erratum. I then added a sentence to reflect the fact that we were not allowed to respond to these additional points. They removed this sentence without my consent, and then rephrased the erratum to put it in the journal’s voice. In the end, this is all they published. The whole ordeal after I pointed out the error, took from March to August, in excess of 40 emails, and a lot of nerves.\nThe original authors also did not permit me to simply post the versions that we were reacting to, including the review process, to actually allow those who care to see the whole back and forth. For posterity, the director’s cut of the erratum is here.\nI still need to read The Enigma of Reason which makes the point that arguing is how we reason best.↩\nOr at least at the level that collaborators and employers will tolerate with respect to how much this slows me down↩\ndifferent data cleaning, used Stata instead of R, used a different model in some places↩\nThey are phrased rather lawyerly, because I would expect a bug bounty program to attract nitpicky people.↩\nwhich I didn’t like because I didn’t feel I should get to influence whether criticism of my work is published↩\n",
    "preview": "https://farm6.staticflickr.com/5578/14804680103_c8d5ef8ef0_o_d.jpg",
    "last_modified": "2020-06-06T06:58:54+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-10-15-are-big-studies-cited-more/",
    "title": "Are big studies cited more?",
    "description": "Does sample size predict number of citations?",
    "author": [
      {
        "name": "Ruben C. Arslan",
        "url": "https://rubenarslan.github.io"
      },
      {
        "name": "Ioanna Iro Eleftheriadou",
        "url": {}
      }
    ],
    "date": "2018-10-22",
    "categories": [
      "meta science",
      "open science",
      "reproducibility",
      "quick job"
    ],
    "contents": "\nAfter asking whether studies that replicate are cited more and finding that this is not the case, I turned to a different analysis that I once did quickly. Namely, I reviewed the N-pact paper by Chris Fraley and Simine Vazire back in 2014. I got excited about the paper, as I’m wont to, and tried redoing their analyses because they had provided the data with their manuscript. I also decided to see if I could find a relationship at the article level as well.\n\n\n\nFigure 1: Ample size? From the Internet Archive Book Images\n\n\n\nIro Eleftheriadou, our new RA, helped me clean up the messy code that I wrote at the beginning of my dissertation (for-loops, ugh).\n\n\n\n\n\n\n\n\n\nAgain, we first needed to get the DOIs by querying the Crossref API. Using the DOIs, we could next get the citation counts. We found 1028 DOIs out of 1042 studies.\nReproducing the original result\nHere, I first checked whether I would obtain the same measure as Fraley & Vazire did, when I simply averaged the citations to the articles in this set for each year and journal. Interestingly, I could not, or at least it was substantially weakened. I’m pretty sure that when I did this in 2014, I could reproduce the finding. As you can also see, the association is even weaker when using medians for the citation counts instead of means. However, I can still reproduce their original result with the Thomson-Reuters Impact Factors that I quickly googled. I know the Impact Factor isn’t just a simple metric, but a shady business including some negotiation. Do journal executives sometimes try a pity move, asking for a higher impact factor by highlighting paltry poor sample sizes? It’s impossible to prove this never happened ;-)!\n\n\n\nFigure 2: Correlation between IF and NF on the journal level.\n\n\n\nMoving to the study level\nFraley & Vazire were interested in highlighting how impact factors do not track a no-nonsense aspect of study quality (all else equal, bigger N is simply better). So, they focused on journals. But I, having recently invested a lot of time into obtaining large samples, was curious to see whether these efforts translated into any sort of citation advantage, and more generally, whether we pay more attention to research that is presumably more reliable.1.\nBy year of publication\nMaybe it takes us some time to notice how paltry a sample size was?2 Or maybe early citations are all about the glamour and flair, but over time the best and biggest studies win out?\n\n\n\nFigure 3: There is no association between N and citations, including for studies that had more years to accumulate citations.\n\n\n\nBy journal\nMaybe we don’t see an association between N and citations in Psychological Science, because some small-N papers are solid vision studies with good power within-subject? In the analysis by journal, we see that there is no association between sample size and number of citations within each journal, including some that focus on personality research only, which is a pretty N-driven area.\n\n\n\nFigure 4: Even in the Journal of Personality and the Journal of Research in Personality, there is no association between N and citations. There appears to be some in SPPS, but the number of studies is small and we should not overinterpret it.\n\n\n\nThe Spearman rank correlations between sample size and citations by journal tell the same story.\n\nTable 1: Correlation values per journal\nIF\nNF\nJournal\nk_studies\ncorrelation\n21\n194\nSPPS\n48\n0.36\n32\n273\nJRP\n129\n0.05\n34\n119\nJESP\n192\n-0.01\n46\n147\nPSPB\n155\n0.14\n49\n334\nJP\n70\n0.01\n65\n132\nPS\n269\n0.02\n85\n225\nJPSP\n179\n-0.01\n\nThere isn’t really any perceptible time trend in sample size,3 but it still seems worthwhile to divide the citation count by number of years until 2018 to see whether that makes a difference.\n\n\n\nFigure 5: We do not collect bigger samples over time.\n\n\n\nIt does not.\n\n\n\nFigure 6: Because there is no sample size trend, citations over years published is not related to sample size any differently than simple citation count.\n\n\n\nMaybe the problem is that I ignored the fact that citations are a count variable and did not adjust those correlations for the publication year. Nope!\n\nTable 2: Predicting citation count\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\nreis_n\n1.00\n0.00\n1.3\n0.2\n1.00\n1.00\nYear\n0.83\n0.02\n-8.1\n0.0\n0.79\n0.87\nTable 2: Predicting citation count by journal\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\nreis_n\n1.00\n0.00\n-0.37\n0.71\n1.00\n1.00\nJournalJP\n1.16\n0.27\n0.54\n0.59\n0.67\n1.94\nJournalJPSP\n2.08\n0.17\n4.36\n0.00\n1.49\n2.90\nJournalJRP\n0.83\n0.23\n-0.78\n0.43\n0.53\n1.30\nJournalPS\n1.74\n0.16\n3.45\n0.00\n1.27\n2.39\nJournalPSPB\n1.17\n0.20\n0.76\n0.45\n0.78\n1.73\nJournalSPPS\n0.70\n0.39\n-0.91\n0.36\n0.30\n1.44\nYear\n0.85\n0.02\n-7.49\n0.00\n0.81\n0.88\nreis_n:JournalJP\n1.00\n0.00\n0.63\n0.53\n1.00\n1.00\nreis_n:JournalJPSP\n1.00\n0.00\n0.73\n0.47\n1.00\n1.00\nreis_n:JournalJRP\n1.00\n0.00\n0.46\n0.65\n1.00\n1.00\nreis_n:JournalPS\n1.00\n0.00\n0.33\n0.74\n1.00\n1.00\nreis_n:JournalPSPB\n1.00\n0.00\n0.44\n0.66\n1.00\n1.00\nreis_n:JournalSPPS\n1.00\n0.00\n1.12\n0.26\n1.00\n1.00\nTable 2: Predicting citation count by year\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\nreis_n\n1.24\n0.19\n1.1\n0.26\n0.85\n1.8\nYear\n0.85\n0.03\n-5.6\n0.00\n0.80\n0.9\nreis_n:Year\n1.00\n0.00\n-1.1\n0.27\n1.00\n1.0\n\nSo, how does sample size relate to citation counts?\nThere seems to be pretty much nothing there! Maybe we can come up with a post-hoc narrative by looking at all studies and seeing which studies are cited a lot and have large or small samples.\nHover your mouse over the dots to see the study titles. Jean Twenge with her humongous name study is hiding behind the legend, scroll to find her.\nMy current favourite post-hoc narrative is that citation counts are just very poor measures of importance. I wouldn’t bet that a better measure of importance would correlate with sample size, but I’m fairly convinced by now that citation counts just suck as a metric even of the thing they’re meant to index (impact/importance). I am especially worried about the difference simple citation counts make between a study that is synthesised in a review that then gets cited a lot4 and a study that doesn’t get synthesised in a review. I tried to make the case for a better, more PageRank-y metric, but it would already be interesting to redo my analysis here and from the previous blog post after excluding self citations. Unfortunately, Elsevier still hasn’t given me Scopus API access, and I lack the chops to do anything more fancy. I really hope projects like the Open Citations Corpus lead to such fancy metrics becoming available.\n\n\n\n\nFigure 7: Interactive plot of Ns and citation counts.\n\n\n\nLimitations\nWell, there are some vision studies in here that used a within-subject paradigm and probably had adequate power for the question they asked anyway. Their small sample size may limit generalizability to other people, but I guess that’s not the dominant question asked in vision science.5 But then again, for some pure-personality journals, we also find no association. There is of course the possibility that studies with very large Ns are more likely to use abbreviated measures, only self-report, and online samples. But then, they are also more likely to be from representative panel studies and online samples aren’t actually any less representative than lab samples. It seems unlikely that any disadvantages exactly balance out with the advantages associated with sample size, so that we arrive at a zero correlation with importance.\nList of studies\nThought of something fun to do with the data that I didn’t do here? Grab the data below!\n\n\n\n\n\n\nFigure 8: Ample size ain’t worth it. From the Internet Archive Book Images\n\n\n\nAppendix\nYou can grab the original data on the OSF. Did we get the right DOIs? There are probably still some mismatches and for some, we didn’t find the DOI at all.\nI understand this is a presumption, see limitations↩\nI hope nobody skims that badly.↩\nUgh, that alone is depressing. Since 2006, so many new tools came on the market. How did that not help?↩\nObviously, reviews aren’t included here, but neither is their reflected glory.↩\nWhich is consistent with the fact that there are low-hanging fruit like The Dress waiting to be picked in the study of individual differences in vision.↩\n",
    "preview": "posts/2018-10-15-are-big-studies-cited-more/are-big-studies-cited-more_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2020-06-06T06:58:54+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 768
  }
]
