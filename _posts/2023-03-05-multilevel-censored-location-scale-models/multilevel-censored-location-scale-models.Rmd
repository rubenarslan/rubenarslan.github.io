---
title: "Multilevel censored location-scale models"
description: |
  Simulating several models and their ability to recover within-subject and between-subject effects on scale
author:
  - name: Ruben C. Arslan
    url: https://rubenarslan.github.io
date: 2023-03-05
output:
  distill::distill_article:
    self_contained: false
    code_folding: true
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
```

```{r}
library(tidyverse)
library(brms)
library(tidybayes)
options(mc.cores = parallel::detectCores(),
        brms.backend = "cmdstanr",
        brms.file_refit = "on_change")
```

Recently, I got mad at a ceiling.

https://twitter.com/rubenarslan/status/1632170569448685569

Let me explain. In a recent paper, Killingsworth, Kahneman, and Meller (2023) re-analyze Killingsworth's experience sampling data to determine the relationship between income and experienced well-being, happiness.

More than 30,000 subjects rated their happiness 50 times on average. KKM aggregated daily happiness to derive the subject's observed mean. Then they used a form of distributional regression, quantile regression, to predict not only the mean of happiness but also its variance by `log(income)`. Specifically, they investigated whether the relationship to the mean flattened above a yearly household income of $100,000. This was not the case, the relationship between income and mean happiness is loglinear above and below this threshold. It's also a quite small relationship, `log(income)` explains less than 1% of the variance in happiness.

In addition, they investigated several quantiles. They find that the relationship between `log(income)` and the 15% quantile does indeed flatten after $100,000. In my view, they inappropriately interpret this as evidence that there is an unhappy group of people for which money cannot alleviate suffering (above 100k).

A better way to put it would have been: at higher log(income) the variability in average happiness is higher. That is, some people have a lot of money and spend it on holidays, household help, and time with their loved ones. Others buy a social media platform and make themselves miserable.

Now, I'm not particularly acquainted with quantile regression. However, I've worked with multilevel location scale models a good deal in the past. I've learned primarily not to trust my intuition, or at least to calibrate with fake data simulation. I've included the simulations below. 


### Two questionable assumptions in the quantile regression
My conclusion is that KKM make two assumptions that I think are unlikely to hold:

1. that the variance of happiness within-subjects is homogeneous
2. that their aggregated means of happiness are free of sampling error

I was fairly sure that #1 is false, because, you know, I've seen happiness data. Not their happiness data, mind you, which they did not publicly share except in aggregated and rounded form.^[I requested the raw data on March 4.]

I was less sure whether #2 has much impact with 51 days per subject on average. 

### Simulation results

- Even with 51 days per subject and homogeneous within-subject variance, sampling error in the mean attenuates the relationship between `x` and `sd(y)` on the between-subject level. Especially at low levels of x, sampling error can still dominate the true `sd(id)` in `y`.
- Heterogeneous within-subject variance can bias the estimate of the between-subject variance (i.e., what is really a tendency for an increased within-subject variance at higher `x` looks like a weak tendency for an increase in between-subject variance).
- Censoring, i.e. when values above or below a certain cutoff (e.g., the maximum value of the scale is 100) get clipped, doesn't help. The bias in the aggregated between-subject model gets worse. Why? The aggregation is done without regard to censoring. So, a person with `c(94, 95, 96, 95)` gets 95, just like a person with `c(90, 100, 100, 90)`. But if we model censoring at the within-subject level, we think the second person probably has a higher true value! Killingsworth report only 5.5% of responses at ceiling, so this probably did not hit so hard. Still, aggregating loses relevant information (after aggregation "less than 0.5% of people in any income group had experienced well-being equal to the response ceiling, on average").
- Not a concern for KKM, but for one of my projects: at 51 days per person, there's minimal bias from between-subject to within-subject (i.e. mistaking a between-subject increase in scale for a within-subject increase in scale). At smaller ns this can happen, but it's not a big bias. The bias occurs because the means are estimated to lie on a normal distribution with a constant standard deviation across `x`.


There are Mplus (McNeish, 2020) and R-Stan ([Martin, 2022](https://cran.r-project.org/web/packages/LMMELSM/index.html)) models to estimate two-part mixed effects location-scale models that allow simultaneous estimation of scale effects at both the within- and between-subject level. However, they do not implement censored distributions. My favourite package `brms` does, but does not natively permit scale effects on the between-subject level to be estimated. However, it allows us to estimate different `sd(id)` for subgroups. I formed quartiles on x and used these as subgroups. Doing so allowed me to get a sort of poor man's version of a censored two-part mixed effects location-scale model. A better man would have coded it up in raw Stan. This model both gets me closer to the between-subject effect than the aggregated model and gets me an unbiased estimate of the within-subject effect.

You can find my simulations (well, one round) below.

## Lessons learned
In German, we of course have a word for this: Verschlimmbesserung. Killingsworth 2021 was a wonderful paper. I think this analysis of quantiles actually made things worse? How come?

I have an idea. Adversarial collaborations don't just sound like an oxymoron. Especially with large status differences, what you can get is a lot of behind the scenes communications, in which power, resources, stubbornness and verbosity can win over superior arguments. If you want to share data with an adversary, may I suggest:

https://twitter.com/rubenarslan/status/1631789472407859200

<details><summary>Simulations</summary>

## Location-scale within-subject
Here, mu(y) is a function of x, as is sigma(y) (only at the within-subject level).

```{r}
fit_models <- function(...) {
  rmdpartials::partial("fit_models.Rmd", ...)
}
```

## Location-scale within-subject
```{r}
fit_models(b_mean = 1,  b_sd_bs = 0, b_sd_ws = 0.8, y_ceiling = Inf)
```


## Location-scale between-subject
```{r}
fit_models(b_mean = 1,  b_sd_bs = 0.8, b_sd_ws = 0, y_ceiling = Inf)
```


```{r}
# remotes::install_github("stephensrmmartin/LMMELSM")
# library(LMMELSM)
# m_melsm_2part <- lmmelsm(list(observed ~ y,
#                     location ~ x,
#                     scale ~ x,
#                     between ~ x),
#                id, days)
# summary(m_melsm_2part)
```

## Censored location-scale within-subject
```{r}
fit_models(b_mean = 1,  b_sd_bs = 0, b_sd_ws = 0.8, y_ceiling = 1.5)
```

## Censored location-scale between-subject
```{r}
fit_models(b_mean = 1,  b_sd_bs = 0.8, b_sd_ws = 0, y_ceiling = 1.5)
```


</details>
